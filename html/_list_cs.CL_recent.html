<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<body><h1><a href="index.html">abstract</a></h1>
<a class="media" href="https://arxiv.org/abs/2204.03409" target="_blank"><h2>MAESTRO: Matched Speech Text Representations through Modality Matching</h2><img class="media-object" src="images/_pdf_2204.03409.png"/><p class="media-body"> They present Maestro, a self-supervised training method to unify representations learnt from speech and text modalities. They establish a new state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 11% relative reduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative) and 21 languages to English multilingual ST on CoVoST 2 with an improvement of 2.8 BLEU averaged over 21 languages.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03324" target="_blank"><h2>Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation</h2><img class="media-object" src="images/_pdf_2204.03324.png"/><p class="media-body"> The ultimate goal of artificial intelligence is to build computer systems that can understand human languages. Understanding commonsense knowledge about the world expressed in text is one of the foundational and challenging problems to create such intelligent systems. ALMEn is an Autoencoding Language Model based Ensemble learning method for commonsense validation and explanation. The method outperforms state-of-the-art models, reaching 97.9% and 95.4% accuracy on the validation and. explanation selection subtasks, respectively.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03262" target="_blank"><h2>Korean Online Hate Speech Dataset for Multilabel Classification: How Can Social Science Aid Developing Better Hate Speech Dataset?</h2><img class="media-object" src="abstract.png"/><p class="media-body"> They suggest a multilabel Korean online hate speech dataset that covers seven categories of hate speech. The 35K dataset consists of 24K online comments with Krippendorff's Alpha label accordance of.713. The base model with 24K initial dataset achieved the accuracy of LRAP.892, but improved to.919 after being combined with 11K additional data. The paper is not only limited to presenting a local hate speech data but extends as a manual for building a more generalized hate speech datasets.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03227" target="_blank"><h2>Accelerating Attention through Gradient-Based Learned Runtime Pruning</h2><img class="media-object" src="images/_pdf_2204.03227.png"/><p class="media-body"> Self-attention is a key enabler of state-of-art accuracy for various transformer-based Natural Language Processing models. The attention mechanism calculates a correlation score for each word with respect to the other words in a sentence. Commonly, only a small subset of words highly correlates with the word under attention. As such, a significant amount of computation is inconsequential due to low attention scores and can potentially be pruned. To best utilize this mathematical innovation, they devise a bit-serial architecture, dubbed LeOPArd, for transformer language models with bit-level early termination microarchitectural mechanism.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03243" target="_blank"><h2>Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators</h2><img class="media-object" src="images/_pdf_2204.03243.png"/><p class="media-body"> AMOS pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. The main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models. For better pretraining efficiency, they propose a way to assemble multiple MLMs into one unified auxiliary model.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03357" target="_blank"><h2>Parameter-Efficient Abstractive Question Answering over Tables or Text</h2><img class="media-object" src="images/_pdf_2204.03357.png"/><p class="media-body"> In this work, they study parameter-efficient abstractive QA in encoder-decoder models over structured tabular data and unstructured textual data. They also ablate over adapter layers in both encoder and decoder modules to study the efficiency-performance trade-off. The models out-perform current state-of-the-art models on tabular QA datasets such as Tablesum and FeTaQA, and achieve comparable performance on a textual QA dataset such as NarrativeQA.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03375" target="_blank"><h2>Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances</h2><img class="media-object" src="abstract.png"/><p class="media-body"> Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Flexible Goal Accuracy is a generalized version of JGA. Unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03315" target="_blank"><h2>Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-trained DNN-HMM-Based Acoustic-Phonetic Model</h2><img class="media-object" src="abstract.png"/><p class="media-body"> In spoken language understanding (SLU) what the user says is converted to his/her intent. They revisit ideas presented by Lugosch et al. using speech pre-training and threemodule modeling. They use as the phoneme module an open-source acoustic-phonetic model from a DNN-HMM hybrid automatic speech recognition system instead of training one from scratch. They fine-tune on speech only for the word module, and they apply multi-target learning (MTL) on the word and intent modules to jointly optimize SLU performance.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03637" target="_blank"><h2>tmVar 3</h2><img class="media-object" src="abstract.png"/><p class="media-body"> tmVar 3.0 is able to recognize a wide spectrum of variant related entities (e.g., allele and copy number variants) and to group different variant mentions belonging to the same concept in an article for improved accuracy. They have also processed the entire PubMed and PMC with tm Var3 and released its annotations on the FTP. tmV3 exhibits a state-of-the-art performance with over 90% accuracy in F-measure in variant recognition and normalization, when evaluated on three independent benchmarking datasets.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03084" target="_blank"><h2>Knowledge Infused Decoding</h2><img class="media-object" src="images/_pdf_2204.03084.png"/><p class="media-body"> Pre-trained language models (LMs) tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. They present Knowledge Infused Decoding (KID) -a novel decoding algorithm for generative LMs. KID dynamically infuses external knowledge into each step of the LM decoding. It maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03619" target="_blank"><h2>Modeling Label Correlations for Second-Order Semantic Dependency Parsing with Mean-Field Inference</h2><img class="media-object" src="abstract.png"/><p class="media-body"> Second-order semantic parsing with end-to-end mean-field inference has been shown good performance. Direct modeling leads to memory explosion because second-order score tensors have sizes of $O(n^3L^2)$ ($n$ is the sentence length and the number of labels), which is not affordable. To tackle this computational challenge, they leverage tensor decomposition techniques. They show that the large second order score tensor. tensor. tensors. have no need to be materialized during mean.field inference, thereby reducing the computational complexity from cubic to quadratic.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03301" target="_blank"><h2>Sequence-Based Extractive Summarisation for Scientific Articles</h2><img class="media-object" src="abstract.png"/><p class="media-body"> This paper presents the results of research on supervised extractive text summarisation for scientific articles. They show that a simple sequential tagging model based only on the text within a document achieves high results against a simple classification model. Improvements can be achieved through additional sentence-level features, though these were minimal. Through further analysis, they show the potential of the sequential model relying on the structure of the document depending on the academic discipline which the document is from. They also show that the model can be used to summarise the content of a scientific article.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03479" target="_blank"><h2>Delta Keyword Transformer: Bringing Transformers to the Edge through Dynamically Pruned Multi-Head Self-Attention</h2><img class="media-object" src="images/_pdf_2204.03479.png"/><p class="media-body"> Multi-head self-attention forms the core of Transformer networks. However, their quadratically growing complexity with respect to the input sequence length impedes their deployment on resource-constrained edge devices. They address this challenge by proposing a dynamic pruning method, which exploits the temporal stability of data across tokens to reduce inference cost. The threshold-based method only retains significant differences between the subsequent tokens, effectively reducing the number of multiply-accumulates, as well as the internal tensor data sizes.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03489" target="_blank"><h2>Position-based Prompting for Health Outcome Generation</h2><img class="media-object" src="images/_pdf_2204.03489.png"/><p class="media-body"> Pre-trained Language Models (PLMs) using prompts indirectly implied that language models can be treated as knowledge bases. They observe that satisfying a particular linguistic pattern in prompts is an unsustainable constraint that unnecessarily lengthens the probing task. They therefore explore an idea of using a position-attention mechanism to capture positional information of each word in a prompt relative to the mask to be filled, hence avoiding the need to re-construct prompts when the prompts linguistic pattern changes. Using various biomedical PLMs, the approach consistently outperforms a baseline in which the default mask language model is used to predict masked tokens.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03117" target="_blank"><h2>BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</h2><img class="media-object" src="images/_pdf_2204.03117.png"/><p class="media-body"> Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference. BiSyn-GAT+ fully exploits the syntax information (e.g., phrase segmentation and hierarchical structure) of the constituent tree of a sentence to model the sentiment-aware context of every single aspect and the sentiment relations across aspects. Experiments on fthe benchmark datasets demonstrate that BiSyn GAT+ outperforms the state-of-the-art methods consistently.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03465" target="_blank"><h2>BERTuit: Understanding Spanish language in Twitter through a native transformer</h2><img class="media-object" src="images/_pdf_2204.03465.png"/><p class="media-body"> BERTuit is the larger transformer proposed so far for Spanish language, pre-trained on a massive dataset of 230M Spanish tweets using RoBERTa optimization. The utility of this approach is shown with applications: a zero-shot methodology to visualize groups of hoaxes and profiling authors spreading disinformation. The motivation is to provide a powerful resource to better understand Spanish Twitter and to be used on applications focused on this social network, with special emphasis on solutions devoted to tackling the spreading of misinformation in this platform.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03286" target="_blank"><h2>Entailment Graph Learning with Textual Entailment and Soft Transitivity</h2><img class="media-object" src="abstract.png"/><p class="media-body"> The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity. EGT2 learns local entailment relations by recognizing possible textual entailment between template sentences formed by typed CCG-parsed predicates. It then uses three novel soft transitivity constraints to consider the logical transitivity in entailment structures. They propose a two-stage method to alleviate the sparsity issue and lead to significant improvement over current state-of-the-art methods.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03558" target="_blank"><h2>Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic</h2><img class="media-object" src="abstract.png"/><p class="media-body"> As natural language processing systems become more widespread, it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized. There is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks. They find that many systems demonstrate statistically significant unisectional social biases. They use these tools to measure gender, racial, ethnic, and. intersectional social biases across five models trained on emotion regression tasks in English, Spanish, and Arabic.</p></a>
<a class="media" href="https://arxiv.org/abs/2204.03251" target="_blank"><h2>Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings</h2><img class="media-object" src="images/_pdf_2204.03251.png"/><p class="media-body"> Language resources such as wordnets remain indispensable tools for different natural language tasks and applications. For low-resource languages such as Filipino, existing wordnets are old and outdated, and producing new ones may be slow and costly in terms of time and resources. In this paper, they propose an automatic method for constructing a wordnet from scratch using only an unlabeled corpus and a sentence embeddings-based language model. Using this, they produce FilWordNet, a new wordnet that supplants and improves the outdated Filipino WordNet.</p></a>
