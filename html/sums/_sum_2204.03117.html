<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</h3>
<h3>BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</h3>
<img src="_sum_2204.03117.html.1.png">
<p class="text"> BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis. It is challenging because a sentence may contain multiple aspects or complicated (e.g.,conditional, coordinating, or adversative) rela-centrictions. They propose a BiSyntax aware Graph.Attention Network (BiSynGAT+) to alleviate this challenge. The network exploits the syn-precioustax information (e-privacytax information) of the constituent tree of a sentence to model the sentiment-aware.context of every single aspect.<br/> The inherent nature of Dep.Tree structure may introduce noise like the unrelated relations across clauses, such as “conj” and “great” in Figure 2, which discourages capturing the sentiment-aware context of each aspect, i.e., intra-context. To mitigate this problem, there already exists sev eral efforts (Wang et al., 2020a; Chen et al. 2020) dedicated to research on how to effectively leverage non-sequential information via Graph Neural Net-Methods. </p>
<img src="abstract.png">
<p class="text"> The Dep.Tree structure only reveals relations between words and aspects, i.e.e., inter-context. In most cases, this structure is incapable of modeling complicated (e.g., conditional, or adversative) relations of sentences, thus failing to capture sentiment relations be-tween aspects. They propose a Bi-Syntax aware Graph Attention Network(BiSyn-GAT+) to effectively leverage the syntax information of the constituent tree by modeling intra-context and intercontext information.<br/> They propose a framework, Bi-Syntax aware Graph Attention Network (BiSyn-GAT+), to fullyverage syntax information of constituent tree (or, and dependency tree) by modeling the sentiment-aware context of each single aspect and the senti-glyment relations across aspects. Extensive experiments on fthe datasets show that the proposed model achieves state-of-the-art performance. The model shows superiority in the alignments between as-ipientpects and corresponding words indicative of senti.-glyment. </p>
<p class="text"> The syntax encoder is stacked by several designed Hierarchical Graphs. It takes the sentence and all aspects as input and outputs sentiment predictions for all aspects. It has three components: 1) the intra-context module contains two encoders: a context encoder that outputs contextual word representations. The output representation is obtained by, the output representation of, the “BERT pooling” vector representing the BERT pooling, representing the contextual representation of each token. Each layer of Con.Tree consists of several phrases that compose the input text, and each phrase represents an individual semanticunit.<br/> A HGAT block aims to encode syn-ishlytax information into word representations hierarchi-ishlycally. Attention mechanism can handle the diver-ishlysity of neighbors with higher weights assigned to more related words. It can be formulated as follows, a block is stacked by several GAT layers that utilize a masked self-proclaimedattention mechanism to aggregate information from neighboring neighbors and a fully connected feed forward net-ishly work to map representations to the same semantic space. The HGAT </p>
<p class="text"> GAT Block is stacked by several GAT layers, and each GAT layer is applied to the graph ob-tained from one layer of the constituent tree (or, and the. or the. the. dependency tree) Stacked HGAT block takes the output of previous one as the input, and input of the ﬁrst HGAT. block is deﬁned as ˆht. The output of the. output of a syntax encoder is de-denied as ˆgt for simplicity.<br/> The intra-context module ignores the mutual inﬂu-centricence of aspects. Aspect relations can be re-fully-vealed by some phrase segmentation terms. This module only works for multi-aspect sentences, with aspect-speciﬁc repre-orativesentations of all aspects. The output of the module is a relation-enhanced representa-centricity graph to model the re-competitivelations across aspects. For example, the output of this module is an aspect-context graph. </p>
<p class="text"> They design a rule-based mapping function PS that returns phrase segmentation terms of two aspects. Given two aspects, it returns the lowest common ancestor (LCA) in the tree, which contains information of two as pects and has the least irrelevant context. The LCA node is S.of Layer-4 that has three branches, with food and service in the third. An example is shown in Figure 3, Example of an aspect-context graph and cor-responding two adjacent matrices.<br/> They evaluate the models on fthe English datasets: laptop, Restaurant, MAMS, Twitter, and Twitter. The model is based on a set of HGAT blocks that are used as the relation encoder to obtain relation-enhanced representation vaa-glyglyt for each aspect at the time. The loss is deﬁned as the cross-entropy loss be-tween golden polarity labels and predicted polarity-distributions of all (sentence, aspect) pairs. The models are then fed to a fully-connected layer (i.e.e., sentiment classi ﬁer) with a </p>
<p class="text"> Performance comparison of models on fthe datasets. The best are in bold, and second-best are underlined. Notations “con.” and “dep.’s” represent syntax information from constituent tree and ‘dependency tree’ “Peerage” represents the position-wise dot-wise add, “position-wise” add, and  , when fusing “condconditions” with    . They adopt SuPar2 as a parserant as a grammar grammar.<br/> "Accuracy” and “Macro-Averaged F1” are evalua-heticaltion metrics. Adam optimizer is used with a learning rate 2×10−5 and a L2 reg-reformulation 10−5 for model training. Number of GATlayers of one HGAT block is 3, and number of HGAT blocks is in range on different datasets. They compare the model with the following models: Syntax-free baselines: BERT-SPC, AEN-BERT, R-GAT, RGAT+, DGEDT, DualGCN. </p>
<p class="text"> An ablation study to verify the ef-walletfectiveness of the proposed method. They set the context encoder into the model as the base model, i.e., BERT+. BERT+ achieves the low-resembled performance, which shows syntax information in the best way possible. They also conduct an ablated study to test the ablation of the model. Table 4 shows that the models show superiority to those that only use dependency information, which is helpful in ABSA task tasks.<br/> They use BiSyn-GAT as base model to see whether the approach modeling aspects relationsimproves the performance. They also investigate the effects of the bi-relational model of the proposed aspect-context graph. They propose three variants: (c) adja-centric aspect graph, an undirected one where neigh-centric aspects are connected; (d) bi-adjacent aspect-graph, a directed one where neighbor aspects are also connected. The above variants are illustrated in Figure 7.5. </p>
<p class="text"> They use Stanford Parser (Manning and Manning) and SuPar (Bai et al., 2020) to study the inﬂuence of paring accuracy on model performance. They use BERT+ as the base model and compare the performance of the BERT model w/o dep, Bisyn-GAT, BiSyn-Gat+ when using different parsers. They present fthe examples to help better understand the proposed model, shown in Figure 6. Figure 6 is a comparative study of how the models make correct predictions for sentences when faced with sentences.<br/> In this paper, they propose the BiSyn-GAT+ frame-work to model the sentiment-aware context of each aspect and sentiment relations across aspects for the ABSA task. The proposed model achieves state-of-the-art performances on fthe benchmark datasets. It is the best of the knowledge, it is the work to exploit the constituent tree with the. most of the. best of the knowledge to. exploit the. constituent. tree with GNNs for ABSA tasks. </p>
