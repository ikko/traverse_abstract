<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale</h3>
<h3>Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale</h3>
<img src="_sum_2204.03514.html.1.png">
<p class="text"> Researchers at the Georgia Institute of Technology have published a paper on learning-reinforcement learning (IL) and PICK&PLACE. They say IL (with no bells or whistles) outperforms RL using 240k human demonstrations. IL agents achieve ∼18% success on episodes with new object-receptacle locations when trained with 9.5K human demonstrations, while RL agents fail to get beyond 0%. IL agent learns efﬁcient object-search behavior from humans – it peeks into rooms, checks corners, turns in place to get a panoramic view.<br/> Many recent tasks of interest in the embodied-athe-AI community – e.g. ObjectGoal Navigation, rear-centricrangement, language-guided navigation and inter-actions, question answering [8–12] – involve some ﬂavor of this visual exploration. With careful reward engineering, reward engineering approaches to these tasks have achieved commendable success [13–17] However, engi-ishlyneering the ‘right’ reward function so that the learned policy behaviors exhibiting desired behavior is unintuitive and frustrating. </p>
<img src="_sum_2204.03514.html.2.png">
<p class="text"> They collect 92k human demonstrations, 80k demonstrations for OBJECTNAV and 12k for PICK&PLACE. They use this infrastructure to collect human demonstra-tion datasets for 2 tasks requiring visual search – 1) Ob-glyjectGoal Navigation (e.g. ‘ﬁnd & go to a chair’) and 2) Pick&PlACE. The success rate (93.7%) suggests that this task is largely doable for humans (but not 100%)<br/> IL-agents achieve ∼18% success on episodes with new-receptacle locations when trained with 9.5k human-human-demonstrations. RL agents fail to get beyond 0% success in PICK&PLACE, while IL agents achieve only 18% success. This suggests that simply collect-ishlying more demonstrations is likely to advance the state-of-art state-augmented search behavior further. Overall, the work provides compelling evidence for investing in large-scale imitation learning of human demonstrations. </p>
<p class="text"> Using the web infrastructure, they collect demonstration data for two embodied tasks – OBJECTNAV and Pick&PLACE They use PsiTurk to manage the tasks as it provides useful helper functions to log task-related metadata, as well as launch and approve tasks. They collect 70k demonstrations on the 56 training scenes from Matter-port3D following the standard splits deﬁned in The analysis in this section was performed on a subset of 35k.<br/> The pick-and-place task (PICK&PLACE) can be thought of as a natural extension of OBJECTNAV. Agents are not equipped with a map of the environment, and only have access to an RGBD camera and a GPS+compass sensor. Human agents must explore and navigate to the object, pick it up, explore the area, and place the previously-picked-up object on it. Humans traverse 3-4x and observe 2x the area of an environment when performing the task compared to shortest paths. The SPL for humans is 39.9% for training split episodes. </p>
<p class="text"> Human demonstrations are longer and have a more uniform action distribution than shortest paths. They generated 25.7k shortest-path demonstrations for PICK&PLACE, each averaging 342 steps. They col-lect human demonstrations for OBJECTNAV on 9 scenes from Matterport3D In each episode, objects and re-receptacles are instantiated by randomly sampling from 457.possible object-receptionacle pairs. The full action space is discrete and discrete and consists of MOVE_FORWARD (0.15m), MOVE.BACKWARD(1) TOOVE_RELEASE, GRAB_REleasing, NO_OP, STOP (step<br/> Humans have signiﬁcantly lower SPL, and 2x higher occupancy and coverage compared to shortest paths, suggesting the need for exploration. They use behavior cloning to learn a policy from demonstra-centrictions. The learning problem can be summarized as:. The base policy is a simple CNN+RNN architecture. They embed all sensory inputs using feed-forward modules. They feed in RGBD inputs of size 640×480 to a GRU that was pretrained on PointGoalnavigation using DD-PPO </p>
<p class="text"> PICK&PLACE and OBJECTNAV are CNN+RNN networks that embed and concatenate all sensory inputs, which are then fed into a GRU to predict actions. The agent also has a GPS+Compass sensor, which pro-pro-proves location and orientation relative to start of the episode. They train this policy for ∼400M.steps (= ∼21 epochs on ∼70k demonstration episodes). They evaluate checkpoints at every ∼15M steps for the last 50M.M. They report metrics for checkpoints with the highest success on the validation split.<br/> They compare the approach with two state-of-the-art RL approaches from prior work. Maksymets et al. train their policy using a reward structure that breaks OBJECTNAV into two subtasks – exploration and direct navigation to goal object once it is spotted. They then combine this reward structure with Treasure Hunt Data Augmentation (THDA) – inserting arbitrary 3D target objects in the scene – to augment the set of training episodes. Table 4c reports results on the MP3D VAL split for several metrics. </p>
<p class="text"> An agent trained on 10k Gibson-Gibson demonstrations combined with 60k MP3D demonstrations achieves 33.9% success and 9.7% SPL (row 11) An IL w/ RGBD + Semantic Input (27.8%) achieves 27.8% success (row 1), i.e. no RGBD and semantic inputs, the approach fails to learn anything (0% success, 0% SPL) They plot VAL success against the size of the human-demonstrations dataset in Fig. 1b. They also benchmark human performance on the.ophobicMP3D VAL split – 93.7%.<br/> They report results in Table 3 across three evaluation splits. IL behaves like supervised learning (as expected) with improvements coming from long training schedules. IL requires 7x fewer unique steps of experience to outperform RL. RL gathers unique agent-driven trajectories on-the-ﬁy. The IL agent learns from a static.dataset vs. an RL agent that gathers unique trajectories. They generate a larger dataset of 25.7k episodes roughly matching the cumulative steps of. experience with human.demonstrations. </p>
<p class="text"> Training on 9.5k human demonstrations achieves 17.5% success, 9.8% SPL on new object-receptacle initial-izations (row 2) Agents trained with IL on human demonstrations have higher coverage (both occupancy and sight), peeking behavior, panoramic turns, and exhaustive search than RL agents. They also trained an RL policy with the exploration and distance-based rewards from, but it failed to get beyond the expectations of the RL policy </p>
