<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Lower bounds for uniform read-once threshold formulae in the randomized decision tree model</h3>
<h3>Lower bounds for uniform read-once threshold formulae in the randomized decision tree model</h3>
<img src="abstract.png">
<p class="text"> A read-once threshold formula can be deﬁned by a rooted tree, every internal node of which is labeled by a threshold function T n-k (with output 1 only when at least two of the three inputs are 1) They focus on the randomized decision tree complexity of such functions. They prove lower bounds of the form c(k, n)d, where d is the depth of the tree. They also treat trees with alternating levels of AND and OR gates and show asymptotically optimal bounds. </p>
<img src="abstract.png">
<p class="text"> A simple randomized framework that can be used to compute both majd and nandd is the following. Start at the root and, as long as the output is not known, choose a child at random and evaluate it recursively. Algorithms of this type are called directional. For majd the directional algorithm computes the output in a single query. It was noted by Boppana and also in that better algorithms exist for majd. In-terestingly, Saks and Wigderson show that the directional algorithms is optimal for the nandD function. </p>
<p class="text"> In this section they introduce basic concepts related to decision tree complexity. The reader can ﬁnd a more complete exposition in the survey of Buhrman and de Wolf. A randomized decision tree QR computes a Boolean function f (with zero error), if p(Q) > 0 only when Q computes f. The cost of Q on input σ, cost(Q; σ), is the number of variables queried when the input is σ. The depth of a tree is the maximum distance of a leaf from the root. </p>
<p class="text"> The method of generalized costs for read-once formulae is to prove a lower bound on the expected cost of any randomized decision tree. The crucial part is to show that they may charge Q′™R more for each query, while maintaining the cost of a cost-function. In the next subsection they recall some deﬁnitions from Wigderson In this section they discuss how to test the reliability of a decision tree QR that computes a uniform threshold read-Once function of depth-d. </p>
<p class="text"> The expected cost of a randomized decision tree QR on input σ under cost-function φ is the corresponding distribution over deterministic decision trees. They will mostly work with simple cost functions in which φ(σ, z) depends only on σz. The main ingredient in this framework is the shrinking process, which entails removing n leaves V = {v1, v2, v3, vn,... The method and some preliminary deﬁnitions are discussed in this article. </p>
<p class="text"> The goal is to determine the “most expensive” cost function c′ for T ′ for a decision tree. To that end, it will be useful to express costµ′(Q′; c′) in terms of Q and T. For an assignment σ and a leaf w, let b = T n.n.k (σv1,..., σvn)and a cost function ψ for T as follows. The distribution of σ as generated by Q′ is µ. Furthermore, each σ is encountered k and n − k + 1 times when σ′ is 1 or 0 respectively. </p>
<p class="text"> For a leaf or cylinder z, let nz denote its length (the non-stars) and kz the number of the stars. For every leaf z of a decision tree over Xk,n with cost Pη(k, n), with no cost P(k) or P(K, n) P(P) is 0 outside V. They claim that the cost of Q′ is less than that of an optimal decision tree Q′ (Q) will gain −η whenever a tree queries a 1 and also gains +η, but with positive probability will pay 0 when it queries a 0 and pays 1. </p>
<p class="text"> Lemma 11: If P(k, n) < P (k − 1, n − 1), then (1, ∗,... (1) is a cylinder of any optimal decision tree for P(K, n), or an algorithm Q′ with negative cost. The proof is by induction on n. Theorem 12: For 0 < k < n, P('k), n), P('n), n('k) = 0; P'(k) is 0 for η = P(n) and P'n/n = k + 1; Theorem 14: For 1 < k, n, k+1: k−1 </p>
<p class="text"> Cost(Q; cη) = 0, the cost of Q given that the ﬁrst bit was a zero is exactly a zero. Corollary 10 determines the optimal decision tree for P(1, n) It queries the variables one by one until a one is found. For 1 ≤ k < n, n−kophobic2k ≤ P(k, n). The inequality is strict, unless k = 1 or k = n − 1 or n = n1. </p>
<p class="text"> A simple directional algorithm evaluates a node by evaluating its children in a randomly chosen order. The algorithm will evaluate all zeroes, except those placed at the end. They show that the Theorem 1, by showing that the directional algo-rithm for the alternating AND-OR tree satis the same recurrence as the one they obtain with the method of generalized costs. They then prove that if there is a decision tree Q for F 2d, C 2d and a cost of C, C, there is no lower bound. </p>
<p class="text"> Theorem 3.2: Theorem. Theorem: A single variable of cost C under the cost-function of a single variable is C. The largest eigenvalue, R(F 2d) = Θ(λd), is the largest eiginevalue of the matrix AB. The bound for any d is follows via the recurrence relations (8). Theorem. 3: C: A decision tree is a decision tree for a single decision tree. It follows the method of generalized costs with generalized costs. </p>
