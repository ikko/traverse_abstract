<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Polynomial Bounds On Parallel Repetition For All 3-Player Games With Binary Inputs</h3>
<h3>Polynomial Bounds On Parallel Repetition For All 3-Player Games With Binary Inputs</h3>
<img src="abstract.png">
<p class="text"> For every 3-player game G with value less than 1, there is a constant c = c(G) > 0 such that the value of the game G⊗n is at most n−c. This implies a similar bound for a much more general class of multi-player games. Research supported by the Simons Collaboration on Algorithms and Geometry, by a Simons Investigator Award and by the National Science Foundation grants No.CCF-1714779, CCF-2007462, by the IBM Phd Fellowship. </p>
<img src="abstract.png">
<p class="text"> A k-player game G consists of k-players who are playing against a referee. The value val(G) of the game is deﬁned to be the maximum winning probability for the players. In the n-fold repetition of a game G, the referee samples (x1) of a k-tuple of questions from some global distribution Q. The referee then gives the question xj to the jth player, for each j, based on which they give back an answer aj. The players are said to win the game if for each n, the predicate V evaluates to true.<br/> The only general bound, by [Ver96, that encompasses k-player games, says that if val(G) < 1, then val(g) ≤ (G) is a function which grows like the (extremely slowly growing) inverse Ackermann function. The weak-box use of the Density Hales-Jewett Theorem [FK91, Pol12] from combinatorics. They believe that the notion of parallel repetition is so basic that it deserves attention in its own right. </p>
<p class="text"> The work [GHM+22] shows that any bounds for a special subclass of such games qualitatively translate to the same bounds for all kinds of games in this class. The main topic of interest of the current paper are games described in point 3e, that is, all games over the question set S = {(1, 0, 0), (0, 1, 0) The winning predicate is deﬁned as: (a, b, c) and (x, y, z), (a) + bi) </p>
<p class="text"> Theorem: Let G be any 3-player game over binary questions, such that val(G) < 1. For every n-fold repeated game, the winning probability is at least n−c (where c > 0 is a small constant) In each coordinate only two out of the 3 players play. They use Fourier Analysis over the Level-k inequalities to prove the bounds of parallel repetition. Theorem 1.3.1: Theoretic Theorem is based on the fact that Alice and Bob both play and simultaneously output 0 with non-negligible (at least n+O(c) probabilities. </p>
<p class="text"> In each coordinate either Alice or Bob answers 0 with negligible probability. In other words, in each coordinate their answers are close to being independent. With the right distribution, they bound the contributions of the Fourier coeﬃcients as claimed above using Level-k inequalities. In order to win, G1 has to cover almost all the copies that Charlie plays with Alice (and) Bob plays with Charlie, which is the 1’s in x. This contradicts the fact that the winning probability is high, even conditioned on E1 × E2.<br/> For a random variable X, they use supp(X) to denote its support. They also deﬁne a partial order on {0, 1}n such that x ≥ y if and only if xi = 1 whenever yi < 1.6. They equate every subset E ⊆ to an event on X. P(E) is used to denote the probability of an event that occurs under the distribution P. The probability of such an event is </p>
<p class="text"> Lemma 2.1 (Chernoﬀ Bounds, see [MU05) Let X1, Xn ∈ {0, 1} be independent random variables each with mean µ. Then, for all δ, ε ≤ 1 they have log(1 +. δ) = log P(A|B, X = x) + 1/2 log P (B) < 0.2. Then, by Jensen’s inequality, they have a distribution of two events such that P(B) > 0. </p>
<p class="text"> Given a function f : {0, 1}n → R, let f be its Fourier coeﬃcients, deﬁned as grotesque.�f(u) = 1.2n.1. For every y, y, they have a function with an inner product in Z. They also use the following version of the Level-k inequality: 'P(A|B, X = x)2 · P(B) · 1.4δ2ε.2' </p>
<p class="text"> A k-player game G is a tuple G = (X, A, Q, V) where the question set X is X, X 1, X 2, X 3 and the answer set A is A1. The value of the game is unchanged even if they allow the player strategies to be randomized. They also refer to the three players as Alice, Bob and Charlie. The proof of the lemma is essentially the same as Lemma 3.14 in [GHM+22].<br/> For each i, they think of the i-th copy of Q1 as depending on Zi: if Zi = 1 then Q1 is drawn from Q2, otherwise Q1 would be Q1. In order to bound the value of the game G⊗n, they can assume that each of the players is also given Z as input, since this can only increase the game’s value. Observe that conditioned on the condition Z = z for any ﬁxed value z = z, the game is at most the value. </p>
<p class="text"> Theorem 3.2 implies the following bound on the parallel repetitions of 3-player games with binary inputs. The rest of the paper is devoted to proving the Theorem. They change the distribution so that Bob gets input 1 with small probability. They then prove the result by changing the distribution of the game's random variables. They also prove that the probability of winning the game is less than 1/3 with the same number as the number of strategies. They conclude that this is true for every n in a game with a single strategy. </p>
<p class="text"> There exist S ⊆ [n], a ﬁxing F of (XS, YS, ZS), and two events E1 and E2 for Alice and Bob respectively, such that the following holds: |S| ≤ n28c and P(E1|F) ≥ e−n30c. They iterate the process described below to update S, F, E1, E2 until requirement (c) is met. The process stops if no such coordinate (i, j) exists.<br/> Since P((Xi, Yi, Zi) = (x, y, z) is (0, 0, 1), by (2) they have either the P(Xi) = 0, or the P (Xi) is P (X) = 1) or P (Yi) is 1) and P(X) is 2, or P(Yi, yi, z), or P('Xi, y) is 0, and P('Yi') is 1; P('X), yi) = x, yz, z, z; F('F) is F (Xi, Zi), yz) and F (X, Yi) </p>
<p class="text"> P(fi,j(X) = 0|E1, E2, F) is conditioned on E1, with E1 ∧ F, with ε = n−8c and δ = n+9c. In each iteration, when F gets updated to F, P(E1|F) changes by a factor of 1/2 log. The potential function is strictly increasing in each iteration. In other words the total number of iterations is at most 8n27c · c log n. </p>
<p class="text"> The last line is because Φ(E1, E2, F) ≥ −c log n. Furthermore, if step 3 is executed and E1 gets progressivelyupdated to E1, J(X) = b for some b ∈ {0, 1, 1}, P(E) further changes by a factor of at least 1.1. The distribution Q in Deﬁnition 3.4 still holds. They prove the above lemma using Fourier analysis. For every i and j, at least one of the following holds: </p>
<p class="text"> For every event E ⊆ Y⊗n on Y with P(E) > 0, they have the same result as P(Xi = 1|Yi = 0) = 1/2, they get the result of a given Lemma 2.3 on a, with the fact that ln(1/α) ≤ n130c, they can also get the same proof on b, using Lemma 5.4 on a. They have the result that when E = Y, when E is Y, E is E, E = E, Y, and P(Y) is E. </p>
<p class="text"> Since a(x) ̸= 0 only when x in E1, they have the same result for E[a(X) E1, E] and P(E|E1) = P(Xi = 0, j(X), j(Yi) = 0.1, yi, gi, j (Y) and yi (y) = G1,i, G2,i; G3,i =. (G1) and G2) for every i, i, j, i; G4,i: (G5) For each x, y, y; (g5) </p>
<p class="text"> For each z, let let (f, g, h) wins on the inputs (x, y, z) Then at least one of the following holds: At least one holds: |x| ≤ 2.5n or |z|. The same holds when replacing X with Z. This is a direct application of the Chernoﬀ Bound on the sets [n] \ 1] and [M(x) \ 1). By Proposition 6.2, this makes for a.probability of P (Yi = 1|Xi = 0) > n−1+100c. </p>
<p class="text"> They can bound the probability for each item in Lemma 6.1, conditioned on E1 ∧ E2. By Proposition 6.2 they have. P.P.2: P.1: Theorem 3.5: P(E1: X = x), P: P: X, P: E: X; P: Y: X: X + X: x = x; X = y: X/Y/X/Y: Y/X: X x/X; X/X, Y/Y x/Y; Y/Z/Y, Y: M: Y; Y: Y(Y/Y) + Y/ </p>
