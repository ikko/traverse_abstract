<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Data-Centric Green AI: An Exploratory Empirical Study</h3>
<h3>Data-Centric Green AI: An Exploratory Empirical Study</h3>
<img src="abstract.png">
<p class="text"> Researchers from Vrije Universiteit Amsterdam, The Netherlands, University of Bristol, U.K., U.S. and France published an empirical experiment on energy consumption of AI models. Authors say data-centric approaches can be utilized to improve AI energy efﬁ-centric techniques. Authors conclude that by changing the algorithm used on datasets, energy consumption can be drastically reduced (up to 92.16%), often at the cost of a negligible or even a negligible accuracy decline. Authors also suggest that by tweaking the model training strategy, two orders of magnitude savings can be achieved.<br/> In this study, they conduct an exploratory study on the intersection of Green AI and Data-centric AI. They investigate the potential impact of modifying datasets to improve the energy consumption of training AI models. They also analyze the inherent trade-offs between energy consumption and performance when reducing the size of the dataset – either in the number of dataset points or features – in the case of Random Forest. The results show that feature selection can reduce the energy-consuming model training up to 76% while preserving the performance of the model. </p>
<img src="abstract.png">
<p class="text"> This is the ﬁrst study to explore the potential of preprocessing data to reduce the energy consumed by AI. They argue that more research in Data-centric AI will help more practitioners in developing green AI models. The main outputs of the research aim at providing developers with informed advice on how to design, develop, and deploy their systems – While traditional (Red) AI only aims to improve accuracy metrics, Green AI includes (computational cost) cost as a performance metric.<br/> The study paves the wayin directly addressing AI sustainability concerns by providing evidence on how dataset modiﬁcations can be used to drastically save AI model training energy at a negligible accuracy loss. Martin et al. focused on studying the energy consump-tion of a machine learning algorithm, namely the Very-Fast Decision Tree (VFDT) The authors analyzed the energy consumption of VFDT at the function level, investigating how different parameters affect all kinds of parameters. Their results demonstrate how function-level energy can lead to improvements. </p>
<p class="text"> Researchers adopt a blocking factor, namely the factor AI-algorithm (IV1) This entails that sub-experiments are divided into different blocks according to the AI algorithm. To answer RQ1, they vary the number of data points (IV2) and number of features (IV3) to their default level. This allows to study the impact that the size of the dataset has on the energy consumption of each algorithm (DV1), while avoiding variation of experimental measurements due to different numbers of features. To mitigate this threat, they perform a dummy CPU-intensive warm-up operation.<br/> The set of AI algorithms (IV1) was chosen by considering the most popular ones. The energy consumption (DV1) is the dependent variable used to answer RQ1 and RQ2. The size of the number of data points and features (10%) was in-stead adopted to ensure sufrencient granularity of results. The F1-score is chosen over precision (P) and (R) metrics, as it allows us to gain an encompassing overview of the overall accuracy of the AI algorithms, while overcoming potential representation problems. </p>
<p class="text"> The entirety of the experimental experiment and data analysis is implemented in Python 3.10.7.7. In total, 3.6K experimental runs are executed to gather data to answer research questions. All sub-experiments are run on a computer equipped with a 2.4GHz Quad-Core i5 processor and 2133 GB 2133 MHz LPDDR3 of memory. They use the Python package scikit-learn-learn1.0, and use the standard hyperparameters as deﬁned in the repository.<br/> RQ2 aims to investigate if dataset modiﬁcations, the number of data points (RQ2.1) and the amount of features may have an impact on the energy consumed by AI algorithms. Energy consumption between algorithms varies between algorithms from a minimum decrease of 20% to a 99.49% decrease in energy consumption (Random Forest KNN) Figure 2 shows the median energy consumption of each algorithm at a varying number of points (reported on the x-axis) </p>
<p class="text"> Energy Consumption (Joules) is based on the energy consumption of the different algorithms when using different data points (ﬁrst row) and when using a different number of features (IV2) The energy consumption was compared to energy consumption using different algorithms (DV1) and the number of data points used by Decision Tree, and AdaBoost) The algorithms were used to analyse energy consumption between the two algorithms. The results show that the algorithms are more efficient when using more data points than the algorithms (IV1)<br/> The number of features and the energy consumption of AI algorithms appear to be correlated for most algo-protein algorithms. The relationship is conﬁrmed by the Spearman’s rank-ranking correlation coef ﬁcient values reported in Table I. For all algorithms other than KNN, the energy reduction achieved by varying the number of. features results to be lower than the one obtainedable by varying. the one obtainable by. varying the. number of data points, they generally do not observe a. notable F1-score decrease. </p>
<p class="text"> The algorithm with the least energy-intensive energy consumption is KNN, using almost 200× less energy than Random Forest. The only exception are Decision Tree and Random Forest, both reporting a strong correlation between number of features and F1-score. KNN is the only algorithm which reports a ρ value indicating a very strong correlation. The energy-consuming algorithms were the most energy greedy, AdaBoost, SVM and Bagging Classiﬁer were themost energy greedy algorithms.<br/> The energy consumption of ensembles is affected by the number of weak learners being used internally and their individual energy consumption. KNN (KNN) and Random Forest Energy (Forest Energy) all yield less energy when reducing the dimensionality of the dataset. The difference goes up to 99.49% energy consumption decrease, with KNN the most energy-efﬁcient algorithm. They need simple metrics to approximate energy consumption, such as CPU usage, CPU usage and number of CPU operations. </p>
<p class="text"> They argue that such strategies have a potential on Green AI that has been overlooked in previous research. Instead of collecting the biggest amount of data, they must aim for a smaller but meaningful datasets. This work paves the way to study other properties of the input data. They foresee potential in studying techniques to democratize Green data-centric Green AI. For example, the widely utilized library adopted for this study, it automatically converts all data to 64 bits (as of version 0.24.2). Users have no way of intervening in this data transformation.<br/> Several AI-leading organizations are aiming to be carbon-free by 2030. This requires massive investments in infrastructure and is far from being a realistic norm for the rest of the AI industry. The results show that, with very simple techniques available to any AI practitioner, one can effectively reduce the carbon footprint of developing AI models. For example, previous work on Green AI bring awareness to the importance of using energy-efﬁcient hardware, datacenters with better access to clean energy, etc. </p>
<p class="text"> RQ2: Extracting smaller datasets is a great opportunity to reduce the energy con-sumption of the machine learning models. KNN shows a strong positive correlation between the F1-score and the number of features in the dataset. Random Forest consistently yields the best performance. Despite being the most energy-greedy algorithm, it trained the most accurate model. Other algorithms, AdaBoost, Bagging Classiﬁer, and Decision Tree follow close behind, showing competitive results.<br/> In the vast majority of use cases, decreasing the number of data points / features dramatically reduces energy consumption. However, this observation does not hold for all algorithms. They discuss the threats to validity of the study by following the categorization provided by Wholin et al. A threat to conclusion validity in the study could be caused by low statistical power of the tests used to answer the RQs. To mitigate this threat, they systematically collected and analyzed data by following a process deﬁned a priori. Byconsidering the combination of factor, treatments, and reruns, they used a total of 3.6K data samples. </p>
<p class="text"> Aims to ensure that all of the data is accurate and accurate, they need to be able to use the data to make sure they have the data accurate. The data is based on the fact that the data should be used to make the data available to the public. The data should not have been used in the way of making the data necessary to make it more accurate. They need to use it as a tool to identify the data that is consistent with the expectations of accuracy and reliability. They are happy to provide an accurate and thorough look at the data.<br/> With this study, they aim at exploring "Green AI" from a novel angle. They investigate if modifying exclusively datasets, rather than the model training strategies, can optimize AI energy efﬁciency. While AI accuracy may be negatively impacted by data-centric strategies, they also observed that in most cases such accuracy loss is negligible. The results show that often “designing for less” while processing a dataset can drastically reduce the energy consumed, while not sacri-cing accuracy. </p>
<p class="text"> A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, “Quantifying thecarbon emissions of machine learning,” 10 2019 [Online]. Available:https://://arxiv.org/abs/1906.02243;. D. Amodei and D. Hernandez, ‘AI and compute,’ Open AI, 2018. Available: https://openai.com/blog/ai-and-compute/<br/> Researchers from Empirical Software Engineering, vol. 24, no. 4, is discussing energy impact of refactoring code smells. They also discuss the use of machine learning to improve performance and energy consumption in hpc systems. Their findings will be published at the 2019 IEEE/ACM 41st International Conference of Software Engineering: New Ideas and Emerging Results (ICSE-GMTNIER), 2019, pp. 101–104. The authors also discuss how to use AI to improve energy consumption and performance.<br/> A. Garcia-Martin, N. Lavesson, and H. Grahn, “Identiﬁcation of energy.hotspots: A case study of the very fast decision tree,” Green, Pervasive, and Cloud Computing, pp. 267–281, 01 2017. “Sustainable AI: Environmental.implications, Environmental.challenges, challengesand.opportunities,’10.20212021[Online]. “Doing more with.apologeticless: characterizing dataset downsampling for automl”<br/> Researchers: “Aligning artiﬁcial intelligence with climate change mitigation mitigation” “Jevons’ paradox,” Ecological economics, vol. 54, no. 1, 2005. “Coresets via bilevel optimization for continual learning and streaming” in Advances in Neural InformationProcessing Systems, H. Larochelle, M. Ranzato, R. Lago, C. Ebert, and C. De Vries, “Greenminer: A hardware based mining software. ‘repositories software energy consumption framework,’ in Proceedings of Machine Learning Research, ‘PMLR,<br/> The article is published in France's Archives-ouvertes. [Online]. Available: http://www.hal.com/hal-03368037. The author of the article is also known as the author of a book entitled, “Le Monde’s,”‘Le Moutier’. The author also published a book, ‘Le moutier.’,‘Moutier: ‘The author </p>
