<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods</h3>
<h3>A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods</h3>
<img src="abstract.png">
<p class="text"> Multi-task learning (MTL) has become in-creasingly popular in natural language pro-genrecessing (NLP) It is still not understood how multi-task-learning can be implemented based on the re-provelylatedness of training tasks. They discuss future directions of this promising topic in a survey of machine learning. They review recent approaches and discuss how to design and train a single model to perform a single task according to task relatedness. They provide an overview of optimization methods in MTL and multi-step training. </p>
<img src="abstract.png">
<p class="text"> Two major multi-task training methods in NLP can be categorized according to task relatedness, according to the types of training methods. Joint training is commonly used when all the task-speciﬁc data can be performed simultaneously. Multi-step training is used when some task’s in-demand needs to be determined by the outputs or hidden representations of previous task(s) In joint training, model parameters are shared (either via soft or hard parameter sharing) among encoders and decoders so that the tasks can be jointly trained to bene-t from shared rep-resentations, as shown in Figure 1.<br/> Multi-task frameworks are composed of three components: (a) en-coder layer (including embedding layer), (b) de-apologetic) and (c) optimization layer. In NLP networks, an embedding-layer is usually applied to generate the embeddingvectors of the basic elements of the input X(i) For the j-th task, the encoder layer learns the hidden-state of X(I) and the de-centriccoder layers learn the hidden state. Popular encoder modules include BiLSTM and BERT (Devlin et al., 2019). </p>
<p class="text"> Zhou et al. (2019) predicted the type of rela-centriction mentioned in a sentence by the RE decoder. Zhou and Zhou (19) predicted a relation tag for every pair of tokens. If the decoder performs the task, it can identify any number and any types of relations. However, the complexity of the complexity is too high to be effectively trained with annotated data. The tasks share the utterance and the intenthave been usually jointly trained (Goo et al., 2019; Qin et al.2020).<br/> Multi-task-like learning plays an important role in leveraging po-centrictential correlations among related classiﬁcation tasks to extract common features, increase cor-centricpus size implicitly and yield classi-centricimprovements. Multi-Task-like training can be applied to machine-translive machine translation (NMT) Neu-ophobicral machine translation is the most im-portant application. Multilinguality between multiple languages motivates multi-task learning on multilingual data. </p>
<p class="text"> They list recent approaches of multi-step training in the ﬁeld of NLP. These include language understanding, multi-passage questionanswering and natural language generation. Multi-view learning is also applied in NLG ap-guiproaches for auxiliary learning objectives. The potential for leveraging multiple levels of rep-re-resentation has been demonstrated in various ways in various. They discuss the approaches to multi-level language understanding and multi-stage training in N.LP.<br/> Reading is to extract multiple answer-span candidates from the retrieved set of relevant passages. Answer reranking (AR) is to re-score multiple answer candidates based on the question and evidence passages. Evidence passages are gener-ishlyated by PR and fed into RC as input; the answer-reranking candidates are generated by RC and given into AR as input. Hu et al. (2019) proposed a typi-orativecal approach called RE3 (for REtriever, REader, re-rankinger, and REranker) The retriever used TF-IDF to prune irrelevant passages. </p>
<p class="text"> Fthe multi-step training NLP applications discussed at §3.2.2 (2nd and 3rd subﬁgures) and§3.3.4.1 (Joint Training) with similar tasks is the classical choice for multi-task learning. They look at task relatedness between the sub-tasks in this section. They also look at the tasks with self-supervised objectives for language model pre-training. Inition, (Guo et al., 2019) used ad-armed bandits to select tasks and aussian process to control the mixing rates.<br/> Partial sharing of model parameters is the mainstream approach to multi-task learning. Liu et al. (2017) adds an adversarial task via a discriminator to estimate what task the encoding sequence comes from. Such a strategy prevents the shared and private la-uvetent feature spaces from interfering with each other. By reversing the gradient of the task, the task-loss is maximized for the main task, which forces the model to learn representations that are indistinguishable between domains. This setup has also received success in multitask-focused training for domain adaptation. </p>
<p class="text"> In BERT pre-training, the next sentence prediction task is used to learn sentence-level representations, which is com-plementary to the masked language model task that focuses on word-level contextual represen-tations. The candidate docu-centricments provided by the “retriever” serves as condi-centrictions to the downstream ‘reader”, which narrows the search space and thus reduces the difﬁculty of predicting the output.<br/> The research direction is known as “Knowledge-enhanced NLP” (Yu et al., 2020c) One could devise learning tasks informed by the knowledge so that the model is trained to acquire and utilize exter-centricnal knowledge. The knowledge-related tasks can be combined as a multi-task setting, resulting in a multi task-learning setting. In this section, they will discuss some promising future directions regarding pre-training tasks or train-training methods of multitask training in NLP. </p>
<p class="text"> The development of adaptive adaptive sharing methods to improve param-proper adaptive sharing in multi-task NLP is needed. They discuss how to train a multi task model and how to do it with pre-training and pre-tuning. They also discuss the development of new training methods to help NLP neural networks with multimanimaniman tasks. In the future, they will use multi-tasking to learn how to group tasks into a single task or group tasks. They will also explore how to share parameters in training a universal model of tasks.<br/> Multi-task supervised learning helps the model fuse knowledge from different domains and generalize to different downstream tasks. Liu et al. (2019) uniﬁed the input-format of GLUE tasks to feed into a single model before ﬁne-tuning on individual tasks. Future research may dive deeper into the bene-benebenebras of the transfer learning paradigm, including the choice of properly including MTL in pre-training or training for better generalization. </p>
