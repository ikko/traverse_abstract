<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>A survey on recently proposed activation functions for Deep Learning</h3>
<h3>A survey on recently proposed activation functions for Deep Learning</h3>
<img src="_sum_2204.02921.html.1.png">
<p class="text"> A survey on recently proposed activation functions for Deep-Learning proposes activation functions. Neurons, inputs, weights, biases, biases and functions are essential components of neural networks. Activation functions are a crucial sub-particular sub-section of neural network. This survey discusses the main concepts of activation functions in neural networks, including a brief introduction to deep-learning neural networks and how they are used in the network. It also discusses some of the challenges, limitations, limitations and alternative solutions. </p>
<img src="abstract.png">
<p class="text"> An activation function is a nonlinear function used to train a neural network by deﬁning the output of a neuron given a set of inputs This function is used in every neuron of the network to help the network learn the complex patterns in the data, allowing it to make predictions. The purpose of activation functions is to introduce nonlinearities into the neural network. An example of an activation function that mathematically transforms the output into the predicted class label of the input data is shown below. </p>
<p class="text"> The tanh function inherits all the valuable properties of the sigmoid function. Tanh function is continuous, differentiable and bounded, and ranges between -1 and 1. ReLU function is not exponential, thus, it has low computational cost as it forces negative values to zero. A variant of the ReLU, called LReLU, was introduced in an attempt to solve the dead neuron problem. PReLU can be generally used to express the result of non-linearities. The leaky ReLU (LReLU) function is a continuous, not-bounded, zero-centered function. </p>
<p class="text"> Swish function outperforms ReLU on deeper models across a variety of large datasets. Mish function is a smooth, continuous, non-monotonic activation function inspired by Swish’s self-gating property. Mish has been proposed as a replacement for novel activation functions, including Swish, which is a more robust activation function with signiﬁcantly better results than ReLU. Mish is bounded below and unbounded above, similar to Swish. Mish eliminates the necessary preconditions for the Dying ReLU problem by retaining a small quantity of negative information.<br/> Mish outperforms Swish, ReLU, and Leaky ReLU in terms of empirical data. Mish has a larger minima than ReLU and Swish because the former contains several local minima. Mish is constantly differentiable, thus avoids singularities, thus, unwanted side effects when doing gradient-based optimization. As compared to the networks equipped with ReLU or Swish with the lowest loss, Mish had the lowest. Mish’s smooth proﬁle also </p>
<p class="text"> Growing Cosine Unit (GCU) has the advantages of using oscillatory activation functions to improve gradient ﬁow and alleviate the vanishing gradient problem. The GCU function is deﬁned as a function that is computationally cheaper than the state-of-the-art Swish and Mish activation functions. Biological neurons in the second and third layer of the human cortex have oscillating activation properties and are capable of individually learning the XOR function. GCU activation also reduces training time and allows classiﬀcation problems to be solved with smaller networks.<br/> In this paper 4 new oscillatory activation functions that enable individual artiﬁcial neurons to learn the XOR function like biological neurons are proposed. The functions outperform popular activation functions on many benchmark tasks, such as solving classi ﬁcation problems with fewer neurons and reduced training time. The fthe oscillatory functions presented in the paper are presented as the following: f(z) = z2 + z2 (sinc (z − π) + sinc) </p>
