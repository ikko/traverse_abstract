<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>MAESTRO: Matched Speech Text Representations through Modality Matching</h3>
<h3>MAESTRO: Matched Speech Text Representations through Modality Matching</h3>
<img src="_sum_2204.03409.html.1.png">
<p class="text"> Maestro learns uniﬁed representations through sequence alignment, duration predic-ativetion and matching embeddings in the learned space through aligned masked-language model loss. The algorithm can transfer to diverse downstream tasks such as Automated Speech Recognition(ASR) and Speech Translation (ST) They establish a new state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 11% relative reduction in Word Error Rate (WER) and 21 languages to 21 languages.<br/> A novel modality matching algorithm, Maestro, that can effectively use small amounts of transcribed speech data to unify representations learnt from unlabeled speech and text. A new algorithm to use additional sources of supervision, Machine Translation (MT) and Speech Translation (ST) to improve multilingual joint representations learnt during pre-training. A new state-of-the-art (SOTA) on the VoxPopuli.-multilingual ASR task with a 11% relative reduction in WER and on CoVoST-2 21 languages-to-English speech translation with an absolute improvement of 2.8 BLEU. </p>
<img src="_sum_2204.03409.html.2.png">
<p class="text"> The proposed framework to pretrain one model from un-transcribed speech, unspoken text and any available labeled data (paired speech and text) is presented in Figure 1. The Text Encoder block uses an RNN-T decoder to explicitly resample text representations and to match speech encoder output es to the model. They also develop approaches to use cross-lingual supervision from MT and ST data during pre-training. The self-alignment process described below learns alignments from the model itself in an iterative fashion.<br/> The Resampler and Reﬁner layers replicate the initially learned text embeddings to match the duration of the speech embedding using this information and a Mean-Squared Error (MSE) training objective. When learning from unspoken text, the speech-text alignment information is unavailable. They replace the original original MLM/BERT loss used in prior work with the aligned-masked language model training objective (LA-MLM) This new objective allows for the use of the same RNN-T loss loss for both unpaired text (aligned embedding) and paired speech (speech embedding es, text) data. </p>
<p class="text"> The languages are sorted by the amount of data from high to low. Maestro closes the gap with LM shallow fusion(Row 3) and yields additional wins with LM fusion (last row). Both use a neural Conformer language model trained from Librispeech text corpora. The model is learnt from all the SpeechStew data while TTS model used in TTS4Pretrain 2.0 is only trained on Librispech. They believe this is the reason they see better performance.<br/> Figure 2 also compares the use of phoneme and grapheme targets during pretraining. Speech Translation (ST): CoVoST 2 X→En results on language pairs. Table 3 demonstrates that Maestro (Maestro) outperforms most of the previous systems except the larger, 2B model mSLAM model. Speech translation (ST) also advances SOTA results on the speech-translation CoVOST 2 benchmark. Table 4 shows that the proposed method can learn meaningful speech-text-textrepresentations even from small amounts of supervised data. </p>
