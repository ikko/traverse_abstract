<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Testing the limits of natural language models for predicting human language judgments</h3>
<h3>Testing the limits of natural language models for predicting human language judgments</h3>
<img src="_sum_2204.03592.html.1.png">
<p class="text"> Neural network language models can serve as computational hypotheses about how humans process language. They compared the model-human consistency of diverse language models using a novel experimental approach. For each controversial sentence pair, two language models disagree about which sentence is more likely to occur in natural text. Controversial sentences proved highly effective at revealing model failures and identifying models that aligned most closely with human judgments. The most human-consistent model tested was GPT-2, although it also revealed signiﬁcant shortcomings of its alignment with human perception. </p>
<img src="_sum_2204.03592.html.2.png">
<p class="text"> A computational model that predicts human behavior in predeﬁned benchmark tasks could still fail to match human judgments across a larger space of possible language inputs. They propose a systematic, model-driven approach for comparing language models in terms of their consistency with human judgments. They generate controversial sentence pairs: pairs of sentences designed such that two language-model models strongly disagree about which sentence is more likely to occur. They then collect human judgments of which sentence in each pair is more probable to settle this dispute between the two models.<br/> All of the nine language models performed above chance (50% accuracy) in predicting the human choices for the randomly sampled natural sentence pairs. RoBERTa, with an average agreement between models of 84.5%, had an average accuracy of 80.5%. To evaluate model-human alignment, they computed the proportion of trials where the model and the participant agreed on which sentence was more probable. Figure 1a provides a detailed graphical depiction of the relationship between sentence probability ranks for one model pair (GPT-2) </p>
<p class="text"> The 900 trials of randomly sampled and paired natural sentences provided no statistical evidence that any of the language models are human-inconsistent. GPT-2, RoBERTa and ELECTRA showed the best human consistency; 2-gram the worst. They evaluated the accuracy of each model’s prediction accuracy to the lower bound on the noise ceiling (Wilcoxon signed-rank test, controlling the false discovery rate at q <.05) All models except BERT, XLM, LSTM, RNN, and Electra performed signiﬁcantly below that ceiling.<br/> Examples of natural-sentence pairs that maximally contributed to each model’s prediction (model 1) “severely failed” In each case, the failing model 1 prefers n2, while the model it waspitted against (model 2) # human choices. Table 1 shows the results for the sentences on which the two sentences on the two models failed. In each of the models, the sentences were presented with sentences with sentences that prefer n2 and n1.<br/> When more than one sentence pair induced an equal maximal error in a model, the example included in the table was chosen at a random rate of at least 1.4%. The model was chosen when more than 1,000 sentences were induced by a sentence pair that induced a maximal error. The example included was chosen to be chosen at roughly random rates of 1/2/3.5% for the model to be given an equal amount of error in the model. For example, </p>
<p class="text"> They developed a procedure for synthesizing controversial sentence pairs in which naturally occurring sentences serve as initializations for synthetic sentences as well as reference points that guide sentence synthesis. GPT-2, RoBERTa and ELECTRA were found to be signiﬁcantly more accurate than the other models (BERT, XLM, LSTM, RNN, 3-gram, and 2-gram) in predicting the human responses to these trials (Wilcoxon signed-rank test), controlling the false discovery rate for all 36 model pairs at q <.05.<br/> If they adjust this sentence to minimize its probability according to RoBERTa, they obtain the synthetic phrase ‘You have to realize is that noise again’ If they instead decrease only 3-gram’s probability, they generate the synthetic sentence “I wait to see how it shakes out” ‘.’ ‘I’ve been here for a decade’. ‘We’ll be here again for a week. We’ </p>
<p class="text"> Testing the limits of natural language models except for GPT-2 were found to be signiﬁcantly below the lower bound on the noise ceiling (Wilcoxon signed-rank test) All nine models at q <.05 were tested at q =.05. Evaluating each model pair only on the controversial synthetic-sentence pairs only on that particular model pair yielded a similar model ranking (Fig. S4b) Examples of controversial synthetic sentences that maximally<br/> For each language model, the optimization unveiled nonsensical or nongrammatical sentences that were strongly favored by the model over more natural alternatives. Qualitatively inspecting these sentence pairs suggests that for each language. model, “nonsensical” or ‘nongramatic” sentences are more likely to be more natural. Examples of controversial synthetic-sentence pairs that contributed to each model’s predictionally contributed to. each model. For each ‘model’, the results for two sentences on which two sentences failed to. which failed the model failed are shown to be. severely.<br/> Twenty pairs of sentences for each model pair were presented to the participants, 10 for which the natural sentence was shown with one of the derived synthetic sentences. For each of these trials, a model that rates the synthetic sentence to be at least as probable as the natural. In each case, the failing model 1 prefers sentence s2 (higher log probability bolded), while the model it was pitted against (“model 2”) and all 10 human subjects presented with that sentence pair prefer sentence s1. </p>
<p class="text"> Model comparison using synthetic sentences. GPT-2, RoBERTa and ELECTRA signiﬁcantly outperformed the other models. This evaluation yielded a below-chance prediction accuracy for all of the models, which was also below the lower bound on the noise ceiling. This indicates that, although the models assessed that these synthetic sentences were at least as probable as the original natural sentence, humans disagreed and showed a systematic preference for the natural sentence. Synthetic sentences were evaluated across all. of the synthetic-natural-natural.sentence pairs for which it was targeted to keep the synthetic sentence at least. as probable. </p>
<p class="text"> Examples of pairs of synthetic and natural sentences that maximally contributed to each model’s prediction error. In each case 1 prefers synthetic sentences, while the failing model ‘failed’, “the failing model, ‘the model” was pitted against 10 subjects presented with 10 subjects that ‘sentence n’t prefer natural sentences’ Synthetic sentences with higher log probability (higher log probability) and ‘Natural Sentence Sentence’ (model 1) prefer synthetic sentences; model 2: ‘natural Sentence N’<br/> Instead of binarizing the human and model judgments, here they measure the ordinal correspondence between the graded human choices and the log ratio of the sentence probabilities assigned by each candidate model. They found GPT-2 to be the most human-aligned, followed by RoBERTa; ELECTRA; BERT; XLM and LSTM; and the RNN, 3-gram, and 2-gram models (q <.05 for all of the comparisons) All of the models were found to be signiﬁcantly less accurate than the lower bound on the noise ceiling. </p>
<p class="text"> They found that GPT-2, LSTM, RNN, 3-gram, and 2-gram models were signiﬁcantly more likely (vs. humans) to prefer low-coherence sentences, while ELECTRA, XLM showed a bias for high-frequency words. These results indicate that strong models like GPT -2 and ELECTRA can exhibit subtle misalignments with humans in their response to simple linguistic features, when evaluated on sentences synthesized to be controversial.<br/> GPT-2 performed the best overall, consistent with this model’s advantage in predicting ECoG and fMRI responses to natural language. RoBERTa, ELECTRA, and ELECTRA showed the best performance. These models are trained to optimize only word-level prediction tasks, as opposed to BERT and XLM which are additionally trained on next-sentence prediction and cross-lingual tasks. This suggests that local word prediction provides better alignments with human language comprehension. </p>
<p class="text"> Testing the limits of natural language models can be seen as a generalization test of language models. They predict that testing model-brain correspondence with controversial synthetic sentences designed to elicit distinct representations in different language models will reveal considerable discrepancies between GPT-2 (as well as the other off-the-shelf language models) and human-like representations. Even the simplest model they considered–a 2-gram frequency table–actually performed quite well on predicting human judgments for randomly-sampled natural sentences.<br/> Many of these methods introduce modiﬁcations that fail to preserve semantics [Morris et al., 2020]. Interactive (“human-in-the-loop”) approaches allow human subjects to repeatedly alter model inputs such that it confuses target models but not secondary participants. Testing NLP models on controversial sentence pairs does not require approximating or querying a human’s ground truth during optimization. Instead, by designing inputs to elicit con�icting predictions among the models, they capitalize on the simple fact that if two models disagree with respect to an input, one of the models must be making an incorrect prediction. </p>
<p class="text"> Using controversial stimuli can identify subtle differences in language models’alignment with human judgments, the study was limited in a number of ways. Future work can introduce (potentially adaptive) controversial sentence optimization procedures that consider large sets of candidate models, allowing for greater model coverage than the simpler pairwiseapproach. They measured only the sentence preferences of human participants, and so cannot directly assess whether these judgments were driven by semantic, grammatical, or other information. The results suggest that at least for the current language models, this full Information-Theoretic formulation may not be necessary.<br/> N-gram models [Shannon, 1948], the simplest language model class, are trained by counting the number of occurrences of all unique phrases of length N words in large text corpora. They tested both 2-gram and 3-gram variants of the N/N-gram language models. The N/PAN networks were trained with open source code from the Natural Language Toolkit [Bird et al., 2009] and the networks were implemented with the open-source repository HuggingFace [Wolf et al. 2020]. </p>
<p class="text"> They trained 2-gram and 3-gram models on a large corpus composed of text from fthe sources. The n-gram probability estimates were regularized by means of Kneser-Ney smoothing [Kneser and Ney, 1995]. They also tested two recurrent neural network models, including a simple recurrent-neural network (RNN) and a more complex long short-term memory recurrent network (LSTM) The models were trained over 100 independent batches of text for 50 epochs with a learning rate of.002.<br/> GPT-2, the second iteration of GPT OpenAI’s GPT model, is the only unidirectional transformer model that they tested. They then sought to compute the probability of arbitrary sentences under each of the models described above. The term “sentence” is used in this context in its broadest sense–a sequence of English words, not necessarily restricted to grammatical English words. The sentence-level probabilities were evaluated using each of these models. </p>
<p class="text"> For unidirectional models, evaluating sentence probabilities was performed simply by summing log probabilities of each successive token in the sentence from left to right given all the previous tokens. For bidirectional transformer models, this process was not as straightforward. They evaluated 100 different random permutations of word positions and deﬁne the overall sentence log probability as the mean log probability from each permutation. The distribution of log probabilities resulting from different permutations tends to center tightly around a mean value (for example, for RoBERTa evaluated with natural sentences, the average coef ﬁcientcient-of variation was approximately 0.059)<br/> Reddit comments were scraped from across the website and all unique eight-word sentences were saved. These sentences were subsequently ﬁltered to exclude spelling errors, inappropriate language, and individual words that were not included in the training corpus used to train the n-gram and recurrent neural network models in the experiment. They eliminated any candidate sentences for which no pair of models held (r(s | m1) < 0.5) and (r (s) | m2) + r(s) = 1 to the most probable one. </p>
<p class="text"> They aimed to select 360 controversial sentence pairs, devoting 10 sentence pairs to each of the 36 models pairs. They used integer linear programming (ILP) as implemented by Gurobi [Gurobi Optimization, LLC, LLC] to solve the selection problem in Eq. 2 as a standard ILP problem. They represented sentence allocation as a sparse binary tensor S of dimensions 85,749 × 360 × 2 (sentences, pair members) and the fractional sentence probabilities ranks as a matrix R of dimensions. Each synthetic sentence was created as a solution for a constrained minimization problem:<br/> When evaluating potential replacement words, they only considered words that were sufﬁciently prevalent and hence expected to be present across training corpora. This vocabulary was determined by intersecting the list of words in the subtlex database [Van Heuven et al., 2014] with the corpus used to train the n-gram and recurrent neural network. They excluded all words occurring less than 300 times in the latter corpus. This resulted in a list of 25,258 potential replacement </p>
<p class="text"> They evaluated the log-probability of 23,180 sentences resulting from each of the 23,000 possible word replacements. They then evaluated the true (non-approximate) sentence probabilities of the replacement word with the minimal predicted probability. For GPT-2, whose evaluation of log p(s | m) is slower, they evaluated sentence probabilities only for word-replacements. For bi-directional models (BERT, RoBERTa, XLM, and ELECTRA) they used a heuristic to prioritize which replacements to evaluate.<br/> They used integer programming to choose the 10 most controversial triplets from the 100 triplets optimized for each model pair. The selected 10 synthetic triplets were then used to form 30 unique experimental trials per model pair, comparing the natural sentence with the other synthetic sentence. For each model, there was exactly one natural sentence in each decile of the natural sentences probability distribution. They presented the controversial sentence pairs selected and synthesized by the language models to the Columbia University Institutional Review Board (protocol number:IRB-AAAS0252) </p>
<p class="text"> 100 native English-speaking, US-based participants (55 male) were recruited from Proliﬁc (www.prolific.co) and paid $5.95 per person. Participants were asked to make a binary decision about which of the two sentences they considered more probable (for the full set of instructions given to participants, see Fig. S1) The order of trials within each stimulus set as well as the left-right screen position of sentences in each sentence pair were randomized for all participants.<br/> For the upper bound (i.e., the highest possible accuracy attainable on this data sample), they included the subject themselves in this majority vote-based prediction. For each analysis, the false discovery rate across multiple comparisons was controlled by the Benjamini-Hochberg-Procedure procedure. The human Likert ratings were recoded to be symmetrical around zero, mapping the six ratings appearing in Figure S2 to −2.5, −1.5 and +1.2. They then sought to correlate the model log-ratios and with the zero-centered. </p>
