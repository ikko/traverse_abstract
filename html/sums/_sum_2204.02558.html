<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning</h3>
<h3>DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning</h3>
<img src="_sum_2204.02558.html.1.png">
<p class="text"> DouZero+: Improving DouDizhu AI by Opponent-Modeling and Coach-guided Learning. Recent breakthroughs have been made in perfect-information games such as Go, Shogi and Shogi. In this work, they propose to enhance DouZero by introducing opponent modeling into DouZero. They pro-pose a novel coach network to further boost the performance of DouZero and accelerate its training process. With the integration of the above two techniques, the AI system achieves better performance and ranks top in the Botzone leaderboard.<br/> There are thousands of possible combinations of cards where different subsets of these combinations are legal to different hands. Unlike Texas Hold’em, the actions in DouDizhu can not be easily abstracted, which makes search computationally expensive and the commonly-used reinforcement learning algorithms less effective. The performance of Deep Q-Learning (DQN) will be affected due to the overestimating issue in large action-space While policy gradient methods such as A3C fail to leverage the action features, limiting the capability of generalizing over unseen actions. </p>
<img src="_sum_2204.02558.html.2.png">
<p class="text"> DeltaDou is the ﬁrst bot that reaches top human-level performance. It makes use of an AlphaZero-like algorithm, which combines neural networks with Fictitious Play Monte Carlo Tree Search (FPMCTS) and an inference algorithm in a self-play procedure. DeltaDou pre-trains a kicker network based on heuristic rules to reduce the action space size, which may have a negative impact on its performance if the output of the kicker network is not optimal. They propose coach-guided learning to fasten the training of the AI.<br/> Traditional reinforcement learning methods such as DQN and A3C exhibit poor performance in poker games such as heads-up no-limit Texas Hold’em poker and DouDizhu poker. Traditional methods are computationally expen-uctive and depends on human expertise, limiting its practicability and performance. They note that this technique is also being adopted in some other game AIs, such as a modern board-game, Kingdomino, and a kind of new chess, Tibetan Jiuqi, </p>
<p class="text"> In the implementation of DouZero system, it makes use of a self-play procedure, where the actors play games to generate samples while the learner updates the network using these data. The input of the network consists of state features and action features. The state feature represents the information that is known to the player, while the action feature describes the legal move corresponding to the current state. For the architecture, a layer of LSTM is used to code historical moves and the output is concatenated with other state/action features.<br/> Following the practice of DouZero that trains three models for the three players in the game, they also train three prediction models for opponent modeling. The prediction model can be viewed as a multi-headypeclassiﬁer and outputs the probability of the number of every kind of card in the hand of the next agent. The architecture of prediction models is also similar to DouZero with a layer of LSTM to provide historical moves and shared layers of MLP. This model is trained using cross-repetration loss function. </p>
<p class="text"> In a shedding-type game, the players’ objective is to empty one’s hand of all cards before others. The quality of the initial hand cards of the three players has a great impact on the result of this game. They propose a coach network to identify whether the cards are balanced in their strength. The idea can also be transferred into the development of other similar game AIs and beneﬁts the current training process. For example, the DouDizhu AI system does not have a bidding phase as the bidding network in DouZero is trained with supervised learning.<br/> They conduct experiments to demonstrate the effectiveness of the improvement that they introduce to DouZero. All experiments are conducted on a server with 4 Intel(R) processors and GeForce RTX 2080Ti GPU. They make use of the open-source models of DouZero as the opponent. They also follow DouZero and use the evaluation metrics, such as the average difference of points scored per game between algorithms A and B, where the base point is 1 and a bomb will double the score. To be speciﬁc, for two competing algorithms, they will play as Landlord and Peasants, respectively, for a given deck. </p>
<p class="text"> The evaluation metrics of WP and ADP can be utilized when deﬁning the reward. For WP, the agent winning a game is given +1 reward otherwise -1 reward. ADP. can be directly used as rewards for ADP settings. For Peasants, the state features utilized by DouZero contain all the information that can be known. The information about the hand cards of the next player. is included implicitly while the idea of opponent modeling. is essentially making such information explicit. In order to determine whether such an idea helps the agents learn better, they make a pre-experiment where they add the hand. cards of next player into state features directly.<br/> The results show that introducing opponent modeling to DouZero mainly improves the performance of models of Landlord against Peasants. The network has to take more features as input and has more neurons, which will slow down learning, but the network manages to grasp more knowledge after enough training and achieve a performance better than DouZero. In other words, the method can be migrated into other environments, helping game AI achieve better performance. The training procedure is discussed in Section IV-A and the upper limit of threshold β is set to be 0.3. </p>
<p class="text"> They upload the model to BotZone, an online platform with DouDizhu competition. They pick some cases from games from Botzone to show the predicted results of “coach network” and also show the actual result from the view of the Landlord. They also add the result of just using ‘coachnetwork” in the ﬁgure. It predicts the winning probability of Landlord based on the initial hand cards of the three players. The Peasant win the game. However, the balanced samples can indeed help the agents learn cautious policy and cooperation better.<br/> Researchers put forward some improvements to the state-of-the-art DouDizhu AI program, DouZero. Inspired by the human player’s prediction about opponents’ hand cards, they introduce opponent modeling. The outstanding performance of the AI on the Botzone platform proves the effectiveness of the improvement. However, there is still room for improvement. They plan to try other neural networks such as convolutional neural networks like ResNet. They hope to combine search-bots with the AI to enhance the performance as search plays an an enjoyable game. </p>
<p class="text"> They will investigate how to improve the ‘sample efﬁciency’ with experiment replay as it still costs a lot of time even utilizing the “coach network”. important role and performs well in research about game AI. They will also try to transfer the methods to other games for. stronger game AIs. In addition, they will also investigate how they can improve the. behavior of the ‘coach networks” to improve the methods in other games.<br/> Libratus, “Superhuman ai for heads-up no-limit-limit poker,” Science, vol. 359, no. 6374, is published on the arXiv.com/ArXiv: 2019. The authors also discuss the effects of deep reinforcement learning on multiplayer poker. They say their findings could be used in the next round of machine learning research on the topic of Machine Learning and Machine Learning in the U.S. National Institute of Technology.<br/> Researchers: “Botzone: an online multi-agent competitive platform for ai education,” in ACM Conference on Innovation and Technology in Computer Science Education, 2018, p. 33–38. “Deltadou: Expert-level.doudizhu ai through self-play.” “Reinforcement learning: An introduction. An introduction to Reinforcement Learning: An Introduction.” by R. S. Sutton and A. G. Barto. “Mastering. doudizhu with. deep. play deep reinforcement learning.’s ‘reinforcement. learning’ is a uniﬁ<br/> J. Zhou, “Design and application of tibetan long chess using monte carlo algorithm and artiﬁcial intelligence,” in Journal of Physics: Conference: Conference-Series, vol. 1952, no. 4, 2021, p. 042104. “Monte Carlo methods. for the game kingdomino,’ in IEEE Conference on Computational. Cognitive.Intelligence and Games (CIG), 2018, pp. 1–8.. F. Southey, M. P. Bowling, B. Larson, C. Piccione, N. Burch, D. Billings<br/> K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in IEEE Conference on Computer Vision and Pattern.Recognition (CVPR), 2016, pp. 770–778. I.G. Bowling, and M. Zinkevich, ‘Accelerating response calculation in large extensive games,’ in International.Joint Conferences on Artiﬁcial Intelligence </p>
<p class="text"> The monte carlo tree search algorithm is used for video game testing. The algorithm is described as a “tree search algorithm” in IEEE Conference on Games (CoG), 2020, pp.229–236. The algorithms are used to test video games in video games and other computer-science applications. The results are published on arXiv:1712.01275, 2017, by S. Zhang and R. S. Sutton. They are happy to clarify that the algorithm is based on the fact that it is a tree algorithm. </p>
