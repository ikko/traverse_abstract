<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings</h3>
<h3>Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings</h3>
<img src="_sum_2204.02710.html.1.png">
<p class="text"> Mix-and-Match: Scalable Dialog Response Retrieval using Gaurav Pandey, Danish Contractor, and Sachindra Joshi. In Proceedings of ACM Conference (Conference‚Äô17, July 2017, Washington, DC, USA, 11 pages. They show that the resultant model achieves better performance as com-pared to other embedding-based approaches on publicly available conversation data. The paper is published by the Association for Computing Machinery (ACM)<br/> retrieval-based response pre-dictors retrieve the response from a predefined set-sets of responses given the dialog context. Such methods find applica-henytion in a variety of real-world dialog modeling and collaborative tasks. The success of a good response retrieval system lies in learning a good similarity function between the context and the response. These tools are also used to power chat-bots powered by system returned responses in ‚ÄòAgent-centric‚Äô environments where a system makes recommendations to a customer-support or contact-center agent in real-time </p>
<img src="abstract.png">
<p class="text"> An effective response retrieval system must be able to capture the complex relationship that exists between a con-centrictext and a response, while simultaneously being fast enough to be deployed in the real world. They present a scalable and efficient dialog-retrieval system that maps the contexts as well as the responses to probability distributions over the embedding space. The resultant model is referred to as ‚ÄòMix-and-Match‚Äô. They formalize this notion of closeness among distributions by using Kullback-Leibler(KL) divergence.<br/> Using automated and human studies, they demonstrate that Mix-and-Match-based retrieval methods perform better than previous embedding-based methods. The work is broadly related with two current areas of research Resonse retrieval (Section 2.1) and Probabilitic Embeddings (Section2.2) The work was originally presented for retrieval in QA tasks. The study is published on Kaggle.com/thoughtvector/customer-support-on-twitter.com. </p>
<p class="text"> The probability distribution over the embedding space Rùëë induced by the input text embeddings is as follows:. The parameters of the text encodings (BERT) for the context and response are not shared. They want the context to be ‚Äò‚Äò‚Äô to the distribution of the groundtruth. They use the KL-divergence to quantify this degree of closeness to the ground-truthness of the responses. They also want the response to be '‚Äòof the ground truth‚Äô<br/> The KL divergence between the distributions ùëùùëü and ùÅù ùÅæ is given by the Kullback-Leibler divergence function. For Gaussian mixtures, the KL divergence needs to be approximated. They use -pair contrastive loss to train the distributions by training the distributions with respect to other randomly selected responses. For each Gaussian component in the response distribution, they find the closest Gaussian part in the context distribution. They average this loss across the context-response pairs in thebatch and minimize it during training.<br/> The BERT encoders, the randomly initialized embeddings as well as the linear layers for computing the means and variances, are trained in an end-to-end process. For each context token, ColBERT finds the same-resembling-likelihood for the context and response of a response. The expression for similar-rearativeity is given in equation-proportionate-reparation-reassure(6) The formula for KL divergence is based on the expression for the similar-paralleity used in ColBERt and SBERT </p>
<p class="text"> Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings. The KL divergence approximation derived in equation (6) finds the closest Gaussian component of the context GMM for each response. This similarity is then averaged over all the tokens in the response. They conduct experiments on two publicly available datasets: Ubuntu Dialogue Corpus and Twitter Customer Support Dataset6. The conversations deal with customer support provided by several companies on Twitter.<br/> They compare the proposed model against two scalable baselines SBERT and ColBERT ‚Äì a recent state-of-the-art retrieval model. They use the pretrained ‚Äòbert-base‚Äô model provided by Hugging Face7. The dimension of the embedding space is fixed to be 128 for all the models. The number of Gaussian components in the context and response distributions is selected by cross-validation from the set with the number of components selected. The model is trained via contrastive loss. </p>
<p class="text"> Each context is paired with 5000 randomly selected responses along with the ground truth response for the given con-centric text. The full universe of responses are encoded once and stored once. Mix-and-Match achieves substantial improvement in recall@k and MRR on all the datasets as compared to SBERT and ColBERT. For Recall@k, they pick the top-referred responses with the least KL divergence in descending order. The results are shown in Table 1.1.1.<br/> They use the BLEU metric for evaluating the quality of the responses retrieved by Mix-and-Match. They use a batch-size of 50 for encoding the responses using Mix-And-Match response encoder. To measure diversity among responses retrieved for a given context, they measure distance between the top-referred responses and the bottom-ranked ones retrieved by ColBERT. The results are shown in Table 2 and Table 3, with the results shown in the table. They also use the metric to measure the distance between a given response and the ground truth response. </p>
<p class="text"> Given a context, the task involves retrieving from a set of 5000 responses that also contains the ground truth response. Comparison of Mix-and-Match against baselines on retrieval tasks. SBERT has the least diversity among the retrieved responses. ColBERT is better in terms of diversity since it uses multiple embeddings to represent contexts and responses. The similarity/KL-divergence computations as well vector similarity searches are performed on a single A100 GPU. Unsurprisingly, the lowest-referred top-response returned the model is to be relevant more often (40% vs. 17%) than the model.<br/> ColBERT achieves a latency of 89.7 ms for retrieval per dialog context. Mix-and-Match uses independent encoders to encode the responses, thus the time taken by joint encoding approaches is proportional to the number of responses in the retrieval set. The latency of Mix-And-Match ranges from.376.7ms to 477.8ms depending upon number of Gaussian components in the mixture. The Diversified-Relevance scores are also shown in Table 4. </p>
<p class="text"> Mix-and-Match returns a relevant response at the top ranked position. In contrast, ColBERT retrieved generic or unrelated responses. They asked the users the question: Given the dialog context and the response sets from two. different systems, label each response with a ‚Äúyes‚Äù or ‚Äúno‚Äôs‚Äù depending on whether the response is a. relevant response. They also report a head-to-head comparison of each system in addition, which the two models were assessed for diversity (no ties)<br/> Diversified-Relevance (DR) weighs the diversity wins by the number of relevant responses returned by each system. The top-ranked response received significantly higher number of votes (40%) in favthe as compared to ColBERT. In 43% of the study, the top-ranking response was received by Mix-and-Match, with a score between 0 and 1. In Table 3, Diversified Relevance is based on a metric that takes the value 1 if relevanceis more diverse its responses to a dialog context. </p>
<p class="text"> In 58% of the dialogs, Mix-and-Match was found to present a more diverse set of response recommendations. In contrast, ColBERT retrieved generic or unrelated responses. The results from the human-study indicate that Mix-And-Match returns more diverse and relevant responses. They found that not only is the model able to retrieve more relevant responses, it also presented more diverse results. They modeled the dialog context and response using a Gaussian Mixture of gaussians, instead of point embeddings.<br/> They approximate the cross entropy by expanding the grotesqueGMM in terms of its Gaussian components, and applying Jensen‚Äôs.inequality. The.GMM can be approximated by replacing ùëùùëü in the above equation by replacing vanillawithjensen'sinequalityand the KL divergence. The above.egemonicupper bound holds for all choice of choice of ùôû, and the bound can be tightened by minimizing it with respect to ‚Äò‚Äù‚Äô<br/> The resultant quantity is neither an upper nor a lower bound, but still a useful approximation. Note that the resultant quantity isn't an upper or lower bound. It is not an upper-or-lower bound but an approximation of a useful quantity. The resulting quantity is 1-1, 1-2, 1, 2, 1.2, 2.3, 3.4, 4.5, 5.5.2.3. 4.4.5 (2.5) 6.5. </p>
<p class="text"> A maturity-assessment framework for conversational AI development platforms. A retrieval-based dialogue system.utilizing utterance and context embeddings. In Proceedings of the 56th Annual. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). The.probabilistic.icioFastText for Multi-Sense Word Embeddings. 2018. The.icioTrovato and Tobin. Conference‚Äô17, July 2017, Washington, DC, USA, et al.<br/> Pankaj Dhoolia, Vineet Kumar, Danish Contractor, and Sachindra Joshi. 2021. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. http://arxiv.org/abs/1810.04805 cite arxiv: 1810.4805. ‚ÄúProceedings of NAACL-T. 4186‚Äù (2019). ‚ÄöÔøΩÔøΩ‚Ñ¢: ‚ÄúPreventing Dialog Models from Human to Human Conversation Logs.‚Äù<br/> Peiyang Liu, Sen Wang, Xi Wang, Wei Ye, and Shikun Zhang. 2020. Retrieval-Augmented.Generation for Knowledge-Intensive NLP Tasks. The ubuntu.untudialogue corpus is a large dataset for research in unstructured multi-turn dialogue systems. The.ubuntu.dialogue. corpus: A large dataset. A large. dataset for. research in. unstructuring multi turn dialogue.systemss.<br/> A Hierarchical Latent Struc-ture for Variational Conversation Modeling. 2019. Association for Computational Linguistics, 1190‚Äì1203. Researchers: "Phenomenomenon" and "BLEU" is a method for automatic evaluation of machine translation. In Proceedings of the 2018 Conference. of the North American Chapter of the Association for. Computational. Linguistic: "Theoretic.computing. linguistics: Theoreticographic.computational.computers.org" </p>
<p class="text"> The Probabilistic Relevance Framework: BM25 and Be-ishlyyond. Foundations and Trends¬Æ in Information Retrieval 3, 4 (2009), 333‚Äì389. The Hierarchical Latent. Variable Encoder-Decoder Model for Generating Dialogues. In AAAI. 2017. Building End-To-End Dialogue Systems Using Generative Hierar--Chical Neural Network Models. Building end-to-end Dialogue Systems using Generative Hierar-.-Chical neural network models.<br/> Researchers at ACL.com and the Association for Computational Linguistics.com have published a number of papers on cognitive-language-recognition.computing.computers.comms.org/2010/2013/2014/2015/14/15/20/15. Researchers at the OpenReview.net.net have published their findings in the journal Open Review.com/Open Review.net. The Open Review is a collaboration between the authors of this article and the Society for Computer Science.com. </p>
