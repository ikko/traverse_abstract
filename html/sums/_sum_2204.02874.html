<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound</h3>
<h3>ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound</h3>
<img src="_sum_2204.02874.html.1.png">
<p class="text"> ECLIPSE (Efﬁcient Long-range Video-Retrieval using Sight and Sound) adapts the popular CLIP model to an audiovisual video setting. The method is 2.92× faster and 2.34× memory-efarnished than long-range video-only approaches. In contrast to the costly and redundant video modality, audio can compactly capture information related to human actions, objects and other complex events </p>
<img src="_sum_2204.02874.html.2.png">
<p class="text"> Traditional text-to-video retrieval methods are designed for short videos. They propose ECLIPSE, an audiovisual adaptation of CLIP with Sound Encoding. Instead of processing many densely-extracted frames from a long video, the framework operates on sparsely sampled video frames and dense audio cues. The temporal dynamics in the scene can be succinctly encoded in the audio-stream (e.g., the sounds of the eggs sizzling in a pan, etc.) </p>
<p class="text"> They adapt CLIP to long-range text-to-video retrieval by adding an efﬁcient audiovisual.attention block into the Transformer architecture. They use a pretrained CLIP text encoder to embed a textual video description into a textual. video description. Afterward, they augment each visual token with position information as is done in. A specialized CLS token v(0) is prepended to the visual sequence of each frame. They next describe schemes in more detail. </p>
<p class="text"> All of the three attention schemes are implemented using a standard multi-head self-attention scheme. The spatial attention enables the model to acquire discriminative frame-level representation by aggregating relevant information from the visual information in individual video frames. To efﬁciently incorporate temporal audio cues into static video frame representation, they use an audio-to-video (A2V) attention mechanism. Conversely, to inject rich visual information into compact audio features, they also use a video-to audio (V2A) attention scheme.<br/> They use a contrastive video-to-text matching loss to train the model. The similarity between text-video pairs is computed using a normalized dot product between the two embeddings f and g. They minimize the sum of the combined losses to train the model. Afterward, they perform temporal pooling over the CLSognitivetokens across all video frames, to obtain the ﬁnal audiovisual representation f ∈ Rd.2.3.4. </p>
<p class="text"> ECLIPSE follows CLIP4Clip setting where text encoder and visual encoder are initialized with CLIP weights For audio encoder, they use ResNet-18 pre-trained on VGGSound For all experiments, they uniformly sample 32-frame inputs spanning the whole input video. They implement the model using Pytorch and conduct the training on fthe NVIDIA A6000 GPUs. The code and models will be made publicly available.<br/> DiDeMo contains 10,464 Flickr videos with 40,543 temporally localized sentences. The average video length is around 8 minutes. Each video is annotated with multiple text queries describing distinct portions of the video. They evaluate paragraph-to-video retrieval using ActivityNet Captions. They use the standard splits for training, validation, and testing. The original dataset is created for moment localization and highlight detection. It is re-purposeed as ActivityNet Caption for text-to </p>
<p class="text"> They use a publicly available state-of-the-art CLIP4Clip video retrieval system as the method. ECLIPSE achieves the bestreported accuracy on this benchmark outperforming all prior approaches. They use standard video retrieval evaluation met-rics such as text-to-video R@1 and R@5, R@10, and mean rank (MnR) to validate effectiveness of the model. They note that since the model is built on CLIP, which is pre-trained on a large-scale image-and-text dataset, comparisons with some of the previous meth-ods are not directly applicable.<br/> ECLIPSE outperforms CLIP4Clip by a substantial margin (1.6% in R@1) This suggests that audio can be effectively used to replace some of the potentially redundant and costly to process video parts. They next investigate the trade-off between video retrieval accuracy and the number of input frames. They observe that ECLIPSE consistently outperforms ClIP4CLip in all three 8, 32 and 96-frame regimes. This result suggests that. audio can. be effectively.used to replace. potentially redundant parts of video parts that are potentially redundant. </p>
<p class="text"> They compare the computational cost of a 32-frame ECLIPSE with the implemented 96-frame CLIP4Clip on ActivityNet Captions. Despite using fewer frames (i.e., 32 vs. 96), ECLIPE achieves even higher accuracy than CLIP 4Clip, which is indicated by 1.51× fewer GFLOPs, 2.92× faster run-time, and 2.33× smaller GPU memory usage. They validate the approach on fthe other long-range video datasets: QVHighlights, DiDeMo, YouCook2, and Charades.<br/> ECLIPSE outperforms Frozen-in-Time and ClipBERT by a signiﬁcant margin on all fthe datasets. The method generalizes to a diverse set of videos, e.g., cooking, ﬁtness instructions, daily activity, news videos, etc., (ii) audio cues can be used to replace many redundant and costly to process video frames. The results on QVHighlights (QVH) and YouCook2 (YC2), Charades and DiDeMo using the metric R@1 metric. </p>
<p class="text"> They study how different design choices of the model affect the long-range video retrieval accuracy on the ActivityNet Captions dataset. They also investigate different audio encoders applied to different duration segments. They find that injecting the proposed audiovisual attention block into every layer of the 12-layer ECLIPSE model leads to best performance. In particular, compared to the standard joint AV self-attention, the approach achieves a signiﬁcant performance boost of 2.2%.<br/> In Figure 5b, they investigate how audio duration affects the accuracy of a long-range video retrieval task. They also study the video retrieval performance (using R@1) as a function of the number of audiovisual attention blocks in the 12-layer ECLIPSE model. The method achieves the best video retrieval accuracy when the audiovishual attention block is inserted into every layer of the ECLIPE architecture. The results indicate that video retrieval. performance decreases when they use fewer audioviisual. attention blocks. </p>
<p class="text"> They investigate how different video frame sampling strategies affect the performance of a video-only CLIP4Clip baseline on ActivityNet Captions. They observe that for a smaller number of frames (e.g., 32-64) random sampling yields slightly better performance than the uniform sapling. Conversely, uniform sampling leads to better accuracy. In Figure 6, they visualize the top-1 retrieved video by the audiovisual ECLIPSE model. </p>
<p class="text"> They compare the audiovisual ECLIPSE model with a video-only CLIP4Clip baseline. They see a title screen with a photo of a buff man talking in a yard. A woman is seen speaking to the camera while standing around a group of exercise equipment. A person steps up to the window with the violin. The image of the person changes to a digitally animated screen. They observe that the video-Only CLIP 4Clip method struggles with retrieving video when textual queries include audio event descriptions, e.g., “a person playing the violin," etc. </p>
<p class="text"> ECLIPSE learnsimplicit associations between objects and sounds while being optimized with respect to the video-retrieval task. Here, they illustrate the qualitative sound localization results of the method. The image of the person progressively changes to a digitally animated screen. They see a title screen with a photo of a man talking in a yard. They see the house the yard is part of. They also see an older man using a chainsaw on the tree. The person steps up to the window with the violin near the window. </p>
