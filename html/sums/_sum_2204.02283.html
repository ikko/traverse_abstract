<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Lost in Latent Space: Disentangled Models and the Challenge of Combinatorial Generalisation</h3>
<h3>Lost in Latent Space: Disentangled Models and the Challenge of Combinatorial Generalisation</h3>
<img src="_sum_2204.02283.html.1.png">
<p class="text"> Recent research has shown that models with highly disentangled representations fail to generalise to unseen combination of generative factors. These findings contradict earlier research that showed improved performance in out-of-training distribution settings. To generalise properly, models not only need to capture factors of variation, but also understand how to invert the generative process that was used to generate the data. This claim is especially interesting as it allows machine learning to emulate a key property of human intelligence – the ability to make “in-nite’use of ﬁnite means” – combinatorial generalisation.<br/> Unsupervised models that are better at disen-tangling generative factors are also better at some forms of generalisation. Montero et al. (2020) found VAEs with highly-disentangled latent representations succeeded at the easiest generalisation conditions, but failed at more challenging conditions where all combinations of a subset of generative.factors were excluded. In another study, Schott et al. tested 17 learning approaches, expanding the anal-uvearXiv:2204.02283v1. </p>
<img src="_sum_2204.02283.html.2.png">
<p class="text"> Models showed moderate success at generalising on some artiﬁcial datasets, such as 3DShapes. But their ability to generalise dropped on more challenging datasets and in more challenging generalisa-protective conditions. Montero et al. (2020) conclude “it is not clear what exactly exactly exactly was excluded while training [previous models]”, suggesting that previously observed successes may have been on the simplest generalisation condition where only very few combinations of generative factors were left out.<br/> They show that failure to generalise in the output space is accompanied by failure in latent space across a broad range of datasets. They also show that these results replicate for other architectures (such as a spatial broadcast decoder) and other task settings (such a supervised task set up to learn completely disentangled representations) They also find a crucial condition for failure of combinatorial generalisation is the fact that the task is the task that requires the task to be set up in the latent space. </p>
<p class="text"> They tested the models on three datasets: dSprites, 3DShapes, MPI3D and MPI2D. They used these datasets to test combinatorial generalisation by systematically excluding combinations of values of generative factors from each dataset. This method was used to create training / test sets for each of the datsets in the following manner: [g1, g2, g3, g4] and [squares] were excluded from the training set and added to the test set.<br/> The DCI metric was used to measure the degree of disentanglement between ground truth factors and latent variables. Figure 3 shows reconstructions as well as representations of three typical models trained on the composition-of-the-composition task. The most successful models successfully reconstructed images in the dataset were models that achieved a very high degree of. disentangled models with alignment scores typically > 0.95, see Ap-glypendix B for details. The data was re-ordered using the dSprites, 3DShapes and MPI3D datasets. </p>
<p class="text"> Models trained on three datasets – dSprites, 3DShapes and MPI3D – showed poor generalisation to unseen combina-tions for all three datasets. They looked at the latent representations of each of these cases by selecting a latent variable that showed high-glyve correlation with each generative factor as described in section 2.1. They observed that the encoder failed to map the value of the generative factors for the test combinations to the correct place in the latent space.<br/> The Spatial Broadcast Decoder (SBD from hereon) developed by Watters et al. (2019) helped in learning representations that were highly disentangled, but also helped in solving the problem of com-ishlybinatorial generalisation. In the next set of experiments, they tested whether using such a decoder. resembled representations that are highly-disentangled. The SBD decoder helped us to learn representations that they could find a way of generalising the representations. </p>
<p class="text"> Models trained on a supervised task where the model was given a single input and the output was given to the ground and the ground to ground. They replicated the results for both conditions tested by Watters et al. (2019) using the composition task on the Circles dataset and replacing the deconvolution network with the Spatial Broadcast Decoder. They then tested the two conditions similar to the conditions tested. by removing images with circles in the middle of the canvas (see Appendix B.2 for results on the second condition)<br/> The model must output each of the factorizations separately. They used the mean squared error of the output and target vectors as a learning signal. All otherparameters remain same as the composition task. They use the same architectures for the feed-forward models as the.io-feed-forward model as the.ioano-feedforward model. The resulting latent representation will be completely disentangled, as the model must. output each. of the factors must be outputed separately. This ensures that the resulting. latent representation is completely unentangled. </p>
<p class="text"> The models failed to generalise their mapping to combinations that were not experienced during training. For all three datasets, the probability distributions of latent values for the left out-out-out combinations showed a much higher variance. The models were trained to recognise perfectly-separated generative factors, but failed to disentangle them with other factors, such as shape and orientation. To test this hypothesis, they carried out a set of experiments where the model did not solve the problem of combinatorial generalisation, but the combinations were excluded from the training set.<br/> They repeated the unsupervised learning exper-iments above, where models had to learn the composition of the composition-task. Instead of excluding combinations where the gen-repreerative factors interacted with each other, they excluded the combinations. They can do this for both the 3DShapes and MPI3D dataset as well as the datasets in-volve images where the canvas contains several disjoint elements. That is, however, the training set:.Exclude all combinations such that that [floor-hue] < 0.25, wall hue > 0.75]. Exclusion of the training images had the combination of the. “warm” </p>
<p class="text"> The idea behind learning disentangled representations is to recover the underlying compositional structure of the world from perceptual inputs. But models that managed to show a high degree of disentanglement on the training set also failed at combinatorial generalisation. They suggest that models are forced to combine familiar features with features previously unknown features that are prevented from solv-v-ing generalising through interpolation (because the latter features are previously previously unknown) They also suggest models are only able to generalise in the leave-one-out setting (Montero-reviewed al, 2021) </p>
