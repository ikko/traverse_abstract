<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic</h3>
<h3>Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic</h3>
<img src="abstract.png">
<p class="text"> There is limited work that studies fairness using a multilingual and.ipient intersectionality framework or on downstream tasks. They use these tools to measure gender, racial, ethnic, and.intersectional social biases across ﬁve models trained on emotion regression tasks in English, Spanish, and Arabic. They discuss unisectional2 and intersectional social biases in multilingual language models ap-ulentplied to downstream tasks using a novel statistical.framework and novel multilingual datasets. In this paper, they discuss the impact of social biases on downstream task results.<br/> In this paper, they refer to biases against a single social social cleavage, such as racial bias or gender bias, as unisectional. They choose emotion regression as a down-stream task because social biases are often realized through emotion recognition (Elfenbein and Am-ishlybady, 2002) and machine learning models have been shown to re-dictate gender bias in emotion recog-ishlynition tasks (Domnich and Anbarjafari, </p>
<img src="abstract.png">
<p class="text"> In this paper, they demonstrate the presence of gender, racial, ethnic, and intersectional social biases on language models trained on emotion-reform tasks in English, Spanish, and Arabic. The presence and impact of harmful social biases in machine learning and natural language process-forming systems is pervasive and well-documented in popular word embedding methods. They propose a novel statistical framework to.detect unisectional and.interintersectional social. biases in language models training on sentiment analysis tasks.<br/> They argue that existing studies in fairness are limited in their ability to uncover biases in and to “debias” language models without engaging with the intersectionality framework. The WEAT has been used to identify intersectional social biases in natural language processing models. Guo and Caliskan (2021) introduce tests that detect both known and emerging biases in static word embeddings and extend the WEAT to contextualized word embedding framework using sentence-less language tests. They argue existing studies have limited their ability in uncovering biases in </p>
<p class="text"> The original EEC uses ten names for each gender-racial cleavage, selected from the list of names used in Caliskan et al. (2017) for Anglo names of both genders. Given names include Ebony for Black women, Alonzo for Black men, Amanda for white women, and Adam for white men. For male Arab names, ten names are selected from Caliska et al., a study that employs the IAT to quantify implicit racial bias. The original eEC also uses emotional state words and emotional situation words sourced from Roget’s Thesaurus. For example, furi-centricous and irritating for Anger, ecstatic and amazing for Joy, ecstatic<br/> For the Spanish and Arabic EECs, native-uran-speaker volunteers translated the original sentence-phrasetemplates, noun phrases, and emotion words. They veriﬁed the generated sentences for proper gram-ipientmar and semantic meaning. For the Ara-uranbic EEC, the authors transliterated names using the Wikipedia pages of individu-culars with a given name. All names are available in the appendix of the study. Researchers used Beta regressions for modeling socioeconomic biases. </p>
<p class="text"> The Beta regression (Eq. 1) measures the inter-productiveaction between the response variable Yi and the independent variables Xji. Yi is the score predicted by a model trained for an emo-tion regression task on a given sentence i from an EEC. The labels for emotion regression restrict YiYi to, although 0 and 1 do not occur in prac-centrictice, such that they may use Beta regression to mea-insuredsure biases.<br/> Using pre-trained language-speciﬁc models, they train models on the emotion intensity re-evaluate tasks in English, Spanish, and Arabic from the SemEval-2018 task 1: Affect in Tweets (Sem2018-2018 Task 1) (Mohammad et al., 2018) They report the performance using the performance of each model and language combination, and report the results using the of-cial compe-inducing compe-itiveness metric, Pearson Correlation Coef-Review Coef ﬁcient. </p>
<p class="text"> The most pervasive statistically signiﬁcant social biases observed is gender bias, followed by racial and ethnic bias, and ﬁnally by intersectional social bias. They are primarily interested in the statistical analysis of the statistical correlation of emotion regression using EECs and novel statistical framework. For example, the opposite: Black women are more likely to be less angry in all three sentences that are all inferred as less joyful in a statistically more joyful in BERT+ERT+.<br/> In all Arabic models, sentences referring to women are predicted to be less angry than those referring to men. English and Spanish models pre-referring to Arabs predict the same sentences as less fearful and sad. They observe that ethnic biases are sometimes split by language. Future work ought to consider the interplay between ethnic biases across various languages because the same social biases may be expressed and measured differently in different lan-gianguages. For example, English models predict fearfulsentences referring to Arabs as more fearful while Arabic models predict them as more sad. </p>
<p class="text"> Anger Coefﬁcients are an example of an emotion-combining language model. EEC inference is based on the model of the English (Black-white) EEC. Anger, Fear, Sadness, Anger, Anger and Fear are all included in the model. The model was used for all emotion combinations in the EEC model. For example, Anger was an example for the English model and emotion-equivalent model of emotion-competitiveness. The EEC is an EEC that has been used in the past to infer infer inferiors for all emotions.<br/> Anglo-Latino, English, Spanish, Italian, French, Italian and Spanish. English, French and Spanish. Italian. Italian. Spanish. Italian. Italian. Spanish. Ethnicities. Ethnicity. Ethnicities. Ethnicity/Ethnicity: Ethnicities/Ethnomenomenon; Ethnicities: English, Italian; Italian; Spanish; Italian, Spanish; English; French; Spanish, French; Italian and Italian;. English; Italian: Italian; English. Italian; French: Italian, English; Spanish: French; English. Italian; Latino; Italian.<br/> Study limited in scope to only social biases in English, Spanish, and Arabic due to the training data available. Future analysis of the study of social biases and stereotypes is highly.nuanced, especially in its application to fairness.in natural language processing. They see a myriad of contradictory results across.language, emotions, and models. This suggests that the social biases encoded by languages modelsare incredibly complex and difﬁcult to study using a simple statistical framework. For example, the work may introduce additional statistical tests or.eECs that better capture the complex nature of so- </p>
<p class="text"> Anger Coefﬁcients.igion.Language.Model.Race/Ethnicity.gender.Gender.ethnic.ethnic/gender. Ethnic. Ethnicy. Ethnicity.ethnicy.ethnic. Ethnicy: Ethnicy, ethnic.ethnic; Ethnicy/gender: Ethnicity/gender; Ethnicity. Ethnicity: Ethnicities. Ethnicities. Ethnicities: Ethnic, ethnic, ethnic; Ethnicities, ethnicities.Ethnicities.ethnicity. ethnic; ethnicities.<br/> In this paper, they introduce four-a-team framework for study of social biases in English, Spanish, Arabic, and English. They also introduce four.ua-to-contribute-a novel study of ethnic, ethnic, racial, ethnic and gender-biases in English and Spanish. They emphasize that there exists no set. of carefully curated sentences that can detect the intricacies of social. biases. Future work can address these shortcomings by creating EECs that represent these.individuals in their totality and by using regression.models that represent non-binary. identities using non<br/> They are interested in working with community members and scholars from the groups they study to better interpret the causes and impli-cations of these social biases so that the natural-language processing community can create more equitable systems. They are grateful to Max Helman for his helpful comments and conversations. This work is supported in part by the Natural Language Processing community and by the University of New York State University of North Carolina, USA, and the National Geographic Geographic Institute of Science Education and Technology. </p>
