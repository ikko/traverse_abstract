<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Knowledge Infused Decoding</h3>
<h3>Knowledge Infused Decoding</h3>
<img src="_sum_2204.03084.html.1.png">
<p class="text"> Knowledge Infused Decoding (KID) is a novel decoding algorithm that dynamically infuses external knowledge into each step of the LM decoding. It maintains a local knowledge memory, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. KID also alleviates exposure bias and provides stable generation quality when generating longer sequences. Published as a conference paper at ICLR 2022: "KID," a novel algorithm for generative LMs. </p>
<img src="_sum_2204.03084.html.2.png">
<p class="text"> Recent retrieval-based models try to tackle these problems by augmenting inputs with retrieved knowledge evidence. Key limitation of such methods is that they retrieve documents only once while ground-forming them in the input static context. Static knowledge becomes a major problem for tasks where longer-ended story generation is expected. They present a novel decoding algorithm KID, aiming to better infuse knowledge into generation in a dynamic manner. KID maintains a local knowledge memory, interacts it with a knowledge trie dynamically created from retrieved supporting documents, and updates the local memory as a knowledge-aware constraint.<br/> KID incorporates extra knowledge from external resources and thus shows strong performance in knowledge-intensive NLG tasks. Two common strategies dominate the decoding algorithms used by most generative models: beam search which maximizes likelihood in a local horizon, and sampling decoding (e.g., top-k sampling) which relies on randomness. The KID algorithm is built on a model of KID, which is based on the knowledge of external resources (Wikipedia) KID has been shown to be </p>
<p class="text"> The KID decoding algorithm is guided by the query results with policy gradient, to generate new token yi.that records all the mentioned entities in current context, to focus on concept-centric knowledge. Current step LMs are trained to maximize the probability of generating ground-truth tokens at each decoding step. The teacher-forcing algorithm used by MLE training leads to the exposure bias problem (Ranzato et al., 2016), as the LM has access to the ground truth sequence up to the next token during training, but does not have such signal during testing.<br/> They compute the t-th step knowledge gain rt as the total log probability of all retrieved tokens from Gext under decoding policy πt as rt = ∑hmaxi=1. The log probability mass on tokens aligning with these tokens is a log of 1 in the coordinate of token v.4. The log log of knowledge gain is 1 log of the probability of the retrieved tokens being retrieved using Gext's decoded policy. They use this </p>
<p class="text"> They use off-policy sampling to collect trajectories to collect data. They deﬁne the t-th step reward JRL,t on trajectories induced by πt as: π∗receive-the-knowledge gain rt. The KL penalty term is to avoid the updated policy drifts too much away from the original one, also known as trust region constraint (Schulman et al., 2017; 2015) Published as a conference paper at ICLR 2022. They consider three diverse types of knowledge-intensive tasks for evaluation.<br/> They also investigate whether KID can beneﬁt NLG tasks that do not have an explicit query form for certain knowledge. They calculate BLEU-1 and ROUGE-L scores to be able to compare directly with work related work (Lewis et al., 2020b; Krishna et al. 2021) They use two extra QA tasks: PIQA (Bisk et. 2020) and PubMedQA whose questions require domain-speci ﬁc knowledge to answer. </p>
<p class="text"> KID decoding improves the generation quality in general(by at most 15%) Compared with beam search (Beam) and sampling decoding (Sample) They report their performance in zero-shot setting (*), and that of that of being ﬁne-tuned (FT) They color those results of KID that achieve > 5% improvement over the next-best performance. They annotate the performance reported by published state-of-the-art models to date (means missing ofﬁcial reports)<br/> They tune the hyperparameters based on the models’ performance on an in-house split dev set. They report the results that were best on the ofﬁcial dev set4.2: They compare the results of GPT2-medium and BART-large on six diverse NLG tasks with beam search (Beam), sampling (Sampling), and the proposed KID algorithm. Compared with task-speci﬉c state-of-the-art models, task-agnostic LMs armed with KID can beat SOTA results on three different tasks (ELI5, ROC, WoW) </p>
<p class="text"> As a model-agnostic method, KID shows particularly strong performance in few-shot scenarios, which can better help LMs transfer to new domain with minimum training data. They report ROUGE-L score with 10% and 100% of the training data. KID outperforms all other methods in comparison with existing methods of integrating knowledge for NLG tasks. They also evaluate on two extra-extra QA tasks: PIQA and PubMedQA QA, where answers are considered not fully covered by Wiki knowledge.<br/> KID with a LM similar in size to the baselines achieves best few-shot performance in all fthe tasks, including PIQA and PubMedQA. Baseline methods tend to generate off-topic and hallucinatory answers when the expected answer length is long. KID, instead, dynamically searches references for grounding, thus showing more flexibility in unfamiliar context. They choose two tasks ELI5 and WoW that provide ground-truth knowledge provenance, which are also the only two KILT tasks requiring long and abstractive generation. </p>
<p class="text"> Impact of hyperparameters on KID’s ELI5 performance when (a) more documents are retrieved, and (b) more hops taken when querying the knowledge trie. (c) Average human ratings on sequences generated by KID, sampling, beam search, and reﬂective decoding. KID generation has more stable quality across lengths by restraining exposure bias. KID with the default DPR retriever outperforms all other retrievers and RAG variants in retrieval accuracy.<br/> They study the impact of number of documents (k) they retrieve and number of hops (hmax) they use to query the knowledge trie. An optimum k is 5 through empirical observation, and similarly, a hmax = 4 brings best performance. They use beam size 5 for beam search, and top p = 0.9 and k = 20 for sampling decoding. They then ask humans to rate these generations with 7-point Likert scoring (Joshi et al., 2015) </p>
<p class="text"> They run paired sample t-test comparing human references with beam search (BM) with beam size 5, sampling (SP) with top p = 0.9 and k = 20, reﬂective (RFLC) decoding, and the KID generation. Human assessments of generation in terms of Relevance, Factuality, and Grammaticality on a 7-point Likert scale. Table 4 shows that overall larger LMs beneﬁts all decoding methods with KID consistently outperforming Beam search and/or sampling decoding.<br/> There is no signiﬁcant difference in grammaticality between KID and vanilla decoding methods. KID doesn’t require architectural changes to existing LMs and is applicable to various generative LMs. Future work of KID could study more efrencient data structures to accelerate knowledge fetching and data integration. They adopt the public implementation of GPT3—GPT-Neo (github.com/EleutherAI/gpt-neo) </p>
<p class="text"> The goal of KID is to provide a general-purpose knowledge infusing decoding algorithm by leveraging purposefullyretrieved Wikipedia documents. The algorithm can mimic undesirable attributes of the target knowledge source documents that could be non-reflective and do not represent current norms and practices. They do not claim that the findings will generalize across all languages, although the framework has potential to be extended to other languages with necessary modiﬁcations. Code and scripts for producing KID are available on GitHub at https://://github.com/Microsoft/KID.<br/> Researchers: Reading Wikipedia to answer open-domain questions. Authors: "Comet: Commonsense transformers for automatic knowledge graph construction" "P19-1470" is published in Proceedings of the Association for Computational Linguistics (Volume 1: Long Papers) "P17-1171," by Antoine Bosselut, Ronan Le Bras, and Yejin Choi. In AAAI, pp. 4923–4931, 2021. URL://ojs.aaai.org/article/view/16625. </p>
<p class="text"> The Association for Computational Linguistics. published as a conference paper at ICLR 2022. The paper is published in the Proceedings of the International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Volume 97 of Proceedings of Machine Learning Research, pp. 1223–1232. The study is published by the Association for Compu-Saharan-Linguistics, which publishes the journal OpenReview.net, 2019. The authors also discuss the use of a neural model for unsupervised multi-document abstractive.ive.summarization. The study will be presented at the ICML.<br/> The Association for Computational Linguistics has published a paper on how to make large models into memories and load fast. The paper is published by the Association for Cognitive Linguistic Association. It is the first of its kind to address the topic of natural language processing. The study has been published in the journal of the Association.computing.org, the journal.comprehensive.org.com, and the International Joint.computational.com/NLP conference.com.org. </p>
<p class="text"> A knowledge-enhanced.pretraining model for commonsense story generation. Published as a conference paper at ICLR 2022for Computational Linguistics. URL: 10.18653/v1/D19-5409. URL https://aclanthology.org/2020.tacl-1.7.7. The curious case of neural text-degeneration. In Proceedings of the. 37th International Conference on. the Cognitive. Linguistic. Conference on Learning Representations. in Austria, May 3-7, 2021.<br/> Researchers: Human-centric dialog training via ofﬂine reinforcement learning. Researchers: A model for open domain long form questionnaires: Explain anything like i’m ﬁve. The results are published in the Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3985–4003, Online, 2020. URL: 10.18653/v1/2020.emnlp-main.327. URL://aclanthology.org/2021.74. </p>
<p class="text"> Researchers: Dense passage retrieval for open-domain question answering for biomedical research question answering. Researchers: A dataset with a billion-scale similarity search with gpus. The paper will be published as a conference paper at ICLR 2022. The paper is published by the Association for Computational Linguistics and the Association of the Association. For more information on this article visit http://aclanthology.org/D19-1259.com/. For more from this article click here: http://ieeexplore.ieee.org/.<br/> SenseBERT: Driving some sense into BERT. In Proceedings of the.apologetic58th Annual Meeting of the Association for Computational Linguistics, pp. 6086–6096, Florence, Italy, 2019. URL: 10.18653/v1/P19-1612. URL://aclanthology.org/2020.com/2019//. Researchers: BART: Denoising sequence-to-sequence pre-sequence training for natural language. </p>
<p class="text"> Researchers from the Association for Computational Linguis-based Linguistics. published as a conference paper at ICLR 2022. Published as a. conference paper. The findings are published in Advances in Neural Information Processing Systems 33:. The journal will be published on December 6-12, 2020. The study is published by the Association of Computational. Linguistic.computing.org, ISSN: 10.18653/v1/2020. For more information on this article visit http://www.aclanthology.org/2019/2022/.<br/> Researchers: Constrained abstractive summarization: Preserving factual consistency with constrained generation. Researchers: A corpus and cloze evaluation for deeper understanding of commonsense stories. The Association for Computational Linguistics: Human Language Technologies, San. Diego, California, 2016, 2016. Researchers: Safe and efﬁcient off-policy reinforcement learning. A corpus of the. corpus and. evaluation for deep understanding of. commonsense. stories. Aims to preserve the consistency of the abstractive. summarization with constrained. generation. </p>
<p class="text"> A deep reinforced model for abstractive sum-to-marization. A human generated machine reading comprehension dataset. A method for automaticevaluation of machine translation. Text generation by learning from demonstrations. A benchmark for knowledge intensive language tasks: KILT: a benchmark for language intensive language. A paper published as a conference paper at ICLR 2022. Published as a paper at the 9th International Conference on Learning Representations. In 9th Interna-fledgedtional Conference on learning Representations, ICLr 2021, Virtual Event, Austria.<br/> Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 794–805, Online, 2020,. URL:https://aclanthology.org/2021.naacl-main.200. URL: 10.18653/v1/2020.emnlp-main-58.549. </p>
<p class="text"> Researchers: Scaling language models: methods, analysis & insights from training gopher. Published as a conference paper at ICLR 2022. Researchers: Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Exploring limits of transferring learning with text to text.transformer. An atlas of machine com-uvemonsense for if-then reasoning. AtOMIC: an atlas. Atlas of machines with machine com com-oumpicsense for If-then-reasoning. In The Thirty-Third AAAI Conference on Arti�*cial Intelligence.<br/> Proximal policyoptimization algorithms. The paper is published by the Association for Computational Linguistics. It is published in the Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. The article is published on the ArXiv preprint, abs/1707.06347, 2017. The journal's first article has been published by John Schulman and Oleg Klimov. The authors of this article are invited to the conference on the topics discussed. The conference will be held in San Francisco. </p>
<p class="text"> Researchers from the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) published as a conference paper at ICLR 2022. ERNIE.2.0: A continual pre-training framework for language understanding. The Thirty-Fourth AAAI.ulentConference on Artiﬁcial Intelligence, AAAI 2020, The Thirty.Second Innovative Applications of Arti.of Arti ﬁrial Intelligence Conference, IAAI 2020. The Tenth AAAI Symposium on Educational.Advances in Arti.-Human Language Technologies.<br/> Researchers: Open relation extraction: Relational knowledge transfer from supervised data to unsupervised data. Researchers: Pertrained encyclopedia: Weakly supervised knowledge-pretrained language model. Pertence-based language model is weakly supervised, but open relation extraction can be applied to other data. The research was published at the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP) in Hong Kong, China, and the 9th International Joint Conference on Natural. Language Processing. </p>
<p class="text"> Researchers from the Association for Computational Linguistics: Human Language Technologies. published as a conference paper at ICLR 2022. They will present their findings at the International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of the 37th International. Proceedings of Machine Learning Research,. Volume 119 of PEGASUS: Pre-training with.extracted gap-sentences for abstractive summarization. In Proceedings of the. 37th. International. Conference on. Machine Learning.<br/> Researchers: Pre-training text-to-text transformers for concept-centric common sense. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. URL: 10.18653/v1/2020.com/2021. Researchers: "Pre-training transformsers" are pre-training texts to be more intuitively aware of each other’s intent. The researchers: “Pre </p>
<p class="text"> They chose eight knowledge intensive NLG tasks to evaluate KID. The dataset statistics of these tasks can be found in Table A1. They detail the process of constructing the knowledge trie and how they query for the hypothesized knowledge in a dynamic fashion. In Figure A1 (a), for a given question (say from the ELI5 dataset), the DPR retriever (Karpukhin et al., 2020) would retrieve k documents. They then use co-referenceresolution to replace the pronouns with their referents (colored in red) and normalize the text.<br/> The local knowledge memory is updated only when a new entity is created. This is because the non-entity words (such as stop words, digits, etc.) are rarely the keys that lead to meaningful next-step constraints. Since the external knowledge trie is constructed off-line, the query time mainly depends on the number of query words in the local memory (with max length wmax), and the maximum number of queries. At each step of the decoding, they ﬁrstlycollect knowledge demonstrations </p>
<p class="text"> Published as a conference paper at ICLR 2022, published as a paper at the ICLr 2022 conference paper. The British Medical Journal indicated that "drivers who consume cannabis within three hours of driving are nearly twice as likely to cause a vehicle collision as those (who) drivers are not under the influence of drugs or alcohol" Driving under a certain concentration of illicit drugs is illegal in the UK. The law prohibits driving with any concentration of. illicit drugs: Heroin, cocaine, ketamine, methamphetamine, cannabis and MDMA. </p>
<p class="text"> KID aims to improve knowledge awareness during decoding. KID can function as a standalone module requiring neither speciﬁc model architecture nor re-training or ﬁne-tuning the language model. The time and space cost of running KID could be a matter of concern especially when they consider real-world applications. They compare the time complexity of KID with other baselines, including beam search and sampling decoding. They run experiments on QA tasks that FiD was trained on, and discuss their differences on performance. </p>
<p class="text"> Little KID is a naive implementation of KID that uses plain list instead of trie to store knowledge. Diverse Decoding, PPLM, and KID are similar as all are inference-time optimization. But KID requires additional off-line knowledge trie-construction (nearly the same time cost as RAG and FiD) Other decoding methods do not have the ability to infuse knowledge, and thus do not bring additional space cost. They study the time consumption during knowledge retrieving, model training, and inference (i.e., generation)<br/> KID can function as a standalone module operating only at inference time, rather than rely on a speciﬁc model ﬁne-tuning or pre-training together (such as RAG and FiD) KID has comparable time ef-time efשּׁ�ciency as those of RAG, but outperforms them in knowledge infusing (discussed in the §4.2). KID is superior especially in the long-generational generation of KID and RAG. </p>
<p class="text"> KID builds a knowledge trie off-line (less than 10Mb for all the tasks they studied), and a local knowledge-memory (a limited length FIFO list) to enable knowledge infusion during generation. Published as a conference paper at ICLR 2022. KID requires pre-store the grounding documents on disk (and corresponding dense vectors) In addition, KID build-ups a knowledge-based trie that is less than 10B for all tasks. </p>
