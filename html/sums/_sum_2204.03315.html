<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-trained DNN-HMM-Based Acoustic-Phonetic Model</h3>
<h3>Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-trained DNN-HMM-Based Acoustic-Phonetic Model</h3>
<img src="abstract.png">
<p class="text"> Using pre-trained DNN-HMM-Based Acoustic-Phonetic Model, they use an open-source acoustic-phonetic model instead of training one from scratch. The outcome of the proposed threemodule model yields an intent accuracy of 99.4% on FluentSpeech, an intent error rate reduction of 50% compared to that of Lugosch et al. The trend toward end-to-end modeling has also entered the domain. The proposed model yields a relative reduction of 40% in intent-classiﬁcation error rates.<br/> End-to-end SLU approaches map speech audio directly to the speaker without explicitly producing a text transcript. These thus eliminate the need for an inde-ipientpendent ASR decoder and reduce overall computational times in SLU decoding. In traditional pipelined systems, the decisions made by the ASR component during decoding typically discard acoustical distances from confusing words. In contrast, an end-to end SLU model can retain acousticals information for all possible words in the middle layers, and hence allow these to compete with each other via both acoustic and semantic scores. </p>
<img src="abstract.png">
<p class="text"> Lugosch et al. use a threemodule modeling approach for end-to-end SLU that involves two major steps: phoneme and word module training using speech data, and intent module.training using SLU data. They adopt their three-module architecture but use different methods for pre-training and MTL. Pre-training reduces their intent classiﬁcation error by 65%. They replace the phoneme module with a well-trained, open-source speech model, as described in the next section.<br/> In the two-model approach, ASR and NLU are typically trained on different types of data without joint optimization over the end-to-end data. Decoding using the maximum over the N-best ASR sentences is a fast way to replace sum-centricming probabilities over all probable word sequences, W. Many systems streamline calculations by using only the 1-worst ASR result to decode intents, as well as using the 2-model SLU approach. The three-model stepwise models are constructed using acoustic-phonetic, pronunciation, and lan-centricguage understanding (LU) modules. </p>
<p class="text"> Pronunciation (phone-to-word) module modeling was trained on the LibriSpeech corpus. The second module converted (sub)phonetic units to (sub-word), word-level units. The third intent module was added as a network with 2 layers of 768 UniLSTM units and trained after the above pronunciation module was ready. Table 3 compares the ASR and SLU accuracy of all models in each modeling step. They also see in Table 3 that all models are superior to that of the pipeline approach.<br/> They trained a CTC/attention Transformer ASR model using LibriSpeech fol-reprelowing with ESPNet4 and experimented on pipeline decod-inducinging with FluentSpeech. The SLU accuracy of these streaming and non-streaming methods is all listed in Table 5. They are interested in whether end-to-end SLU is as necessary as using advanced end to-end models. They jointly optimize the LU mod--like mod-like for word to-word conversion with the above phone-To-word module using MTL. </p>
