<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Best-Response Bayesian Reinforcement Learning with Bayes-adaptive POMDPs for Centaurs</h3>
<img src="_sum_2204.01160.html.1.png">
<p class="text"> Best-Response Bayesian Reinforcement Learning with Bayes-adaptive POMDPs for Centaurs. They present a novel formulation of the inter-action between the human and the AI as a sequential game where the agents are modelled using Bayesian best-response models.<br/> They identify novel trade-off for centaursin partially observable tasks: for the AI’s actions to be acceptable to the human, the machine must make sure their beliefs are suffi-ciently aligned, but aligning beliefs might be costly. They present a theoretical analysis of the trade-offs and its dependence on task structure.<br/> Hybrid intelligence aims to combine human and machine intelligence in a complementary way in order to augment the human intellect. In Procof the 21st International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2022), Online, May 9–13, 2022, IFAA-rehensiveMAS, 9 pages.<br/> Cognitive science research has been providing empirically veri-fied computational models of human decision-making that can be used as forward and inverse models in control and reinforcement settings The interaction protocol between the AIand the human must never prohibit an option for the human.<br/> It argues that agents who seem irrational behave rationally according to their subjective models and constraints. This implies that in a centaur, the AI and the human may disagree on optimal behavithe. It also suggests that AI and humans disagree on what is the optimal path to the most preferred restaurant.<br/> In decision-making with delayed re-wardwards, human time-inconsistency is well-modelled by discounting the rewards with hyperbolic functions of the form 𝑑(𝑡; 𝚆) = re-ordereddecision-makingwith delayed-rewards.<br/> The purple trajectory costs two time-steps extra, but the human may allow this detthe if it's not worth overriding the autonomous driver. In this paper, they formalize these intuitions by developing adecision-theoretic multiagent model for centaurs. They formulate the interaction protocol between the human and the AI as a sequential game. </p>
<img src="_sum_2204.01160.html.2.png">
<p class="text"> They identify a novel challenge in centaurs for partially observable cases, called the belief alignment problem, and analyse some of its theoretical properties. They model the agents with Bayesian best-response models (defined in Sec-uroustion 3), where learning about the human and nudging them is cast into a Bayesian reinforcement learning problem.<br/> They model a class of humans who are optimistic about the machine and compute their best-response approximately. They simulate two cases of human bounded-rationality: time-inconsistent preferences and over (or under) estimation of probabilities. The experiments show that if a human’s incentives allow it, the AI is able to nudge the AI into making better decisions.<br/> Bayes-adaptive POMDP (BA-POMDP) is a BRL model where the unknown transition and observation probabilities are modelled with Dirichlet distributions. The solution to the BA-PomDP is the Bayes optimal policy in terms of the exploration/exploitation trade-off with respect to the prior.<br/> In a best-response model, the protagonist is considered in its own perspective of the multi-agent environment. The protagonist’s reward function is defined as A𝑗, A-POMDPs and the recursive modelling method The transition dynamics and observation function are defined for the joint set of actions A and the observations A.<br/> BRMs are a class of BRMs which, when solved, provides the agent’s best response to �𝑗. The agent is uncertain about some elements of some elements of the BRM such as or but has a set of best-response models that are consistent with the known elements of it.<br/> The Bayes-adaptive best-response models (BA-BRMs) are based on the Dirichlet distributions of the 𝑇 and 𝑂. They can be modelled with Dirichlets as in BA-POMDPs, which leads to BA-BRM models. </p>
<p class="text"> They prove that Proposition 3.1 follows directly if the human is solving a BRM of their own to decide when to override, using their optimal policy for 𝑆𝑇𝑀ℎ, for predicting how the machine will behave in the future. This result provides a decision-theoretic grounding for the MoH model.<br/> A MoH’s internal state is defined as 𝐼ℎ and 𝑎𝑚, where is its belief state in revealativestatein a POMDP. The human affects the machine's dynamics only through 𝎎 through the machine'sactionsthroughthe human.<br/> The most general Bayes-adaptive approach would be to place a Dirichlet prior on the categorical distribution. This would ignore the structure in ¯𝐷𝑚 as in BA-POMDPs. Instead, in this case it is more sample-efficient to maintain a posterior over 𝑆𝑇𝑀ℎ. They will choose an appropriately parameterized distribution over the set of possible possibletasks.<br/> An adaptation of the BA-POMCP algorithm proposed in Katt et al, for the case of machine-optimistic human models. The agent receives a nega-uroustive reward for each time-step the shelter remains collapsed, and can re-build it by visiting its grid.<br/> The (PO)MDPs of STMs are dis-crete, thus the set of possible ¯𝜋ℎs is finite and discrete for machine-optimistic human models. This implies that for MoH, there are many behavioural equivalence classes over the space of 𝑆𝑇𝁇𝑀s. They take advantage of this implication and discretize 𝜇 with a weighted set of particles ˆ�<br/> They consider two cases of bounded-rationality when humans want the machine to help them make better decisions. In that case, this can be amortized by training a neural network parameterized by ˆ𝜋ℎ(¯𝑎 ℎ) The converse of this case, where the human can correct the ma-chine’s behavithe and help it learn a better policy. </p>
<p class="text"> The machine’s return in the Food Truck environ-ishlyment with a time-inconsistent human (𝛾 = 7.5)Low 𝑐ℎ(0.01): the human always overrides, so the machine cannot help avoid the red trajectory.Medium 𝚚 (0.21): machine fol-ishlylow the purple trajectory and increases the human’�s per-formance drastically.<br/> The subjective task models of agents are receiverewards of rewards with discount function 𝑑(𝑡; 𝚾) and 𝁁�(𝚉) The model space is expressive enough to capture both time-inconsistent and consistent behavithe.<br/> When 𝛾 is high (≥ 2.0), the human’s solution to 𝑆𝑇𝑀ℎ produces the time-inconsistent red tra-centricjectory. If 𝚾 is too low (≤ 0.2) the AI is overridden whenever the human disagrees. The details are now deferred to Supplementary.lyrics.<br/> The centaur learns that following the red trajectory is the only option. If the 𝑐ℎ is high, the machine learns to follow the blue trajectory. The machine improves the human’s return drastically in experiments with low 𝚾.<br/> The former may result from a fear and an overestimation of the risk. They use the same FoodophobicShelter environment from their paper in the experiment, shown in Figure 2. They model the case of overestimation as follows: Given an 𝑆𝑇𝑀, the STMs are where the human and the machine disagree in transition probabilities.<br/> Dimitrakakis et al assume that both the 𝑇ℎ (i.e.e𝜖) and 𝑐�u are fully known to the machine, but in the experiments they are unknown and must be learned from the interaction. The centaur can infer the true human model in about the first 10 time steps.<br/> Figure 4a shows the machine’s undiscounted return (i.e’�) for three cases: centaur is the method, naive, naive and ideal. The human shows the human's optimal solution to 𝑆𝑇𝑀ℎ evaluated in the real environment. </p>
<p class="text"> The centaur shows the mean return for the method with unknown (𝜖,𝑐ℎ), naive is when the machine executes the policy MCTS gives with 𝑆𝑇𝑀𝑚, ignoring the human’s model. The human is the best case policy. The naive per-verselyforms very badly since the machine is overridden many times.<br/> The human effectively teaches the machine to perform a better policy via overrides. This way, the human effectively learns how to use overrides to teach the machine better policies. The human and the machine may disagree on the state they are in, but in the case of partial observability, the two agents may disagree.<br/> At any given time, agents’ beliefs may not be the same even if they have the exact same action-observation history. This is an additional challenge brought by partial observability, and they will call it the belief alignment problem. To be able to nudge the human’s decisions, the machine must make sure their beliefs are sufficiently aligned.<br/> A rover can measure any rock from any grid, but the distance to the measured rock increases observation noise. When the rover is on top of the rock, the distance is zero, thus there is no noise. However, aligning beliefs might potentially be costly, and in some cases this trade-off can make it infeasible for the machine to augment the human.<br/> The machine thinks the true sensor-efficiency is much higher, and its optimal policy is to measure all the rocks from the starting corner and simply pick up the good ones. However, the machine does not allow this and overrides the human’s optimal policy. The machine knows that the real sensor-efficient is much more.<br/> Machine learns a policy that, instead of measuring from the start location, gets closer to the rocks before measuring. This way it can avoid getting overridden, pick up the good rocks, and still save some time compared to the human’s policy. However, this may not be possible because the human considers them very noisy. </p>
<p class="text"> Red rocks are bad quality and greens are good. The quality of the rocks are partially observed through noisy measurement actions. The rover must pick good rocks to avoid picking bad rocks. The problem arises due to the properties of the RockSample environment. They examine what these properties are, and derive theoretical results intended to guide further research.<br/> The dynamics of RockSample is fully deterministic, and the transitions do not change the unknown factor 𝑠𝑢 (quality of rocks) at all, which means T𝑎(𝑏) = T (T) receivean action and observation is received, rational POMDP agents update their beliefs with O𝑜,reformativebelief<br/> HMM’s value of observation is defined as inf𝑥:|| 𝚥: | 𝓥; | M𝚾(𝚓) | M𝚚; | ��: | M/C: | 1/2/3/4/5; | C: 1/4: | | �: M/CC: | POMDP: | C/C; | S:<br/> When the transition dynamics are deterministic, the 𝛼(𝑇) = 0, and the contraction result becomes unformative. For deterministic dynamics in general, the bound does not tell us whether the contraction happens or not. The following lemma provides a necessary condition for the strict inequality to be strict in the case of deterministic transitions.<br/> The condition of the lemma is satisfied by the RockSample dynamics. In general, for 𝛼(𝑇) = 0 (unobservable) and 𝛾(𝚾(%), 𝚚) mean beliefs cannot contract. The proof of the theorem is given in Supplementary Section 1.<br/> If the dynamics are deterministic and unobservable, beliefs will not expand. The contraction of beliefs depend on how bad the human’s approximate observation model is. The only thing machine can do to align beliefs is to increase 𝛾(𝑂�)<br/> Hybrid intelligencehas emerged as a paradigm for designing AI systems that amplify human intelligence and decision-making Here, they see two main directions of research: (I) Autonomous AI agents that try to try to be autonomous agents that are autonomous agents, (I), Autonomous robots that try and make decisions with autonomy. </p>
<p class="text"> Semi-autonomous AI tries to influence its own user via ad-vice, negotiation, or other means. The closest model to the HuMaCe is the multi-view MDPs and c-intervention games proposed by Dimitrakakis et al They generalize this to partially observable environments and other types of disagree-riddenments between models.<br/> The interaction protocol is a Stackelberg-style competition where the machine commits to a policy for the whole of the horizon, and the human observes the machine’s policy and best-responds. In many cases, it is more realistic to assume the human will not be able to act as a follower of the machine's policy.<br/> Human studies indicate that humans may misjudge the system’s performance because their subjective view of the world does not account for high stochasticity. Hyperbolic discounting has been proposed as a good model for time-inconsistent behaviour, consistent with the evidence from human studies.<br/> Cognitive science can provide us with models useful for learning from human decisions. The solutions proposed in these works forbid some op-tions of the human by removing them, and therefore not suitable actions for an AI. They have formal-formed a framework for modelling the decision-making of half-human half-AI agents, centaurs.<br/> They consider the human as an independent, self-interested agent, and the 𝑐ℎ comes from their own incentives. This formulation can capture various human factors in automation, such as automation misuse where the human is over-reliant on the AI. Of course, all models of humans are wrong, but some are useful.<br/> BRMs can capture a wide range of behaviours, including bounded-rationality. Future work can consider more advanced models for the human such as learning agents with bounded adap-iop-uctive capabilities to the machine’s behavithe. An adaptive human can also learn a better with interaction through interaction.<br/> A fixed-depth recursion is still tractable within the framework thanks to Zettlemoyer et al. The work was supported by: the Academy of Finland, the European Research Council (ERC) under the European Union’s Horizon 2020 research and in-novation programme (grant agreement No758824) </p>
<p class="text"> George Ainslie2001Breakdown of Will: The breakdown of the Will. The study was published in 2001 by Cambridge University Press. The authors have published numerous other articles on the topic of artificial intelligence and artificial intelligence in the past. The findings have been published in the Journal of Artificial Intelligence Computer 53, 8 (2020), 18–28.<br/> The International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 1468–1476. The Helper-AI problem was discussed by Xavier Boyen and Daphne Koller in 1998 in UAI ’98: Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence.<br/> Researchers at the AAAI Conference on Artificial Intelligence in Phoenix, Arizona, USA, and the University of California, CA, will present their findings at the 2016 conference of the International Conference on Autonomous Agents and Multiagent Systems in Phoenix. The AAAI conference is hosted by the U.S. National Institute of Technology in California.<br/> The value of the value of monitoring dynamic systems in artificial intelligence is debated in IJCAI 2007. The study was published in the Proceedings of the International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, in Manuela MVeloso (Ed.)2474–2479.<br/> Piotr JGmytrasiewicz and Prashant Doshi. 2002Time dis-counting and time preference: A critical review. 2004Rational Coordination in Multi-Agent. environmentsAutonomous Agents and Multi.Agent Systems 3 (2004), 319–350. 2004. 2014. 2015Computa-uctive rationality: A converging paradigm for intelligence in brains.<br/> Cognitive science as a source of cognitive models of human decisions for robotics and controlCoRRabs/2109.00127 (2021)arXiv: 2109. Gummadi 2019: Decision Making with Machine Assistance: An Experiment on Bailing and Jailing.<br/> Researchers: Preference reversals due to myopicdiscounting of delayed reward. Researchers: Time-inconsistent planning: a computa-centric problem in behavioral economics. Proposal: Planning Problems for Sophisticated Agents with Present Bias. Proposal by Jon Kleinberg, Sigal Oren, Manish Raghavan: 2016, ACM.com/ECM/16.<br/> Researchers at the 21st International Conference on Intelligent Transportation Systems, ITSC 2018, in Maui, HI, USA, November 4-7, 2018, Wei-Bin Zhang, Alexandre MBayen, JavierJSánchez Medina, and Matthew JBarth (Eds.)IEEE, 1475–1480.<br/> The Quarterly Journal of Economics 107, 2 (1992),573–597. The Be-glygief in a Favorable Future is a model for the future of artificial intelligence. The model is based on the model of human behavior, rather than a model of behavior change, according to the author.<br/> 2016 plan-for-autonomous cars that Leverage Effects on Human Actions is published in The New York Journal of Science and Technology. 2016 plan for autonomous vehicles was published in the US National Institute of Technology (NUJ) and the UK National Institute for Research (RJ) for the first time.<br/> Alexandre M. Bayen: "Lagrangian Control through Deep-RL: Applications to Bottleneck" Alexandre Bayen has published a book on machine learning. Bayen is the author of the book, "Robotic Automation" and "Automation" The book is published by Springer, Springer and MIT.<br/> Researchers: Emergent Behaviors in Mixed-Autonomy Traffic in Machine Learning Research (Proceedings of the 1st Annual Conference on Robot Learning, Vol78, 2017. Researchers: Multi-Agent Filtering with Infinitely Nested Beliefs in NIPS. Bayen 2017. </p>
<p class="text"> The Machine-optimistic Human from Decision-theoreticPrinciples with Bayes-adaptive POMDPs for Centaurs. The state and observation spaces are augmented with Am to allow the human to observe the machine’s actions. The augmented re-orative function is deﬁned as ¯Rh(¯Sh,¯-sh,¯ah) = Rh(s, ac(am, ¯ah),−I[¯ah ̸<br/> The dynamics of BRMh.h is deﬁned as follows:. (1) and (2) The important diﬀerence between the dynamics. of the dynamics of the. machine’s BRM is what the. policy of the other agent is used for. </p>
<p class="text"> The term marked as Machine Term inequation (1) can be entirely replaced by π∗ipienth. This also eliminates the need for the term 'Machine Term in Equation 1' to be replaced with πapologetich. The current state at t is ¯s(t)ipienth and the human’s belief in their subjective POMDP Mh is b(t). The human does not keep track of Im due to its approximation of<br/> If noop is played, there is no cost of eﬀort, am gets executed and then the continuation happens according to π∗apologetich. In other words, Q(b(t)t) is the one step look-ahead action values where the return from.t + 1 onwards is estimated.<br/> The condition for the condition for overriding given in equality (5) is equivalent to:apologeticV π∗ophobich(b(t)uroush) The condition is equivalent in equality of (5), (6) and (7) The result is that the πaburateh is equal to: (4) — πâ∗,)H(4) H(6) </p>
<p class="text"> Ta is a deterministic transition matrix with entries (i, j) where Ta(b) = Tab. If rank(Ta) <| S | is a necessary condition for the strict inequality. Theorem: Each row has a single non-zero entry, which equals 1.<br/> Let Tac be the transition matrix of the hidden Markov model induced by a ﬁxed centaur policy πc, with entries T(s′ | s, ac) where ac-denotes the executed centaur action. They need the following results from previous work for the proof.<br/> For every pair of belief states b1 and b2, Oo.eo∼O1(.|b1) [KL(Oo.o) |o) + ϵO = O1(| b1) | O2(| s) | s; Oi(o) = Oo1(o/b1), Oo2/b2/s; OoOi(eo/s), Oi </p>
<p class="text"> Proposition 3.5 of ? ] Theorem 6.4.4. Theorem: O, ˆO is an approximate observation-model such that KL(O(| s) || O(S) | s) and O(O) is O, so the machine’s observation model is correct.<br/> Applying the lemma 3.9 of ? ] to the equation 7 gives:.apologeticEo∼Om(.|bm) [KL(Oo,ac.),ac,ac (Tac(bm) || Oo,ac) + ϵO KL(Om) = Oh(| bm) Applying proposition 3.5 of ?] to equation 8’s right-most term gives.apologicEo(O </p>
<p class="text"> The belief is discretized with 2000 particles. The state space S consists of regular grids and restaurant grids. Each restaurant grid consists of a chain of three states: the entry, the exit, and the terminal state. The rewards are: vegan-entry: −10, vegan-exit: +20.<br/> The belief converges to the true human MDP, however the MCTS in this case needs a lot of iterations to converge to the optimal policy with random rollout. The agent starts at the shelter, and the. environment needs long exploration and is deterministic. They used the time-step distance to vegan restaurant as a heuristic to evaluate the states.<br/> The beliefs are discretized with 12 ϵs generated as a grid in [0.0, 0.45] and 100 in 0.5. The results were run on a laptop with an Intel Core(R) i7-8550U CPU @ 1.80GHz 2.00 GHz. </p>
<p class="text"> In both experiments, the diﬀerence between the subjective task model of the human STMh and the machine STMm was represented by a parameter. In Food Truck, this is the discount parameter γ, whereas it is action noise ϵ in Food Shelter. The general idea here is that there is a structure in the space of human’s subjective task models.<br/> There are ﬁnitely many behavioural equivalence classes for an optimizingist, since the space of all possible (ϵ, ch) pairs maps to a discrete set of optimal policies. Therefore, intuitively the neural network trained asabove would need to learn to classify an ( ϵ) to the correct equivalence class, and then imitate the policy of the class.<br/> The accuracy of both the belief approximation and simulations will depend on how good the neural network can approximate ¯πh for new particles and how often reinvigoration will be necessary. They expect the centaur to be able to eventually learn that the human’s. model agrees with the machine, and perform the same as the human. model, STMh = STMm = OTM.<br/> Machine is using an online MCTS-variant to choose its next action. When the machine ignores the human and acts as if it is alone, it performs worse. This is because, even though the STMh = STMm, the machine is using a online version of the algorithm. </p>
<p class="text"> When the machine is wrong, the human’s STMh = OTM, but STMm ̸= OTM. The human has the correct T with action noises 0.1, whereas the machine over-estimates these as ϵ + 0.2 for vertical and horizontal moves, and 0.3 for diagonal moves. In this case, naive performs very badly, leading to a bad policy.<br/> Machine ends up paying a lot of cost for getting overridden (cm = 0.2) Human performs the best since they have the correct model of the task. The most interesting cases are the ideal and the centaur. In ideal, the machine knows the model of a human, but still wrong about the model. </p>
<p class="text"> Since it knows when will human override it, the machine is able to learn a policy that avoids getting overridden by the human. The centaur does not know the model of the human, and infers it from interaction. Clearly, it can do this quickly and perform similar to the Ideallyideal. </p>
