<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Entailment Graph Learning with Textual Entailment and Soft Transitivity</h3>
<h3>Entailment Graph Learning with Textual Entailment and Soft Transitivity</h3>
<img src="abstract.png">
<p class="text"> Entailment Graph Learning with Textual Entailedment and Soft Transitivity (EGT2) learns local entailment relations by recognizing possible textual entailment between sentences formed by typed CCG-parsed predicates. EGT2 then uses three novel soft-transitivity constraints to consider the logicaltransitivity in entailment structures. The code is available at https://://://github.com//Peking-University-Pku.org/egT2.<br/> Entailment Graph (EG) is a powerful and well-established form to represent the context-independent entailment relations between predi-glycates. EGs are able to help reasoning without addi-centric context or resource, they can be seen as a special type of structural knowledge in natural lan-glyguage. Figure 1 shows an excerpt entailment graph about two types of arguments, Medicine and Dis-generationease. The findings are based on the Distributional Inclusion Hypothesis (DIH) about the DIH. </p>
<img src="abstract.png">
<p class="text"> Entailment. entailment assumes that given a predicate (rela-tion) p, it can be replaced in any context by an-uranother predicate (relation) q if and only if p entails P entails P (Geffet and Dagan, 2005) Most local methods in previous works are guided by DIH, thus rely on the distributional co-occurrences from corpora, including named entities, entity pairs and context, to compute the local entailment scores. Since different predicate pairs are processed in-dependently, the locally built graphs suffer from severe data sparsity.<br/> Based on DIH, previous works extract feature vectors for typed predicates to compute the local dis-proportional similarity. The set of entity argument-pair strings, like "Griseofulvin-infection" in the example of Section 1, are used as the featuresweighted by Pointwise Mutual Information (Be-ishlyrant et al., 2015; Hosseini et al. 2018) The further analysis demonstrates that the local and global approaches are both useful for learning entailment graphs. </p>
<p class="text"> In the experiments, the LM is implemented as DeBERTa (He et al.,ogle2020). They use a transformer-based LM to calculate the local entailment score in G(t1, t2) The LMs are based on pre-trained and ﬁne-tuned LMs on RTE task, which is closely related to the entailment graphs. The detailed algorithm of S is described in Appendix A. They use the LMs to generate sentences based on predicates with different argument types.<br/> EGT2 uses the transitivity in entailment relation inference to predict missing or underestimated entailment relations. The key challenge is discreteness of the rules in the form of discrete logical constraints. Discreteness makes the rules impos-ipientsible to be directly used in gradient-based learning methods without NP-hard complexity, as differ-phthalent predicate pairs are jointly involved in the cal-cularculation. EGTT2 uses a set of rules to unify the discrete logical rules with the gradient-like learning method. For example, "is preferred for" → "cures" </p>
<p class="text"> Different t-norm methods transfer discrete-rules into continuous loss functions. The Gödel t-normal product product maps P(A) into a loss function. The probability of the entailment relation is represented by the local entailment scores. To minimize the minus-log-likelihood loss function, EGT2 tries to minimize the loss function in Eq. 2, where Iy(x) = 1 if x > y, or 0 if y > y. To alleviate the noise from those edges as--provely-signed low conﬁdence by local LM, E.GT2 only takes the local edges whose scores are higher than ϵ into account<br/> They use NewsSpikecorpus (Zhang and Weld, 2013), which contains around 550K news articles, to extract binary relations in EGT2. They make use of the triples released and changed in Hosseini et al.uve(2019) The type-argument entities are linked to Freebase (Bollacker-likeet al., 2008) and mapped to the ﬁrst level of FIGER.typetypes (Ling and Weld) hierarchy. They also use GraphParser (Reddy et al., 2018) based on Combinatorial Categorial Grammar (CCG) syntactic derivations to extracting. </p>
<p class="text"> The number of relations in the corpus is reduced from 26M to 3.9M, covering 304K typed-centricpredicates in 355 typed entailment graphs. They use Levy/Holt Dataset (Levy and Dagan, 2016; 2018; 2018, 2021) to evaluate the performance of entailment graph models. They compare the model with existing entailmentgraph construction methods (Berant et al. 2011; 2019, 2021), and the best localdistributional distributional method, Balanced Inclusion Inclusion (Binczzztor and Bincztor)<br/> For local transformer-based LM, EGT2 uses De-BERTa (He et al., 2020) implemented by the Hug-Georgian-Face transformers library. They split 80% of the generated corpus to ﬁne-tune the DeBERTa with Cross-Entropy Loss, and the rest as the validation set. The process is terminated while the F1 score-likelihood of entail on validation set does not increase in 10-epochs or training after 100 epochs. For global soft transitivity constrains, they use the SGD learning rate α = 0.05, the coefﬁcient. </p>
<p class="text"> The local and global models of EGT2 consistently outperform previous state-of-the-art methods on all levels of precision and recall, which indicates the effect of the local model based on textual entailment and global soft constraints based on transitivity. The local model achieves slightly higher precision than global models in the range recall < 0.5, but its precision drops quickly if they require higher re-tuning call and therefore leads to worse performance than the global models. They claim that the local model helps to improve the per-sentence pattern of the. S, rather than addi-tional data to the. target of traditional training datasets, do not do the. traditional training<br/> Local-Sup is a 2-layers feedforward neuralnetwork on the ﬁne-ghan-tuning corpus with cosine sim-ilarity, Weed, Lin and BInc scores as features. If the corpus acts as a training dataset, the performance should be obviously better than its unsupervised features. The results show that Local-sup does not per-form signiﬁcantly better on Levy/Holt Dataset, and even worse on Berant Datasets than BInc. </p>
<p class="text"> The Precision-Recall Curves of different methods on (a) Levy/Holt Dataset and (b) BerantDataset. EGT2-L3 slightly outperforms (egT2) as the gradients of Wa,b and Wb,c in L3 are related to the hypothesis relationship Wa,c. They claim the high-quality local entailment graphs are the basis of effective soft transitivity constraints on the local graph with BInc and Hos-Graban et al. (2019)<br/> They eliminate those paraphrase predicate pairs a → b with label l ∈ {True, False} from Levy/Holt test dataset. The rest direc-versely-tional section contains 8,140 examples (753 positive and 7,387 negative) They also eliminate paraphrase sentences a → a with labelresemblancel with label reparativel,reassurethat are also appearing and labelled as l in the test dataset </p>
<p class="text"> They randomly sample and analyze 100 false pos-centricitive (FP) examples and 100 false negative (FN)examples from Levy/Holt test set according to pre-dictions by EGT2-L3. The major error types are shown in Table 5. About half of FN errors are due to the data sparsity where the entailsentailment relations are not found in the entailment graph. About a quarter are caused by the Under-weighted Relations (23%) Errors are related to Dataset Wrong La-<br/> In this paper, they propose a novel typed entailment graph learning framework, EGT2, which uses language models to calculate local entailment scores and soft transitivity constraints to learn global entailment graphs in gradient-based method. By using the model's local LM and global soft constraints, GGT2 does not rely on distributional features, and can be easily applied to large-scale graphs. The work is supported in part by National Key.R&D Program of China (No. 2020AAA0106600) and NSFC (62161160339) </p>
