<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>title sample</h3>
<img src="_sum_2203.16781.html.1.png">
<p class="text">  Memes have transcended onto the boundary of online harassment against women and cre-ulentated an unwanted bias against them. The model relies on pretrained-based representations from different state-of-the-art transformer-based language models and pretrained image pretrained models to get an effective image representation.<br/> Women have a strong presence online, especially on image-based social media like Twitter, SnapChat, Instagram.78% of women use social media several times a day, compared to 65% of men. Some of the most popular communication tools in social media platforms are memes. Memes are essentially images characterized by the content of a picture overlaid with text that was introduced by people.<br/> They present team Poirot’s solution to SemEval 2022 Task 5 competition as described in detail in Fersini et al.(2022) They focused the efforts on building a Multi-Modal-Multi-Task module that uses features from both images and text.<br/> The task is comprised around two main sub-tasks: Sub-Task A: a basic task about misogynous or not misogynous. Sub-Tasks A: A task about a meme should be categorized either as misogynous, or not. A task should be about a misogynous meme, where a meme can be categorized as either a meme or not a meme. </p>
<img src="_sum_2203.16781.html.2.png">
<p class="text">  The competition is challenging, as iden-tifying the misogynous nature of a meme is more complex in a multi-modal setting than performing the same task only on textual data. The datasets for the competition provided by the organisers are memes collected from the web and manually annotated via crowdsourcing.<br/> The teams’ performance is evaluated by the macro-glyglyF1 score for task A. The weighted F1score is computed for each subtask (misogynous, shaming, stereotype, objectiﬁcation, violence), and the average F1 score of these subtasks is used to rank the systems. Each sample is sup-ipientported by an image and the corresponding text tran-ishlyscription (if it exists) on the image.<br/> The proposed approach leverages on multimodal information to pro-ceive the classiﬁcation of a sample. The approach can be broadly divided into binary approach and multil               abel approach. The pipeline is divided into two streams running in parallel which on later-generation stage is joint together.<br/> The output of the feature extractor would give an image of a feature extracted from the input image. The output image would give the output image I ∈ Rd×d, where d = 224. The input image I would then give an output image of the extracted image. This image image would be the image of an image extracted by an extractor. </p>
<p class="text">  The model consists of two essential parts: Feature Extraction Module and Graph Classiﬁcation Module. Multi-label model keeps the feature extracting pipeline of the network in binary model intact. Graph has an effective message passing system, which can be modelled to reflect the inter-dependency of labels among each other.<br/> The neural net-work can be used to penalize class weights in terms of class weights. Calculated weights for regularizing cross-repetitropy loss in the custom loss function. Calculated weights are based on the number of training and trial samples.<br/> They calculate the weighted importance of a given class using the below equations. The calculated weights are shown in table 2.2.1 and 3.4.1. They use the baseline provided by the task organisers which depend on the Sub-Task and use a different set of features for different tasks. </p>
<p class="text">  Feature Extraction Module has a pretrained ResNet, pretrained sentence transformer SBert. Features are passed passed through a 5-layer classiﬁer stack generated from the Graph Classi ﬁcation Module. Graph Classifier takes input the label’s semantic information to generate the output multi-label prediction. : The general architecture of the Multi-Modal-Multi-Label model.<br/> The model is based on the concatenation of deep im-centricage and text representations. It is based upon a single-layer neural network and a multi-label model for predicting if a meme is misogynous or not and, if misogynous, the corresponding type is the same. They normalize all the sentences by converting all white-space characters to a normalized space.<br/> They adopt a 2-layer graph network for the best performing system. For node features, they use the 300-Dimensional GloVe embeddings on the Wikipedia Dataset. Table 3 contains the list of general hyperparameters they used. Table 5 and 6 compares the macro and weighted-f1 scores of the best-performing models. </p>
<p class="text">  Major hyperparameters used for the best performing models. Parameter Name Name Name: "Peer-optimizer," "Pre-Trained BERT" and "Binary Multi-Label Backbone Model" λ effect on model performance is found in Table 4.<br/> The results of the experiments for binary classiﬁcation task can be seen in Table 5.1Task Results. They divide the model according to different types of loss used during the training stage. The results are shown in Table 6:. The results can be read in Table 4:. An additional navigator network (SM) on the image end outperformed the single modality models and the simple multimodal concatenation model.<br/> The results of the experiments for the multi-label classiﬁcation task can be seen in Table 6. Amongst the model using ResNet backbone, the model ﬁne-tuned on nsfw had an edge over the model which had been pre-trained on imagenet dataset. This can be an indication that there is an indicator of women’s image representation with the meme                being a misogynstic one. </p>
<p class="text">  They perform ablation studies from two different aspects, including the classiﬁcation model and the depths of the multi-label classi-label model. They determine the relative importance of the two models with respect to each other. They also use the depth of the Multi-Label Backbone Model to study the effects of the model perfor-orativemancerere-reform city of misogyny as compared to that of feature feature representations.<br/> The results are shown in table 4, where the per-formance of the two models based on ResNet-101 backbone are compared pretrained on two datasets. It can be observed that the textual-stream information is of higher importance in both the classiﬁcation problem as the performance boost is skewed for roughly 0.6 to 0.7.<br/> The possible reason for the performance drop may be that when using more GCN layers, the propagation between nodes will be over-smoothing. The result can be explained to the fact that that the model predicts a positivity for the label which has higher positive weightage, the loss value would increase.<br/> They have described the systems developed by as to successfully solve the Multimedia Automatic Misogyny Identiﬁ-iopcation challenge at Semeval 2022. In the best per-forming submission for SubTask-A , they framed the problem as a binary classi-classi-cation task and used two separate streams of information simulataneously to identify misogyny. For the model, they tried to ﬁnd the semantic relation between the type of misogyny and their relative importance </p>
