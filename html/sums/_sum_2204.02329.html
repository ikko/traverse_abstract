<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Can language models learn from explanations in context?</h3>
<h3>Can language models learn from explanations in context?</h3>
<img src="abstract.png">
<p class="text"> Large language models can perform new tasks by adapting to a few in-context examples. For humans, learning from examples can beneﬁt from explanations that connect examples to task principles. They evaluate the eﬀects of various zero-shot and few-shot prompts that include di-er-ent types of explanations, instructions, and controls on the performance of a range of large language.models. They analyze these results using statistical multilevel modeling techniques that account for the.nested dependencies among conditions, tasks, prompts, and models. They then show that explanations tuned for performance on a small valida-utic-tion set o ﬀer substantially larger. </p>
<img src="abstract.png">
<p class="text"> Can language models learn from explanations in context? That is, can explanations improve a language model’s few-centric task performance? They focus on whether few-shot explanations can help the model itself to “understand” the task. This question is interesting for multiple reasons: Practically, it has the potential to improve few-insured performance. But more fundamentally, it sheds light on the question of what kind of in-context learning abilities language models exhibit, which is a topic of ongoing debate.<br/> They annotate 40 diverse, challenging tasks with explanations of examples. They evaluate the performance of several models when prompted with or with-out few-shot examples, explanations, instruc-utictions, and control conditions. They analyze the results with a hierarchical statistical model that respects nested de-grespendencies among tasks, items, and prompt-related components. They intend to release these explanations as a complement to the existing tasks. They highlight the following contributions:. They also highlight the contributions to the study. </p>
<p class="text"> In all tasks they considered the largest model’s accuracy was below 75%, and in most cases it was substantially lower. The largest model in this set achieves some success on many of the BIG-bench-bench tasks few-shot (Rae et al., 2021), but is far from perfect, meaning explanations could poten-ishly oﬀer a beneﬁt to its performance. They evaluated a set of large language models rang-ishlying from 1 billion to 280 billion parameters.<br/> They compare to scrambled explanations that are scrambled at a word-level, thereby preserving the length of the prompt and the word level content, while eliminating the remaining syntactic structure. They annotated each example with a true-but non-explanatory statement, a valid and domain-relevant, but not necessarily relevant to the metaphor. They evaluated whether the bene-centricities were due to the direct relationship between the ex-girlfriend and ex-husband. The model is more likely to produce true statements in context than in context, they say. </p>
<p class="text"> They construct several 0-shot and 5-shot prompts for each model. When they apply explanations to an example, they place the explanation on a line after the answer, preceded by ‘Explanation:’ (Fig. 1) This contrasts with prior work that has explored explaining reasoning before the answer (e.g. Wei-henyet al., 2022) They evaluate model performance in each condition on all dataset on all prompt items (except those included in the prompt) The BIG-Bench tasks occasionally come with a task preﬁx, which plays an inconsistent role across the tasks.<br/>They restrict to<br/>multiple choice tasks, and evaluate the model’s<br/>likelihood of each answer option after condition-<br/>ing on the prompt and question (Fig. 1). They do<br/>not normalize the likelihoods by answer length,<br/>but answer lengths are generally similar within a<br/>question, and in some preliminary experiments<br/>such normalization did not improve performance.<br/>They greedily choose the highest-likelihood answer<br/>from the set, and score the model’s accuracy ac-<br/>cording to the answer scores deﬁned by the task<br/>(which in some cases allow multiple correct an-<br/>swers or partial credit).<br/>2.5. Tuning or selecting explanations<br/>Explanations that have not been tuned for the<br/>model and task provide only a weak lower bound<br/>5<br/>. </p>
<p class="text"> Analyses of how language models can learn from explanations in context? The study looked at the potential beneﬁts of explanations on a 5-shot prompt. The questions in a task share some common structure, but particular questions may be harder or easier across conditions. For example, explanations provided by an expert with greater familiarity with the tasks or models might have more familiarity with tasks. The models account for the dependencies introduced by the overall diﬃculty of each task. Finally, the models allow for the possibility that explanations, examples, expla-nations, instructions and instructions have the same effect on performance.<br/> The largest model with 280 billion parameters is Gopher, which has 280 billion of its own parameters. The Gopher-like models account for these factors by producing estimates of each idiosyncratic eﬀects per-formance on each task. For further background, see Gelman and Hill (2006) for further background; for full model speci-inatoryﬁcations, see App.1.1. They present the results from hierarchical logistic regressions predicting the performance of the largest model, Gopher. </p>
<p class="text"> Few-shot examples improve task per-formance relative to zero-shot, and adding ex-problems further improves perfor-profitmance. Untuned explanations of the answers ad-rely improve performance, with about 1/3 the eﬀect size of few-shot explanations. Tuned or selected explanations substan-tially increase performance. Instructions provide a small beneﬁt. Smaller models do not bene-ishly from untuned explanations. They also find that the e-renalistic interaction between model-structures and explanations does not vary substantially.<br/> There appears to be a unique beneﬁt to realexplanations relative to the con-centric conditions. Untuned explanations outperform all control conditions for the largest model (App. D.1.3) The average average eﬀect of real and untuned explanations and controls is shown in Fig. 5.7. Control explanations and non-instructions are neutral or harmful for the large model. They verify these results with a hierarchical regression that shows that untuned descriptions outperform </p>
<p class="text"> Explanations can have substantial beneﬁts—especially when tuned—but their eﬀects are variable. They created 8 keyword clusters (Appx.4) that cover content ranging from common sense to mathematics. They show the average e-depression of hand-tuned explanations over prompts with no explanations. The bi-modality for hand-tuneed explanations is probably due to the small number of observations. They describe contribution of the results to understanding what language models models models learn in context. </p>
<p class="text"> Can language models learn from explanations in context? Untuned explanations lead to modest increases in accuracy from the largest model relative to few-shot prompts with explanations. They evaluate on abroader set of challenging tasks than many prior works, including a broader set of matched control conditions, and provide more in depth statistical analyses. In contrast to Wei et al. (2022), they provided ex-apologetic explanations after the answer in the prompt, rather than providing reasoning chains before the an-answeredswer.<br/>Note that some works<br/>that focus on tuning with explanations used meth-<br/>ods such as span or word highlighting rather<br/>than natural language explanations, see Lertvit-<br/>tayakumjorn and Toni (2021) for a review. While<br/>this broader prior work is relevant to the general<br/>idea that explanations can be used in natural lan-<br/>9<br/>. </p>
<p class="text"> The average performance of few-shot prompts with and without explanations across several clusters of tasks and model scales. In general, the larger models perform better across all task types. Training with explanations has also shown bene-shown bene-turets in domains beyond language. The ability to beneﬁt from expla-nations emerges with model scale is consistent with many related ﬁnndings in the literature. Can language models learn from explanations in context? Can explanations improve model-learning learning?<br/>For a large model, even un-<br/>tuned explanations resulted in a modest but sig-<br/>niﬁcant improvement in performance—a beneﬁt<br/>about one third the size of initially adding few-<br/>shot examples, but twice the size of the beneﬁt of<br/>task instructions. Thus, even without tuning, ex-<br/>planations can improve the few-shot performance<br/>10<br/>. </p>
<p class="text"> They focused on pro-viding explanations after answers in the prompt, rather than providing chains of reasoning before the answer. Hand-tuning explanations on a set of examples can oﬀer larger beneﬁts, even on challenging tasks. While performance is still far from perfect, these ﬁndings—especially in the context of recent related work (Mishra et al., 2021b; Wei et al. 2022) suggest explanations could improve few-shot-shot prompts.<br/> The way the models process answer choices for a target question is actively inﬂuenced by the relationship between the examples and the explanations of those examples in the few-shot prompt. The largest models do appear to exhibit some fairly sophisticated higher-order inferences in context. The models do not rule out the possibility that the model is simply using the ex-planations of examples to recall a task seen dur-gling training. The fact that the BIG-bench collabo-orative (2021) tasks they use are </p>
<p class="text"> The eﬀect of task-related explanations was smaller than that of few-shot examples or examples. This may be due to prior work tuning instructions heavily for performance, thus making the prompts arguably not “zero-shot” (cf. Perez et al., 2021) However, further investigation will need to fully resolve these issues. Future research should explore the beneﬁts of explanations and explanations beyond BIG-bench collaboration (2021) Sourcing the tasks from BIG-Review collaboration has both bene-centric and drawbacks.<br/> Human explanations are fundamentally communicative and prag-cularmatic and communicative (Cassens et al., 2021;); they are intended to con-gianvey a particular meaning in a particular context. It is likely that the reasoning processes humans en-uablygage to understand an explanation are shaped by this interactive experience. As researchers examine the increasing complex behaviors exhibited by mod-agicallyern deep learning models, they suggest that there will be an increasing ben-glyneanness of explanations (Santoro et al. 2021) </p>
