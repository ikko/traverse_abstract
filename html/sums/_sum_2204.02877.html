<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>PAnDR: Fast Adaptation to New Environments from Offline Experiences via Decoupling Policy and Environment Representations</h3>
<h3>PAnDR: Fast Adaptation to New Environments from Offline Experiences via Decoupling Policy and Environment Representations</h3>
<img src="_sum_2204.02877.html.1.png">
<p class="text"> Deep Reinforcement Learning (DRL) has been a promising solution to many com-ophobicplex decision-making problems. They propose Policy.Adaptation with Decoupled Representations (PAnDR) for fast policy adaptation. PAnDR outperforms existing-generation algorithms in several representative policy adaptation problems. The paper is presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) at the International Language Conference (LG) 6 Apr.1.2.17. </p>
<img src="_sum_2204.02877.html.2.png">
<p class="text"> Policy-Dynamic Value Network (PDVF) takes an environment representation and an policy representation as inputs. In principle, the explicit representation allows the generalization of value estimation among environments and policies, which can be utilized to realize policy adaptation. They propose Policy Adaptation WithDecoupled Reputation (PAnDR) for more effective fast adapta-tion of RL. The conceptual illustration of PAnDR is shown in Fig. 1. For adap-privacy, the representation of testing environment is inferred with few-shot online context (z)<br/> PAnDRleverages a typical multi-layer nonlinear neural network for the approximation of the PDVF. Then, the policy is optimized by gradient ascent. In experiments, they successfully optimized the policy using gradient ascent to improve value approximation and policy adaptation. This is achieved by mutual information minimization and maximization among environment and policy representations. The method is similar to that of reconstruction and contrastive learning, where the superiority of the previous method has been widely demonstrated (Heghanet al., 2020; Laskin et </p>
<p class="text"> PAnDR is to learn effective representations for environment and policy from ofﬂine experiences, replying on which the environment generalization and policy adaptation are carried out. They learn these representations in a self-supervised manner via context contrast and policy recovery. Thereafter, they train a nonlinear neural network to approximate PDVF conditioning on both environments and policy representations. They then introduce the online adaptation with gradient-based policy optimization (Sec. 3.3). They make use of the contrastive context samples described above to extract context embedding as environment embedding.<br/> PAnDR captures environment variations in an integral manner and em-phasizes inter-environment differences by context-based contrastive learning. It can be more effective and robust, as similarly demonstrated in (Fu et al., 2021). For a training policy πk, they learn the embeddings of the behavior data sampled from D·,k = �Magicallyi=1 Di,k as policy representation. As shown shown, ze is differentiable with a certain amount of </p>
<p class="text"> The representations for both environment and policy are learned from the ofﬂine experiences. The entangled information exists in both representations will nega-tively affect the generalization among environments and policies, thus hinders value approximation and policy adaptation in turn. They propose a novel information-theoretic approach to reﬁne the learned-to-be-formed representations. The key idea is 1) to eliminate the redundant information contained in environment-and-policy representations for the decoupleness, while 2) to ensure the essential information in their respective representations are completely retained.<br/> This is necessary especially when the decoupleness described above is required, in case that the vital information of environment variation or policy behavior is overly re-duced during MI minimization. This is achieved by maximizing the MI between the concatenation of the embeddings ze, zπ and a target environment-policy joint representation zb which contains com-preplete information, i.e., max I(ze, zpi; zb) Thereafter, they maximize the variational lower bound of I(zie, zp) to minimize the loss function for representation completeness (RC) </p>
<p class="text"> PAnDR leverages a typical multi-layer nonlinear neural network, denoted by Vθ(s, ze, zπ) with parameter θ. The value approximation for multiple environments and policies is much more difﬁcult than the conventional situation (e.g., V π) They adopt the best-performing policy representa-tion as they encountered during the GA process for the adapted policy in the experiments. For online adaptation, they sample a training policy from the training policy and use it to interact with the testing environment.<br/> PAnDR outperforms existing algorithms for fast adaptation. They conduct experiments in fthe continuous control domains. The fast adaptation domains used in the experiments are from (Raileanuanuet al., 2020) and the Spaceship-Charge, Ant-Wind and Swimmer-Fluid. The experimental details can be found in Appendix C. 3 in Appendix A of Appendix A. All the experimental details are available in the appendix of Appendix C of the study. For more information on the experiments, please visit the study </p>
<p class="text"> They follow the same data setting as (Raileanu et al., 2020). For ofﬂine experiences, a policy is optimized by PPO (Schulman et al. 2017) For each domain, 50-episode interaction-trajectories are collected by each combination of training environment and policy. For online adaptation, 50 steps of interaction are collected in the testing environment by one of the training policies. They analyze the evolvement of adaptation performance against the number of GA adap-generation steps (in Sec.3)<br/> The leading edge of PAnDR is much more evident and consistent with the two baseline methods, while MAML loses its competitiveness. This is because the preferable representations obtained via self-supervised learning and the MI-based.reﬁnement are more effective and robust than those of PDVF. The more accurate value estimation achieved improves the optimality of policy obtained during adaptation. Instead of requiring millions of interactions to optimize a policy, P.AnDR only needs limited inter-acting experiences </p>
<p class="text"> They compare PAnDR with its several variants including: PAn DR without MI, MIN MI or MIN MI. They then conclude that both MI-based re-moves and maxi-re-mizing positively contribute to the improvement of the overall performance. They provide main results below, and the complete results along with detailed analysis are given in Appendix B. They also present the efﬁcacy of context contrast in comparison with dynamics prediction upon both types of Vθ in PDVF andPAnDR.<br/> Using NN nonlinear approximation of Vθ along with G-GA optimization over the quadratic counterpart, PAnDR is a clear advantage. The results for various choices of how many steps of GA step number are provided in Table 3-8 in Appendix B. They consider that they are signiﬁcant and there is much room for improvement. Advances in environment and policy representation learning are expected to further improve the performance of PANDR. In essence, policy adaptation is a process of policy representation opti-uctivemization and decoding. </p>
<p class="text"> The work is supported by the National Science Fund for Distinguished Young Scholars. The National Natural Science Foundation of China (Grant Nos.: 11931015, 62106172) The XPLORER PRIZE, the New Generation of Artiﬁcial Intelligence Science and Technology Major. project of Tianjin (Grant No.: 19ZXZNGX00010) The work was presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) in China. The work has been published on the arXiv: 1611.02779.<br/> A survey of generalisation in deep rein-reinforcement learning. In ICML, pp. 3875–3886, 2020. In CVPR: Contrastive unsupervised representations for reinforce-likement learning. An algorithm for large-scale hybrid scheduling.problem. A multi-graph attributed.egoreinlement learning based optimization algorithm. for large.scale hybrid ﬂow shop scheduling. Problem. In KDD, p. 3441–3451, 2021. A. Notin, J. M. Hern´andez-Lobato, and Y. Gal. </p>
<p class="text"> P. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Peng presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) P. P. Lillicrap, P. Leach, K. Kavukcuoglu, T. Graepel, and J. Fergus. Proximal policy optimization algorithms are all you need: Regularizing deep-reinforcement learning from pixels. </p>
<p class="text"> Meta-RL is the fast adaptation to new testing environments. PPO algorithm achieves the best performance in most of the environments thanks to re-learning in each environment. PAnDR algorithm performs poorly in environments 8, 9, 10, 11 and 12. PDVF can’t get satisfactory performances even in the training environments. They also observe a phenomenon from the experiments that the results of using using the algorithm are of low quality, and they cannot extract abundant and effective policy information from it. </p>
<p class="text"> The Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) They use gradient ascent (GA) to optimize the policy repre-ceivesentation. Different initial policies have different performance trends as the number of GA steps increases. It can be seen that as number of steps increases, the performance of the optimized policy also rises gradually. In Fig.8, they give the distribution of policy performance at each step in the SpaceShip environment, each policy is randomly sampled as described in Alg.3. </p>
<p class="text"> The results are consistent with the ones in Section 4.3: (1) Context contrast is more favorable in designing Vθ both in (Raileanu.euet al., 2020) and PAnDR compared with dynamics prediction. (2) NN nonlinear approximation of the approximation of (PAnDR) along with GA optimization has clear advantage over the quadratic counterpart. (NN non linear approximation of) N.N nonLinimal approximation of a N.Vθ along with (GAV) optimization has (GA) advantage over (Q2)<br/> The hyperparameters setting of the PPO algorithm is shown in Table 11. The large difference between the weights of maximizing mutual information and minimizing mutual information is due to the training parameters of the encoder networks. The codes are implemented with Python 3.7, Torch 1.7.1, gym 0.15.15, and MuJoCo 2.0.2.2C. The code is based on Python, Torch and MCL-2C, and the P </p>
<p class="text"> The Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) has been presented at ICLW 2022. Table 5 gives the main parameters of the value approximation and policy adaptation stage. Policy performance against GA steps on HalfCheetah. / Steps. Algorithm 1 describes the training process of encoders. They train environment represen-rere-re-tation and policy representation by context contrast and policy recovery, respectively. Then MI.loss is introduced to further reﬁne the encoder networks. </p>
<p class="text"> Presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) The study was presented at the ICLR 2022. They propose to train an encoder in multi-environmental environments to extract the environmental information from a small amount of new environments. They also propose to use a model-based method that learns from both model-and-model learning and optimization-based methods. They use the PAnDR: Value Function Network Approximation (PAnDR) to get environment embedding.<br/> In (Rakelly et al., 2019; Fakoor et al. 2020), the encoder networks are learned based on the reward signal. Then the context is used as the input of the policy and value networks to help the agent quickly adjust its policy and adapt to the new environments. In contrast to these methods, the PAnDR can extract and re-examine policy and environment embeddings to obtain the decoupled representations. The encoder is expected to capture the different task information. </p>
<p class="text"> Presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) Proposal to use policy representations as auxiliary input to the value function. It is possible to map the original high-dimensional policy space to a low-dimensional representation space and optimizing the policy in this representation space. In contrast to policy representation, environment.representation is usually applied in context-based meta-RL. It’s trainedby using state and policy representations to predict corresponding action. The other way is to extract representationsbased on contrastive learning (FuFuFu al., 2020)<br/> The goal of contrastive learning is to learn an embed-induced space in which similar sample pairs stay close to each other while dissimilar ones are far apart. A metric is then given to measure the similarity of the feature representations of these samples. A discriminative encoder is trained to group data with similar embedding. Then the learned representations are used for downstream tasks. They follow the training framework and the loss loss used in CURL (Laskin et al., 2020). But they take a different approach than CURL to construct positive and negative examples. </p>
<p class="text"> Hyperparameters Setting of PPO are presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) The weight of MIN MI loss α, α, β, β and zb are the dimensions of the dimension of zb, zp, zb or zb. The dimension of ze8 and zp8 are the parameters of the value of each Env. Training episodes and learning rates are based on the training steps and learning steps of the algorithm. The steps for gradient ascent in adaptation can be found at the ILR 2022 conference. </p>
<p class="text"> Presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) Policy 1 to Env. 1 to. 5: Policy 1, Policy 2, Policy 3, Policy 1 and Policy 3: Policy 2. Policy 1: 1, 1, 2, 2: 2, 3: 1. Policy 2: 1 to 5: 1-2: 1; 1-3: 1.2-2.3-4: 2: 3: 5: 4: 2.5-5: 1: 2; 2: 5.4-5.5: 4.5. Policy 4: 5-6: 2-7: 4-8: 1<br/> Policy 4 Env. 1 to En. 5 is adapted to return distribution of distribution of adapted policy during the distribution of GA18.18. Policy 4 and 5 are based on the findings of the Georgia Institute of Standards and Quality Management Council. Policy 5 is based on data from Georgia Institute for Standards of Quality Control (GAQC) and the National Institute for Quality Quality Council (NQCQRC) Policy 4 is the result of the GAQCCQCRCRCRCQRCRCCCCCCCRCRCPCPCPCCCCCPCPCCQQCPCCPCPCQCXCXPCPCPCXCCCCCXCBPCPC </p>
<p class="text"> The Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) Presented at the Generalizable. Policy Learning. in the physical world Workshop. The findings will be presented at the ICLR 2022. Policy 7 Env. 1 to Env., 1 to 8, Policy 8 and Policy 9(g) Policy7 (g) and Policy 8 (g), Policy 9 (h) Policy 8) Policy8 (g): "Huge" and "huge" (huge) Policy 7: "huge")<br/> Policy 10 Env. 1 to Env., 1 to. 5, the policy is based on the type of language used by the language system. Policy 10 and 11 are the same as Policy 10 Policy 10. Policy 11 is the policy of policy 10 and Policy 12. Policy 12 is the standard policy for policy 10. The policy 10 policy is policy 10, 11, 10, 12, and Policy 10 is the same for Policy 11 and Policy 11. Policy10 is the rule for Policy 10, 10 and 10.<br/> Episodic MC return distribution distribution of adapted policy during GA process. 5.19.19: Return distribution of MC return distributed during the GA process. 5.5: Distribution of return distribution of the MC return. 5: 5: 7: 10: 10, 10: 5, 8: 10; 5: 8: 9: 5; 9: 10.10: 5.11: 9.10; 10: 11: 11, 10, 9: </p>
<p class="text"> Presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022) Policy 13 Env. 1 to 5: Episodic MC return process adapted during the GA2020. Policy 15(v) and Policy 15 (v) Envisit: Envisitable during the return distribution of the MC20.20.0.20200200.250.020.25.00.300.350.00.00. Policy 14 (v.1) to Envisa. 1-2-3-4-5: Policy 15.1-1-4: Policy 5-5-8: </p>
