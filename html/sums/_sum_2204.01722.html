<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Performance Portable Solid Mechanics via Matrix-Free $p$-Multigrid</h3>
<h3>Performance Portable Solid Mechanics via Matrix-Free $p$-Multigrid</h3>
<img src="_sum_2204.01722.html.1.png">
<p class="text"> Performance-Portable Solid Mechanics via Matrix-Free p-Multigrid? They demonstrate the reliability, efﬁciency, scalability of matrix-free p-multigrid methods with algebraic multigrid coarse solvers. They investigate accuracy, cost, and execution time on multi-node CPU and GPU systems for moderate to large models using AMDMI250X (OLCF Crusher), NVIDIA A100 (NERSC Perlmutter), and V100 (LLNL Lassen and OLCF Summit)<br/> Nonadaptive high order elements reduce complexity by being drop-in substitutes for low order elements. Quadratic and higher-order elements are often shown to be more accurate per deformation analysis despite the presence of certainsingularities preventing any asymptotic bene-tors. Such methods are available in niche commercial products such as StressCheck as well as open source ﬁnite element libraries –, but are rarely used in production computational engineering. </p>
<img src="_sum_2204.01722.html.2.png">
<p class="text"> The paper is organized as follows: section II introduces the hyperelastic formulation and section III describes the solver and implementation. section IV investigates accuracy in terms of mesh resolution and number of DoFs. section V investigates accuracy and solver robustness. section VI discusses implications and opportunities for further research. They demonstrate that high order methods improve accuracy per DoF for hyperelastastic simulations of multiscale structures, even at coarse tolerances in the presence of singularities. They also demonstrate that such models can be solved robustly on a range of modern architectures at a fraction-of the cost of per linear elements.<br/> An isotropic Neo-apologetic Hookean material is deﬁned by its strain energy density. They use the Jacobian form of a (u, v) as The residual (2) and Jacobian (4) forms require derivatives with respect to (solution dependent) current conשּׁ�guration. They pull these forms back to reference coordinates by way of the chain-rule rule. They explain this approach for a general Dirichlet problem: ﬁndo.<br/> The discrete form of (7) is given by Ee. Ee is the element e restriction operator that separates DoFs based on the elements they belong to, and Λ repre-repre-phthalsents pointwise function evaluation. The diagonal weighting of W e = det (det (∇ξX) ˆW ⊗ ˗W ≪�W e is a quadrature weighting weighting operator. The </p>
<p class="text"> Matrix-free p-multigrid methods provide an efﬁcient preconditioning framework for obtaining uniform convergence rates with re-spect to resolution and model extent. The multigrid algorithm is applied recursively, with Galerkin coarse-to-covery operator Ac = P T (P T) Ac, Pctof (Pctof) is constructed matrix-free until coarsening to linear elements, then via algebraic multiigrid. They use the transpose of the prolongation operator as the ﬁne to coarse grid restriction operator.<br/> The Jacobian on each level is represented by the 17 scalar values per quadrature point, ∇xξ, τ, log J, and log J. Coarsening from Q2 to Q1 elements reduces the number of DoFs by a factor of 8. When using direct solvers in 3D, this reduces the vertex-separator by 4 and thus supernode factorization(the asymptotically dominant cost) by 64. CPU backends call conventionally compiled functions (10) and Jacobians (11) at quadratures points, thereby enabling a rich debugging experience. </p>
<p class="text"> PETSc has developed a new interface in PETSc that enables efﬁcient GPU-based sparse matrix assembly with strong encapsulation. PETSc’s interfaces splits COO assembly into symbolic MatSetPreallocationCOO in which the row[], col[] are provided, followed by one or more calls to MatSet-ValuesCOO. The new COO-based assembly avoids data races with new algorithms, and handles MPI-parallelism. The resulting kernel is compiled at runtime via NVRTC/hipRTC, inlining the above call and making loop bounds and memory access offsets.<br/> When the number of basis nodes per element is low (up to and including Q2 hexahedra), a two-dimensional thread block processes the row and column combinations in an element’s output matrix. When this design would exceed the allowed number of threads per block, the assembly switches to a one-dimensional block with an additional loop in the kernel. Each accumulated value is assigned to the val[] array at a speciﬁed index determined by element and component ordering, and the array is provided to MatSetValuesCOO. </p>
<p class="text"> They perform a convergence study using linear and high-order geometry meshes produced by Gmsh They observe that very coarse meshes are the most efﬁcient path to accuracy. High order methods have better accuracy con-stants, but they are rarely used in practice, mainly because assembly and linear algebra are so much more expensive (no improvement in asymptotics) They consider conformal meshes that attain comparable accuracy with fewer DoF and fewer elements with fewer elements.<br/> To generate such meshes, start with a 24-element 2D manifold mesh of a single unit cell embedded in 3D, replicated to the prescribed extent in each embedding. This mesh is partitioned and distributed using ParMETIS, then re-designed with nodes projected to the closest point on the implicit surface. Larger and smaller models are created by changing the extent, keeping the applied surface traction constant so the deformation is similar. These models provide excellent tests for solvers since they show all compressive and bending modes, nonlinearities are activated at local and global scale. </p>
<p class="text"> Simulation used 2 reﬁnements, 2 layers, thickness 0.2, and Q2 elements. Table I quantifies the accuracy of a solve with linear, Q2, or Q3 elements. Increasing the number of layers in the mesh helps decrease the relative error of the simulation. They see that the Q2 and Q3 solutions are more accurate per DoF than those with linear geometry. They present GPU-based results on LLNL's Lassen, OLCF’s L.L.Mutter and NERSC's Perlmutter.<br/> Perlmutter consists of nodes with one AMD EPYC 7763 CPU connected via PCIe-4.0 to 4 NVIDIA A100 40 GiB GPUs. The GPUs are connected to each other with NVLink-3 and each node has 2 Cray network interfaces connected to the CPU. Table IIdescribes the environment used on each machine. Lassen (Spectrum MPI) and Crusher (Cray MPI), but was disabled on Summit and PerlMutter because of bugs. </p>
<p class="text"> The parallel operator application efﬁciency running on Lassen with aijcusparse and matrix-free shell operator representations. Both are latency-limited for smaller problem sizes and plateau as memory is ﬁlled for larger sizes. The high order discretizations (with more nonzeros per DoF) nearly saturate the STREAM bandwidth. The model at order 2 has about 30 DoF per element and each element has 27 Quadrature points that must store 17 ﬂoat64 values each, resulting in about 140 B/DoF.<br/> The impact of latency is ever-present, with memory capacity limiting the right end of each curve. Figure 10 depicts the relative costs and relationship of the major phases, with linear solve (KSPSolve) and pre-conditioner setup (PCSetUp) dominating. The linear solve(Figure 11 and 12) is communication-intensive since each application goes through a V-cycle (with an increasing number of levels as the model gets larger) Figure 13 considers preconditioner setup, which consists of preconditionser setup. </p>
<p class="text"> Problem sizes range from 2.7 MDoF to 1.9 GDoF with hypre/CUDAMatSetPre..Preconditioner setup efﬁciency spectrum for Q2 ﬁnite elements using matrix-free Newton-Krylov with p-MG preconditioning and BoomerAMG coarse-marshalizer. The times and ef encies are summed over all Newton steps. The preconditioner is based on the use of Galerkin products to calibrate the smoothers.<br/> Jacobian assembly is nearly perfect problem-size independence at about 4GDoF/s/GPU (over all solves, with Q2 elements) on Crusherand Perlmutter. Jacobian assemblies and preconditioner setup both decrease when going to Q3 elements. They observeup to 2x performance improvement when using L-BFGS as described in subsection III-A. The use of matrix-free methods requires quadrature-based linearization (“partial assembly”) of forward (and possibly adjoint) operators. </p>
<p class="text"> Enzyme is a new LLVM plugin with GPU support that provides split forward and reverse mode AD on LLVM. Split mode populates a ‘tape’, which con-tains opaque intermediate values at quadrature points, and is stored in ordinary libCEED arrays. The resulting vectorized code is on par with hand-written code that doesn’t exploit symmetries andancellation. Table IV compares total solve time (over many.problems) on a small cube mesh with 3630 DoF on a single process of an AMD EPYC7452.<br/> They have shown that this performance landscape is transformed by changing data structures to matrix-free representa-tions with linearization deﬁned at quadrature points. The use of high order elements has the additional coarser meshes can be used, thus reducing pre-processing time and I/O costs related to element quality. They ﬁnd that the algorithms here provide substantial bene-tutalready for quadratic elements, and thus is a viable drop-worthy drop-hardware. </p>
<p class="text"> This research is supported by the Exascale Computing. project (17-SC-20-SC), a collaborative effort of two U.S. Department of Energy. organizations (Ofﬁce of Science and the National Nuclear Security Administration) They thank PETSc, hypre, and libCEED developers, especially for their contributions. They note that shell structures usually have small.vertex separators and thus direct solvers and parallel adaptive adaptive adaptive.solvers such as PETSc’s PCBDDC offer sharp.convergence guarantees at manageable cost.<br/> P. Frauenfelder and C. Lage. Concepts—An Object-Oriented Software.Package for Partial Differential Equations. Mathematical Modelling and Mathematical. Analysis, 36(5):937–951, 2002. P. F. Bangerth and Oliver Kayser-Herold. C. Fergiele Panozzo. Panozzi. The ESRD. has developed a software called StressCheck Professional. The software is based on a free software that analyzes stress-check.computing.com.<br/> Ananias Tomboulides, Vladimir Tomov, Arturo Vargas, Tim-othy Warburton, and Laetitia Le Pourhiet have worked on a computer algorithm for long-term lithospheric dynamics. The results have been published in the Journal of High Performance Computing: Parallel Computing, 108:102841, 2021. The results are based on the work of David May, Jed Brown, Michael J. May, and Ian Karlin, who worked on the algorithms for efﬁcient exascale discretizations. </p>
<p class="text"> Researchers at the Argonne National Laboratory are working on PETSc for GPU-based exascale systems. They have proposed new ways to speed up performance of PETSc/TAO users manual. The results are published in the Proceedings of SC14: International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2014. The paper has been published by the American Society of Mechanical Engineers, ASME, and the University of California-based National Institute of Technology, Stanford University, in California, California, and MIT.<br/> Researchers: Accelerating small matrix multiplications by runtime code. In Proceedings of the International Conference for High.Performance Computing, Networking, Storage and Analysis, SC ’16. In Springer, 1982. Guide to multigrid development. In Multigrid methods, in Springer, 1980s. In IMR, pages 343–354. Citeseer, 2002. P-version: 1607.04245, 2016. arXiv:1607.4245,. The PetscSF scalable commu-funding layer.<br/> Researchers at the International Conference for High. Performance Computing, Networking, Storage and Analysis, SC ’21, ‘�21,’ and ‘20,” will be held in New York, NY, USA, this year. Researchers have been working on a new approach to 3D solid mechanics and nanatomaterials. They have been able to print aluminium alloys using selective lasermelting to make them more resilient and more resilient using laser-melting.<br/> Engineering with Computers, 20(3):273–285, 2004, 2004. The GLVis: GLVis is an open-source visualizations of high-order methods. The book is published in TILDA: Towards Industrial LES/DNS in aeroonautics, pages 321–345. The book, "TILDA," is published by Springer, in 2021. It will be published in the U.S. and Europe in October, 2018. For more information, visit www.glvis </p>
<p class="text"> Stefano Zampini. PCBDDC: A class of robust dual-primal methods in.PETSc.Sc. published in. SIAM Journal on Scientiﬁc Computing, 38(5):S282–S306, 2016. Stefano. P.Zampini. P.P.D: P.B: A robust set of robust, robust and reliable methods in computer science. PZ: PBDDC is a robust and robust class of methods that can be used in a robust manner. </p>
