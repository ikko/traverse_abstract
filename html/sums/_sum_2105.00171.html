<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>title sample</h3>
<img src="abstract.png">
<p class="text"> The AlloST: Low-resource Speech Translation without Source Transcription. The framework is based on an attention-based sequence-to-sequence model. The encoder generates the phonetic embeddings and phone-aware acoustic representations. The decoder con-forces the fusion of the two embedding streams to produce the target token sequence.<br/> More than 7,000 different languages are spoken in the world. They explore the use of byte pair encod-uctiveing (BPE), which compresses a phone sequence into a syllable-like segmented sequence. The performance is close to that of the existing best method using source transcription.<br/> Speech-to-text translation (ST) takes text as input to generate text in another language. The input of speech translation is just a speech signal stream. The advantage of this cascading approach is that it can make the best use of text and speech re-naissancesources.<br/> When the transcription of the source speech is available, the ST model that implements the end-to-end method of joint train-ing of ASR and MT components can effectively translate the given source speech. The jointly trained model can provide good initial model parameters for the ST-only task.<br/> Salesky and Black proved that phone features are very effective for both cascading and end-to-end STapproaches under low-resource conditions. Phone features are a better substitute for the textual resources of a language, they say. The results leave us.certain things to imagine: Can they obtain reliable phonetic in-forming of a speech utterance of an unwritten language?<br/> Over 3,000 languages have not yet developed a writ-ipientten form, but are still in use. For these languages, text-resources usually do not exist or are in poor condition. How to directly trans-late and document low-resource unwritten languages becomes a problem they are trying to solve.<br/> In, Li et al. developed a neural system for extracting a universal phone se quence from a speech utterance allosaurus was built using the PHOIBLE dataset, a curated phone dataset containing morethan 2,000 languages. The results show that the method outperforms the conformer-based end-izersarXiv:2105. </p>
<img src="abstract.png">
<p class="text"> The proposed model is open-sourced and is named AlloST. The traditional phone recognition method either recognizes speech to the international phonetic symbols (e.g., IPA) of all languages or phones of a speciﬁc language. The performance is very close to that of the existing best approach using source transcription.<br/> Allosarus, a language-independent phone recognizer, aims to increase accuracy under low-resource conditions. It leverages knowledge from other languages to predict the phone inventory of an unknown language. Allosaurus has two execution modes: 1) producing lan-gianguage speciﬁc phone sequences for known languages; 2) pro-ducing the IPA sequence for any language.<br/> In the experiments, they adopt the default setting be-cause they assume that the source language is unknown. Because the training data contain a Spanish corpus, it produces more accurate phone recognition results for Spanish than Taigi. They adopt BPE to segment a phone sequence to ﬁnd possible morpheme units. They assume that phone labels can be used to express a certain level of semantics.<br/> BPE was used to solve the problem of out-of-vocabulary (OOV) However, it can also capture the relationship between pronunciation and semantics. They use trainable embeddings with positional encoding to transfer semantic information from raw speech to high-level presentations, and then fuse them with acoustic features.<br/> Allosaurus’s phone se-quence output does not contain time information. Multi-head attention is used to learn the alignment between phone features and acoustic features. They use a similar structure to achieve the goal of proofreading. They use the basic setting of the transformer decoder, but add an additional stacked multi-head. layer after the origi-glynal multi-headed attention layer to deal with the phone embedding. </p>
<p class="text"> The results of different ST models for two examples in the Fisher Spanish-English corpus. Text in green stands for correct translations in this example, and there are no missing words in other methods. Impact of BPE on the BLEU scores on the Fisher.Spanish-English. corpus.<br/> Fusing phone features and acoustic features in both Encoder and Decoder at the same time is generally more effective than fus-forming phone features in Encoder or Decoder alone. Fusing the phone features with the acoustic features and the target tokens that have been translated in Decoder alone are not as effective.<br/> The second task of this study is ST for the Taigi-Mandarin pair. Table 2 shows that “Decoder Fusion” is more effective than the combi-ishly nation of the two. Table 3 shows the translation results of different models for two examples in the test set.<br/> In the second example, “Conformer w/ MTL” incorrectly translated the word “Cds” to “have” instead of “has” and “Have”. In the first example, the translation of ‘Cd’ and ‘D’ was incorrectly translated to ‘cds’ in the first word.<br/> As the BPE unit inventory increases from 230 (the size of IPA) to 1k, the phone-sequence length can be reduced by about 30% ∼ 60%. They believe combining BPE-based phone features will enable the model to have a deeper understanding of sentences. They con-itionallyducted experiments using different sizes of BPE vocabulary.<br/> Fusing raw phone features with acous-roustic features will make the model pay too much attention to the pronunciation and ignore the meaning of the sentence. Fusing the acoustic features in the encoder, decoder or both can improve the performance, but the best setting depends on the language pairs.<br/> This work was supported in part by MOST-Taiwan under Grant:110-2634-F-008-004. Acknowledgments: The authors of this article are happy to acknowledge that their work has been published in the U.S. and Taiwan. </p>
