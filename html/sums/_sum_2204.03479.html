<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Delta Keyword Transformer: Bringing Transformers to the Edge through Dynamically Pruned Multi-Head Self-Attention</h3>
<h3>Delta Keyword Transformer: Bringing Transformers to the Edge through Dynamically Pruned Multi-Head Self-Attention</h3>
<img src="_sum_2204.03479.html.1.png">
<p class="text"> Zuzana Jelčicová and Marian Verhelst: Bringing Transformers to the Edge through Dynamically Pruned Multi-Head Self-Attention. In Proceedings of tinyML Research Symposium (tinyMLResearch Symposium’22). ACM, New York, NY, USA, 8 pages. The authors propose a dynamic pruning method, which exploits the temporal stability of data across tokens to reduce inference cost. The threshold-based method only retains significant differences between the subsequent protections, effectively reducing the number of multiply-accumulates, as well as the internal tensor data sizes.<br/> The method does not require any training and can, therefore, be used directly in the existing pre-trained Transformer models. It reduces computational complexity during inference and offers intermediate data compression opportunities. The evaluation is done on a pretrained Keyword Transformer model (KWT) using the Google Speech Commands Dataset The results show that the num-glyber of computations can be reduced by 4.2𝑥 without losing any accuracy, and 7.5𝚥 while sacrificing 1% of the baseline accuracy. This work represents the next step to enable efficient inference of Transformers in low-power edge devices. </p>
<img src="_sum_2204.03479.html.2.png">
<p class="text"> The DeltaTransformer is based on the KWT, proposed in. The key component in Transformers is a stack of several identical Transformer blocks. Each Transformerblock comprises of Multi-Head Self-Attention (MHSA), Multi-Layer (MLP), layer normalizations, and residual connections. The typical Transformer encoder adopted in KWT consists of a stack. It consists of several. identical. Transformer. blocks, with several attention mechanisms (heads) that attend to different parts of the inputs in parallel.<br/> A learnable positional embedding is added to form a final input to the Transformerencoder: 𝑋𝑃𝐸 + 𝑥 is added. The matrices are then divided into attention heads to perform the self-attention computations in parallel. A high-level overview of the KWT model along with its dimensions. The model is based on the model of KWT and its dimensions: XCE, XMFCCW0.<br/> MHSA is defined as concatenation of the attention heads, weighted by a projection matrix. MHSA output is then added to the input 𝑋 with a residual-referred connection and passed though the first layer normalization and the MLP block, followed by another addition of a residual input and a second addition. The KWT-3 configuration configuration is listed in Table 1. The class embedding vector is extracted from the output of the last Transformer block to perform classification. This structure is repeated 𝐿 times, denoting layers, to create an architecture of stacked Transformer layers. </p>
<p class="text"> tinyML Research Symposium’22, March 2022, San Jose, CA. This is a general form, with the delta representation, only non-zero Δ𝑋 are stored and used for multiplications as visualized in Baseline delta algorithm. These delta multiplications are used in a weight-weighted matrix multiplication matrix. The output 𝑅(𝑡) of the tensor operation can hence be computed by accumulating the result of the previ-referous reference token with the multiplication results of the delta values only. The updated referredto the new baseline for the upcoming token: �<br/> The multiplication of 𝐴 row with the first 𝐵column is done as usually without using deltas. The multiplication 𝑟00 is replaced with 𝎎00𝑏00 + 𝁎01 𝓓regere, in horizontal direction, where the expression can be replaced with theexpression in horizontal direction. The<br/> Delta algorithm for 𝑸𝑲𝑻 represented with matrices apologeticand pronouncedin the Delta algorithm. The algorithm is based on the formula proveativeargorithm for the rows resembledwith matricesprovocativeandreformargorithmsat the top of the rows. The results are calculated in the order of the matrices and the order for each row.<br/> An approach for multiplications for all the other positions is demon-strated on the second row and second columns of 𝐵. The terms with 𝑟10 are replaced with replacedwith and the terms withΙ�sdenotedby 𝙟01, while those with'D' are replacedwith'G' and 'D'. The softmax algorithm cannot be directly applied for softmax.<br/> As done earlier, they will again start by performing an altered processing of the initial row 𝑟0 = [ �ureure00 “𝑥” (�ury00). The next row of the scaled input is already expressed with a regular softmax function:𝑠 -𝑓 vanishedwith the function:-vanished-vanishingfunction:vanishing-function:vanished_functionVanished-function. </p>
<p class="text"> The KWT model offers optimization in the last layer of the Delta KWT. The MHSA and the MLP block consists of two fully connected layers with weight matrices of dimensions (192,768) and (768,192) respectively. Without any delta modification, ∼39% of the multiplication of the original KWT can be found in the MHSA. The resulting savings are thus worth 59.64%, making the total savings at least 4.97% for the KWT without accuracy.<br/> For simplicity, all the terms use matrices 𝐴 and 𝐵, and 𝑟𝑜𝑤 and for dimensions. The last layer are expressed as:. (𝑐�’11 = 1) and (1) (2) (3) (2)) (1), (2), (3), (4) (4), (5) (6), (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (<br/> The GSCD v2 is used to evaluate the method as well as the original KWT performance. The proposed delta approach neither requires expensive hardware nor comes with a large memory overhead. The downside of the method is compute and data irregularity due to the algorithm’s unstructured pruning. Section 7 will therefore analyze the complete accuracy-complexity trade-off for real data sequences. The dataset contains 105,000 1-second audio snippets of 35 different words sampled at 16 kHz.<br/> The model-like MHSA classifies 4,800 keywords from a test set into one of the 12 cate-gories: ”up’, ”down”, “left” and ”right” They focused on those configurations that yielded at least 94% accuracy. The thresholds might be different for each delta-encoding within the MHSA block, they are the same across every layer. MHSA in the first layer uses the same thresholds as MHSAs in other layers. From these sweeps, the thresholds leading to a Pareto-optimal accuracy-computations are used in a full run. </p>
<p class="text"> X-axis represents MACs, while the left and right y-axis correspond to the accuracy and speedup of the KWT model. Percentage of executed MACs averaged across the layers for one instance of each keyword category. The KWT-3 model that achieves 98.4% accuracy with 100% MACs is slightly outperformformform the original KWT 3-3-2-3 version of the model. The results of running the original and the delta version of KWT are published at the San Jose. tinyML Research Symposium’22, March 2022, San Jose, CA.<br/> Table 2 shows the % of executed MHSA operations for one instance of each keyword category, averaged across the lay-genreers. Figure 8 shows the delta values of the input data and the softmax output of the 7th layer of a keyword right. Figure 9 shows how much operations are approximately performed in each of the parts of the MHSA. The proposed technique can potentially be exploited to enable an ultra-low power wake-up word detection that triggers a more powerful detector once a keyword is recognized. </p>
<p class="text"> Researchers from Interspeech and IEEE International Conference on Acoustics, Speech and Signal.Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021. Researchers from the U.S. National Institute of Linguistics: Human Language Technologies, NAACL-HLT 2019, Volume 1 (Long and Short Papers), Volume 1, 2019,. Researchers at San Jose University's Keyword Transformer Research Symposium’22, March 2022, San Jose, CA.<br/> An Image is.ishlyWorth 16x16 Words: Transformers for Image Recognition at Scale. 2021. In 9th International Conference on Learning Representations, ICLR 2021, Austria, May 3-7, 2021. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In Proceedings of the.ophobic5th Workshop on Representation Learning for NLP. Association for Computational.Linguistics, Online, 143–155. https://doi.org/10.18653/v1/2020.<br/> In 4th International Conference on Learning Representations, ICLR 2016, Yann LeCun (Eds.) and Yoshua Bengio Bengio (Ed) in New York, New Jersey, will present their findings at the International Conference of the Association for Computational Linguistics (ACI/IJCNLP) in the fall of 2021. Weigh-adaptive Transformer: Train once with length drop, Use Anytime with Search. In Proceedings of the 59th.Annual Meeting of the. Association for. Computational. Linguistic Linguists and the 11th. International Joint Conference on Natural Language Processing, ACL/IJ CNLP 2021, will discuss<br/> Pruning a BERT-based Question Answering Model. 2019. Are Sixteen Heads Really Really Better than One? Are 16 heads really better than one? Are sixteen heads really good enough to answer that question? Is a distilled version of BERT: smaller, faster, cheaper and lighter? How do you do it? They look at the results of the 2019 NeurIPS 2019 conference in Vancouver, BC, Canada, on December 11-17-14, 2019. How do they do this?<br/> In Proceedings of the 58th Annual Meeting of the Association for Com-putational Linguistics, ACL 2020, Online, July 5-10, 2020, online. The Association for Computational-Linguistics is 2158–2170. The study will be published in the journal of the American Economic Review Institute for Economic Policy and Organization for Economic Research (ARIS) and the International Conference of Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of the ICML) </p>
<p class="text"> Researchers from the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, in Florence. The next year's conference on computer vision will be held in San Jose, CA, California, February 27 March 3, 2021, in the U.S. and February 27, in Seoul, Korea, South Korea. The next conference will be hosted by the International Conference on Computer Vision (ICCV) in Tokyo, Japan, June 9-14, 2019. </p>
