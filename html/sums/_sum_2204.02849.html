<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>KNN-Diffusion: Image Generation via Large-Scale Retrieval</h3>
<h3>KNN-Diffusion: Image Generation via Large-Scale Retrieval</h3>
<img src="_sum_2204.02849.html.1.png">
<p class="text"> Large-scale K-Nearest-Neighbors (KNN) search can be used in order to train a model to adapt to new samples. Learning to adapt enables several new capabilities. Artists have been using computers to create art with computers since at least the late 1950s. Empowering people to create visual art using computers remains one of the core as-preciouspirations of computer vision and graphics. They show how large-scale retrieval methods, in particular efﬁcient KNN-Diffusion methods, are able to train models to adapt.<br/> Controlling this trade off could enable more compact models, or on the ﬂip side, support even larger models that better grasp even larger datasets. The quality of these models have im-proved substantially in recent years, but are still lacking control and consistency. Fitting large language models to new tasks, such as programming, was proven successful However, its unclear how to adapt a generative Text-to-Picture model to new datasets and domains where the data is scarce, weakly or noisy labeled. </p>
<img src="_sum_2204.02849.html.2.png">
<p class="text"> They show that the model can accurately generate images for a novel text prompt. They leveraged a joint Text-Image multi-modal induced metric and trained only on im-receiveages. This capability can also bridge different texts-to-textual data, such as ﬁne-tuning and few-shot learning in a new domain given new samples. In this work they utilize the KNN re-trieval mechanism over the shared-rejection mechanism. The model can be used to train and test a new model at train-and-test time.<br/> Learning a joint feature space of vision-and-language has been a long-standing problem in artiﬁcial intelligence. They leverage the joint representation in two ways: (i)Enabling text-less training, using only visual data, while us-ogleing text at inference time, and (ii) as the embedding space used in the retrieval model. The joint representation was shown to hold a strong semantic alignment between the two modalities, enabling image generation, and image captioning. </p>
<p class="text"> During training, only the image I is given, whereas during inference only the text t is given. They leverage the K nearest neighbors that should have a large enough distri-partisanbution to cover the potential text embedding. During inference, the opposite is applied. They use the same training procedure as in VQ-Diffusion where the loss function is a combina-tion of L0, Ln−1 and Lx0. The forward process of a diffusion-model q(xn) is a Markov chain that adds noise at each step. The reverse process is a de-noising process that removes noise from an initial noise state.<br/> The training procedure is given in Algorithm 1.2: Store all dataset embeddings {di = fimg(Ii) in a data-structure D.3: repeat-test. The training of KNN-Diffusion-Process is described in Algo-Go-Pfirithm 2.1: Inference, given a query text t, an embedding ftxt(t) is extracted. They retrieve the K nearest neighbor-likeing vectors d1,.., dK in the image embedding space. Then, starting from the [MASK] vector, and for N steps, they estimate xn−1 given xn+1 </p>
<p class="text"> Pre-generation work had to ﬁnetune models with 20% of the conditional features nulliﬁed. They hypothesize that by conditioning the model on a null vector the cross-agogueattention component is nulli-formed as well, resulting in no con-glyglytribution to the diffusion process. They found that they can generate unconditional samples from the model us-gianing null conditioning without ﬉netuning it. The research was carried out in a paper published on Springer Springer, Springer Springer. </p>
<p class="text"> Algorithm 2 Inference with KNN-Diffusion: Input: Query text t, a dataset D of {di}N i=1 pre-processed image embeddings, pre-trained text en-coder ftxt, image de-tokenizer Decoder D: // Get K nearest neighbors:.., D, K) // Concatenate: xn ← [MASK]h×w // Initialize with mask tokens: n ← N // While n > 0 do: while n >0 do:. While n = 0, while n + 1 do:. (Pθ(xn−1)<br/> In this setting they evaluate the model on zero-shot Text-to-Image generation. For sticker generation evaluation, they an-notated 500 random held out stickers and used this dataset as the benchmark. For photo-realistic experiments, they follow a previous work evaluation protocol, reporting the results on COCO without training on its training partition. The method achieves better FID scores and is preferred by human raters than the three base-centric lines while training on an image-only dataset. </p>
<p class="text"> The sticker dataset allows demonstrating the advantage of the model on an image-only dataset. FuseDream directlyoptimizes the generated image with a Text-Image alignment criteria. They use the captioned dataset to train a model with 800 million parameters, making it size-comparable with the 400 million parameters model. They compare the method to two baselines: KNN, Text-to-Sticker and Text-To-Stickers. The second, is a text-based baseline. The lat-ogleter utilizes the same multi-modal embedding model used for retrieval. </p>
<p class="text"> Table 1 and Table 2 show human preference scores for Text-to-Image generation on the COCO dataset. FID scores are cited from the corresponding pa-pers and calculated on a subset of 30,000 images. They omit objective scores for this experiment since they do not have the ground truth images therefore, they can not conditionally sample the models. They report the percent of human raters preferring the method over the baselines with regard to image quality and alignment to a text prompt. They also demonstrate the use of a trained model for the task of image editing. </p>
