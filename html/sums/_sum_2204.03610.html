<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Unified Contrastive Learning in Image-Text-Label Space</h3>
<h3>Unified Contrastive Learning in Image-Text-Label Space</h3>
<img src="_sum_2204.03610.html.1.png">
<p class="text"> In this work, they intro-duce a new formulation by combining the two data sources into a common image-text-label space. In this space, they propose a new learning paradigm called Uniﬁed Con-Trastive Learning (UniCL) with a single learning objective. The UniCL is an effective way of learning semantically rich yet discriminative repre-centricsentations. It attains gains up to 9.2% and 14.5% on zero-shot recognition benchmarks over the two methods.<br/> Language-image contrastive learning has emerged as a promising approach by leveraging huge amounts of webly-crawled image-text pairs. These pairs are noisy, free-form but cover lots of visual concepts. The largest scale but private JFT-300M covers 18,291 concepts, and the largest scale-but-but private JFS-300m cover 18, 291 concepts. Researchers say language-language contrastive language learning is a promising way to learn more about visual concepts than previously thought. </p>
<img src="_sum_2204.03610.html.2.png">
<p class="text"> A natural question is: can they have one model for both discriminative representations and broad visual concept coverage? In this work, they take the ﬁrst step to answer this question. They introduce a new perspective of image-text-label space, which can seamlessly unify image-label and text data. They propose a uniﬁed contrastive learning method called ‘UniCL’ to seamlessly accommodate both data-types for visual-semantic representation learning. With UniCL, they combine images and texts together to learn discrimina-centric representations.<br/> The work presents the ﬁrst uniﬁed contrastive-language learning method that can seamlessly leverage both. It calls back the textual concepts behind the labels and use them as a special format of language. The work is close to these works in that they also use the image-text data as one of the data sources. Contrastive learning has laid the foundation for the best performing SSL models </p>
<p class="text"> Latexit sha1_base64="S2cTaS+VEITU0ySh8UfWKZQ7yeU=">AB/nicbVDLSsNAFJ3UV62vqLhyM1gEVyWRoi6LblxWMG2hiWUynbRDJzNhZiKU=";"";";";;";";""; ""; ";";" ";" "";"""; "", ";"";" "" " """ "" is "S2CTaS" and "V"<br/> ="q/kI/mfvjblD6uYW510qD7aUaY=">ACFnicbVDLSsNAFJ34rPUVdelmsAhuLIkUdVnqxmUV+4Amlsl0g6d. =""q"""Q/k" is the word for the "opinionistic", "inquirative" and "opiristic".<br/> It sha1_base64="DA7uNIJXcln2WgNY1ZJ9Yb0G7oc=">ACG3icbVC7TsMwFHXKq5RXgJHFokJiqpKqAsaqliwF0F0YfUhMpxnda. it sha 1_base 64="aXwGO2+RykKZ0fHeWHJ4LnHCU2A=">acD.glybVDLSsNAFJ34rPUVdelmsAquSiJFXRbd CG6q2Ac0scxMJ+<br/> Texit sha1_base64="S2cTaS+ VEITU0ySh8UfWKZQ7yeU=">AB/nicbVDLSsNAFJ3UV62vqLhyM1gEVyW ���™™™:‘I’m’. ‘‘-’ ‘I’s‘.’’.<br/> 510qD7aUaY=">ACFnicbVDLSsNAFJ34rPUVdelms"=">AhuLIkUdVnqxmUV+4Amlsl0g6dTMLMRChpvsKNv+LGhSJuxZ1/4yT reproveativeproveableprivacyvxZLwY78bHrHrhXJKGYOwB8Ynz9AfqAb</latexit><br/> "ogq4H+6fFaHG+7VjmnK6AHkntc=">ACFni. "OGq4h+6F FaHG" "OGQ4H" is "acFni" and "OGX4h" is a "latexit sha1_base64" with a base64. The base64 sha has been set up in the middle of the base64 of the sha. The sha is set to be set up by the end of the day.<br/> FaHG+7VjmnK6AHkntc=">ACFnicbVBNS8NAEN34WetX1KOXxSJ4sSRS1GOpF49VTFtoYtlsN+3SzSbsboS5l. FaHg+7vjmnk6AHc="="""" """ is a reference to a person who has a history of violence and violence. "" is an example of violence"<br/> Figure 3. 1XokREFs=">ACEnicbVDLSgMxFM3UV62vUZdugkVQkDIjRV0W3bis4NTCdCyZNOGJpMhyQhlmG9w46+4caGIW1fu/BvTdkBtPXDh5Jx7yb0nTBhV2nG+rNLC4tLySnm1sra+sblb+0lEglJh4WTMh2iBRhNCaepqRdiIJ4iEjt+Hwcuz<br/> They argue that LBiC is more general than LCE, from two aspects: Augmentation with Li2t. The additional text-to-image term Li2T plays the role of regularizer. Given a language description tj, all image features with the same tj in the batch are clustered toward the text feature; otherwise they are pushed away. This can help prevent over-ﬁtting, as demonstrated in the experimental experiment. The training process of UniCL and SupCon is summarized in Al-R-R.<br/> The TargetM function ensures that each unique language description in the batch has a unique label index. After training, the learned visual and textual encoder {fθ, fφ} can be used jointly for open-vocabulary recognition, i.e., recognizing the categories seen dur-gling training or novel ones beyond the annotated categories. The visual backbone fθ is used indepenen-ishlydently for feature extraction in linear probe or for full-fledged </p>
<p class="text"> They study the models based on publicly available publicly available datasets. The ratio of #images/#concepts clearly demonstrates the different trade-off between image diversity and semantic-richness over different datasets. They use the same prompt strategy and tokenizer for the image-text-label data as proposed in CLIP. They then calculate the number of unique words and report it as the vocabulary size. For example, they use Spacy to extract the noun phrases that appear more than 5 times. In the uniﬁed uni.-referred-to-training data, all these datasets can be jointly used for learning.<br/> They evaluate the quality of learned representa-heticaltions on a set of computer vision tasks. They evaluate on ImageNet-1K as well as 14 datasets used in CLIP and employ the same text-encoder architecture. The whole model including vision and text encoder are trained from scratch. They use Mask R-CNN as the detec-transformtor and follow the standard 1× schedule. All models are trained for 500 epochs with a batch size of 4096. </p>
<p class="text"> Using CE, SupCon and the Uniﬁed Contrastive Learning, they used ResNet-50 and Swin-Tiny as visual encoders. The network is language-aware, which means they can directly use it for zero-shot recognition. UniCL outperforms CE by 3 points when training on larger datasets (from CIFAR to ImageNet) or with strong augmentation (MixUp and CutMix)<br/> They use ImageNet-1K as the base dataset, and gradually add different sets of image-text pairs. They vary the default batch size from 4096 to 2048 and 1024. UniCL is robust to the variation of batch size, regardless of which language encoder is employed. This is different from contrastive methods such as SimCLR in self-supervised learning. They use a balanced data sampler to ensure that the model is trained with the same number of images per epoch. </p>
<p class="text"> Using GCC-15M, they observed much more improvements as well for ImageNet-1K (+1.9),Linear Probe (+3.5) and COCO detection. Adding image-text pairs can also improve the performance across all metrics. They plot 1000 classes for both imageNet-20K (30.2 v.s. 36.3 v.S. of images) in Fig. 4. They also show the exemplar image corresponding to each concept.<br/> They study how image-label data can assist the learning with image-text pairs. They use random crop as the data augmentation and train all models for 32 epochs. They compare against two baselines: (i) CLIP, a language-centric image contrastive learning method without label supervision, and (ii) Multi-task learning that performs with multi-task data, and CLIP on image-based data. The UniCL achieves compa-privileged ImageNet-1K zero-shot performance to YFCC-14M with Swin-Tiny. </p>
<p class="text"> UniCL outperforms both baselines signiﬁcantly on the ﬁrst 3 datasets, and shows higher averaged scores on others. The gain between UniCL on mixed data and CLIP on YFCC-14M data is shown. UniCL combines the advantages of learning rich concept coverage from. image-text pairs and discriminative representations from image-label data. Three categories “teddy bear”, “siberian husky” and “tibetan terrier” are highlighted.<br/> They have presented UniCL, a new contrastive learning paradigm for generic multi-modal representation learning. They show that UniCL trained with image-label data yields a more discriminative feature space. They also discuss its connections to existing learning methods, and empirically demonstrated that the learning method stand-alone is a good alternative learner. They refer the readers to Florence for large-scale pretraining and evaluation on a boarder set of tasks including VQA and video understanding. </p>
