<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Adversarial Learning to Reason in an Arbitrary Logic</h3>
<h3>Adversarial Learning to Reason in an Arbitrary Logic</h3>
<img src="_sum_2204.02737.html.1.png">
<p class="text"> An algorithm can work in an arbitrarily speciﬁed logic without any knowledge or set of problems. It is able to learn to work with any logical foundation, even when there is no body of proofs or even conjectures available. The approach is stronger than training on randomly generated data but weaker than the approaches trained on tailored axiom and concrete axiom sets. They practically demonstrate the.feasibility of the approach in multiple logical systems. They show that the proposed universal learning for logic also works on some encoded games, such as Sokoban.<br/> The AlphaZero algorithm has also been applied in theorem proving to the synthesis of formulas (Brown and Gauthier 2019) and functions (Gauthier 2020) There are many approaches to applying MCTS with policy and guidance (Kaliszyk et al. 2018; Rawson and Reger 2019) The results are better than those they are able to get here, but no new log-ishlyics or problems are tried and generalization and transfer have been very limited so far. They show that learning for logic can also be also applied to these games. </p>
<img src="abstract.png">
<p class="text"> The core of the AlphaZero algorithm (Silver et al. 2017) is purposefully learning from self-play. It trains a neural network to evaluate the states of a game to estimate the ﬁnal outcome of the game, as well as a policy maximizing the expected outcome. To train policy estimation, a Monte Carlo Tree Search (MCTS) is used to compute a bet-ridden policy, then the network is trained to return this better policy. The new policy is deﬁned to be proportional to the number of nodes explored below each of the immediate children of the tree.<br/> The AlphaZero (Silver et al. 2017) algorithm uses a neu-generation network to estimate state values. It estimates state values in range of state values (−1, 1), they will call it vθ) and policies (a vector with as many dimensions as the size of the action space) Then a purposefully constructed the-reformable theo-theoreticorem is passed to the prover, after replacing all remaining variables with fresh constants. The second player tries to prove the theorem, winning when the list is empty. If the proof is successfully completed, this vari-formable they started with will be uniﬁed with a provable the </p>
<p class="text"> Theoretically, the system works with a logic system deﬁned by a set of rules. They train the system to solve Sokoban-type problems using a pseudo-“logic system’s rules of the game as pseudo-inference rules. In the experiments with using a saturation prover, the system got stuck after the ﬁrst step, with the prover winning all games per episode. The system can (at least theoretically) be used to learn to reason in any de-prolog (and decidable) context.<br/> If the prover was winning every game, however, they would need to rely on exploration for the adversary to ﬁnd some-hard to prove. This situation is virtually im-possible, because of the exploration noise used during play-out. This should lead to adversary towards theorems where prover is uncertain and sometimes loses due to explo-ration noise. Another failure state which if reached, would be entirely stable. It is possible because construc-mittedlytion of theorem is inherently easier than proving. </p>
<p class="text"> A single Graph Neural Network is used to evaluate the states for both players, the prover and the adversary. Game states are represented as syntactic graphs. One graph contains all terms that need to be proven, together with information about which player the state belongs to, and (for the adversary player) the state of the constructed theorem. The graph is shown in ﬁgure 6: An example graph representing the game state. After initial 4 moves, they generate more playouts – 106 (they use all data generated this way to train a network to. estimate policy and value)<br/> They run three experiments with classical logic, trying out three different proof systems. For the test set, they use a small subset of the Mizar40dataset (Kaliszyk and Urban 2015) of formulas that do not do not equal equality. They also run experiments with sequential calculus, Tableaux connection prover and the Tableaux connected prover. They use a part of the ILTP library (Raths, Ot-ishlyten, and Kreitz 2005) for the </p>
<p class="text"> They train the prover to work with modal propositional logic (Blackburn, van Benthem, and Wolter 2007), in fthe variants: K, T, S4, S5. They also train in linear logic (Girard 1987), only in the propositional setting. For the encoded Sokoban games the results are particularly good, with many games solved only with the adversarial-logical setting. Among the tried calculi, only for the clas-.-insuredsical sequent calculus and Hilbert-calculus there is no ad-generation advantage this is likely due to the fact that the calculus is much closer to the syntax.<br/> An algorithm for learning to reason in an arbi-trary logic has been presented. The system learns to construct increasingly harder problems and learns to prove them. It is the only proposed theorem-proving system that may continuously learn and improve without any dataset. The performance is weaker than that of domain-speciﬁc Automated-Theorem Provers and provers trained on tailored datasets. It remains an open question if trying a compute power comparable with AlphaZero (Silver-Glanet al. 2017) would produce better results. </p>
