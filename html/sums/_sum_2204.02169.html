<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Hybrid Predictive Coding: Inferring, Fast and Slow</h3>
<img src="_sum_2204.02169.html.1.png">
<p class="text"> Predictive coding is an inﬂuential model of cortical neural activity. It proposes that perceptual beliefsare furnished by sequentially minimising ‘prediction errors” the differences between predicted and observed data. Successful perception requires multiple cycles of neural activity, says Alexander Tschantz.<br/> This is at odds with evidence that several aspects of visual perception arise from an initial ”feedforward sweep” that occurs on very fast timescales which preclude substantial recurrent activity. They propose a hybrid predictive coding network that combines both iterative and amortized inference in a principled manner by describing both in terms of a dual optimization of a single objective function.<br/> Hybrid predictive coding model combines the beneﬁts of both amortized and iterative inference to obtain accurate beliefs using minimum computational expense. The model offers a new perspective on the functional relevance of the feedforward and re-spective activity observed during visual perception. It offers novel insights into distinct aspects of visual phenomenology. </p>
<img src="_sum_2204.02169.html.2.png">
<p class="text"> Hybrid predictive coding combines the beneﬁts of both iterative and amortized inference. The network is trained with biologically plausible prediction error minimization using only local Hebbian updates. It can be trained faster and using less data than standard predictive coding networks. It could account for distinct aspects of visual phenomenology.<br/> In contrast with this classical bottom-up view, Bayesian theories cast perception as a process of (approximate)Bayesian inference. Prior expectations are combined with incoming sensory data to form perceptual repres entations. Under this Bayesian perspective, the loci of perceptual content reside predominantly in top-down predictions. Top-down signalling has long been recognised as playing several important functional roles.<br/> At the same time, bottom-up sig-nalling has been convincingly linked to rapid perceptual phenomena such as gist perception and context-independent object recognition. These and other disparate ﬁndings have fueled a long-standing debate over the respective contributions of bottom up and top-down signals to visual perceptual con-orativetent. </p>
<p class="text"> Hybrid predictive coding (HPC) combines iterative predictive coding with (standard) predictive coding and (probabilistic) predictions. The architecture comprises several hierarchical layers, with top-down signals predicting the activity of the subordinate layer and bottom-up signals predicting superordinate layer’s activity. Top-down predictions learn to generate (the parameters of) beliefs at higher layers.<br/> The model casts bottom-up processing as amortised inference and top-down processing as iterative-inference. The model offers a uniﬁed inference algorithm that inherits the rapid processing of amortious inference while maintaining the robustness, robustness and context sens-itivity of iterative inference.<br/> The hybrid predictive coding (HPC) architecture is described in Section 3 of this article. They demonstrate that HPC performs supervised and unsupervised learning simultaneously. They also demonstrate that bottom-up, amortised predictions reduce the number of iterations required to achieve accurate perceptual beliefs.<br/> They argue that the model provides a powerful computational framework for interpreting the contributions of bottom-up and top-down signals in terms of different aspects of visual perception. Conversely, they demonstrate that bottom-down approaches to perception should beneﬁt from incorporating top down generative feedback.<br/> Bengio, Scellier, Bilaniuk, Sac-Sac-ramento and Senn (2016) also consider a feedforward amortized sweep to initialize an iterative inference algorithm. The authors also consider feedforwardamortizedsweets toinitiallybeginnerizean iterativeinferencealgorithm. </p>
<p class="text"> One classical view of perception is as a primarily bottom-up pro-productivecess, where sensory data x is transformed into perceptual representations z through a cascade of feedforward feature-detectors. In contrast, predictive coding suggests that the brain solves perception by modelling how perceptual rep-re-resentations z generate sensory data, which is a fundamentally top-down process.<br/> Contrastive Hebbian methods differ from predictive coding in that they require a ‘free phase’ where the network output is unclamped and clamped and then the weight update is proportional to the difference between the two phases. Bengio et al(2016) show that the networks can be amortized and predicted in a feedforward pass and that this reduces the number of inference iterations required.<br/> In addition to predictive coding, predictive coding has been proposed as a way to explain the phenomenology of perceptual experience in terms of neural mechanisms (Hohwy & Seth, 2020), the novel architecture also offers insights into why gist perception and focal perception have characteristic phenomenological properties that they do. </p>
<p class="text"> To support adaptive behaviour, the brain must overcome the ambiguous relationship between sensory data and their underlying (hidden) causes in the world. Bayesian inference describes the process of forming probabilistic beliefs about the causes of data, to accomplish perception. They can denote sensory data (e.g., the pattern of retinal stimulation) as x and the hidden cause of this data as z.<br/> Bayesian inference prescribes a normative and mathematically optimal method for updating beliefs when faced with uncertainty. It provides a principled approach for integrating prior knowledge and data into inferences about the world (Cox, 1946; Jaynes, 2003; Knill & Richards, 1996) Bayesian Inference provides an elegant framework for describing perception, but the com-iopatations it entails are generally mathematically intractable.<br/> It has been suggested that the brain may implement approximations to Bayesian Bayesian inference. Variational inference posits the existence of an approximate posterior qλ(z) with parameters λ. The goal of variational inference is to minimise the difference between the true and approximate posteri-centricors.<br/> Variant inference minimises an upper bound on Eq2, i.ea quantity which is always greater than or equal to the quantity of interest. 2, it is still necessary to evaluate the true posterior distribution p(z|x) Variational inference circumvents this issue by minimising the upper bound. </p>
<p class="text"> Minimising variational free energy F will ensure that the qλ(z) tends towards an approximation of the true posterior, thus implementing an approximate form of Bayesian inference. This minimisation takes place with respect to the parameters of the approximate posterior, and can be achieved through methods such as gradient descent.<br/> Variational Inference provides a general scheme for approximating Bayesian inference. It is necessary to specify the approximate posterior and generative model, as well as the optimisation scheme for minimimising variational free energy. By doing so, the model encode information about the environment.<br/> A standard method is to optimise the parameters of the variational distribution for each data point. They refer to this mode of optimisation as iterative inference, as it requires multiple iterations to converge. This method underwrites a number of popular inference methods, such as stochastic variational inference and black box variational inference.<br/> The predictive coding algorithm (Friston, 2005) operates on a hierarchy of layers, where each layer tries to predict the activity of the layer below it (with the lowest layer predicting the sensory data) The approximate posterior is deﬁned to be a Gaussian distribution. </p>
<p class="text"> They assume the factors of the generative model pθ(z, x) = p(x|z)p(z) to also be Gaussian. They can rewrite variational free energy F as (Buckley et al(2017)): "P(µ)F(¹) is now written in terms of the sufﬁcient statistics of qλ(z), i.ethe objective is now F(¿), rather than<br/> The goal is now to ﬁnd the value of µ which minimises F(µ, x) This can be achieved through gradient descent (with some step size) Equation 9 can be implemented using simple Hebbian plasticity (Bastos et al., 2012; Bogacz, 2017)<br/> Predictive coding is usually implemented in networks with L hierarchical layers. Each layer tries to predict the activity of the layer below it (besides the lowest layer, which predicts the data) To make predictions match input data, the dynamics described by Equation 8 and Equation 9 prescribe that prediction errors are minimised over time.<br/> In hierarchical models, ¯µ would not be ﬁxed but would instead act as an empirical prior. The same logic applies for εp, which will be discussed further in the context of hierarchical models. The free energy is equal to the sum of (precision-weighted) </p>
<p class="text"> Predictive coding models the world in a top-down manner e.git learns to predict features from objects, rather than predicting objects from features. Inference is achieved by ‘inverting’ this model, i.egoing from data to hidden states (features)<br/> Amortised Inference provides an alternative approach to performing variational inference. It learns a function fφ(x) which maps from the data to the variational parameters. The parameters of this function are then optimised over the whole dataset, rather than on a per-example basis.<br/> Amortised inference is fundamentally a bottom-up process: it predicts objects from the bottom up. The goal is then to optimise the parameters of this amortised function to minimize the free-energy on average over the entire dataset. Amortisation gap (Cremer et al. 2018) is caused by the decrease in performance incurred from sharing parameters across the dataset.<br/> Several methods have been proposed for implementing amortised inference. In the current context, fφ(x) acts as a discriminative model which learns the parameters of the variational model. It is straightforward to extend the scheme into a supervised setting. This can be achieved by turning the predictive coding network on its head, so that the model tries to generate hidden states from data. </p>
<p class="text"> The hybrid predictive coding (HPC) model combines both amortised and iterative inference into a single biolo-glygically plausible network architecture. The model is composed of L hierarchical layers, where each layer tries to predict the activity of the layer below it. The error units measure the disagreement between these predictions and the actual input.<br/> The amortised parameters deﬁne non-linear functions which map activity at one layer to activity at the other layer above, thereby implementing a bottom-up prediction: µi = fφi(µi−1)Here, the lowest layer operates directly on sensory data: µ0 = f φ0(x)<br/> The second ‘iterative’ phase is generated by generating top-down predictions and iteratively applying Equation 8. The process is symmetric to the original predictive coding model, except that predictions now also also learn in the opposite direction. The amortised predictions learn to predict beliefs at higher layers, after the beliefs have been optimised by iterative inference.<br/> HPC can simultan-ously perform classiﬁcation and generation tasks on the MNIST dataset. In effect, the amortised predictions learn to ‘shortcut’ the costly process of ‘iterative inference, allowing for fast mapping from data to beliefs. Figure 2 provides a schematic of the model and Algorithm 1 presents the details of the inference and learning procedure. </p>
<p class="text"> Hybrid predictive coding combines two phases of inference as follows. At stimulus onset, data x is progressivelypropagated up the hierarchy in a feedforward manner, utilising the amortised functions fφ(·) These predictions set the initial conditions for µ, which parameterise posterior beliefs about the sensory data. The initial values for µ arethen used to predict the activity at the layer below, transformed by the generative functions fθ( ·)<br/> They show that novelty of the data adaptively modulates the number of iterations enabling more rapid adaptation to nonstationary environments and distribution shift. They then demonstrate the practical beneﬁt of fast-prone inference by plotting the accuracy of hybrid and standard predictive coding against the number. They also investigate additional properties of the model.<br/> The MNIST database consists of 60,000 training examples and 10,000 test examples. Each example is composed of an image and a corresponding label between 0 and 9, where each image is black and white and of size 28 x 28 pixels. The network can adaptively reduce computation time for well-learned stimuli but also increase it again for novel data. </p>
<p class="text"> The model’s highest layer is composed of 10 nodes (one for each label) The input layer is fed into the predictive coding network via an input layer consisting of 784 nodes. In the context of both Hybrid predictive coding and standard predictive coding, labels are encoded as priors at the highest level of the hierarchy.<br/> The hybrid and standard predictive coding models are composed of 4 layers of nodes. During training, these nodes are ﬁxed to the corresponding label: the node which corresponds to the label is set to one, while the remaining nodes are set to zero. During testing, the highest layer of the network is left unconstrained.<br/> The lowest layer, which is used during training and testing, comprises 784 nodes and corresponds to the current image. The next two layers are composed of 500 nodes each, and the highest layer is formed of 10 nodes, which correspond to 10 nodes. The current image is composed of around 700 nodes each. </p>
<p class="text"> For both the hybrid and standard predictive coding models, the generative, top-down functions fθ(·) use tanh activation functions for all layers besides the lowest, which do not use an activation function. For the amortised, bottom-up functions fφ(·), a tanhactivation function is used for all layer besides the highest.<br/> They use an adaptive threshold which cuts off inference if the average sum (across layers) of mean squared prediction errors is less than 0.005. They train and test accuracy after every 100 batches, where the batch size is set to 64 for all experiments. They do not measure accuracy over entire epochs but instead measure accuracy as a function of batches.<br/> Unsupervised capabilities of HPC derive from learning the top-down generative parameters in the absence of labels. These parameters distil statistical regularities in the data, forming a generative model which can be used for downstream tasks. The ability to utilise both supervised and unsupervised signals is not unique to hybrid predictive coding.<br/> They compare the results of hybrid and standard predictive coding on the MNIST dataset. They also compare the accuracy of the amortised component alone. They show that hybrid coding affords several additional beneﬁts which are not provided by standard coding. The results are shown in Figure 3.<br/> There is no signiﬁcant difference between the accuracy of the hybrid and standard predictive coding (Figure 3A) This is to be expected, as the iterative inference procedure ‘trains’ the amortised component (as amortized connections learn) </p>
<p class="text"> Figure 3A shows that the amortised component’s accuracy cannot be higher than that provided by iterative inference alone. The hybrid and standard predictive coding models are equivalent in terms of their ability to generate data, and Figure 3C & 3D show that samples generated from each of these models are qualitatively similar.<br/> Hybrid predictive coding retains standard predictive coding’s classiﬁc-like accuracy and generative capabilities. It is worth noting that the asymptotic performance is somewhat lower than usually reported on the MNIST dataset. This discrepancy is explained by the fact that they are using models that are fundamentally generative, i.etheirobjective is to generate the data, not perform classi-cation.<br/> Prediction error can be thought of as a proxy for perceptual certainty because it is equivalent to variational free energy in the current context. Figure 4B shows that the number of iterations required to reach perceptual certainty decreases over batches. Once converged, asymptotic accuracy is achieved without requiring any iterations at all.<br/> Figure 4A demonstrates that this reduction in iterations has no detrimental effect on the accuracy of hybrid predictive coding. Hybrid predictive coding can obtain equivalent performance with a little as 10 variational iterations, whereas standard predictive coding fails to learn at all under these conditions. At 25 iterations (Figure 5B), they see that the performance slowly decreases over batches.<br/> Hybrid predictive coding facilitates fast inference by bypassing the need for costly iterative inference when amortized inferences are sufﬁciently accurate. No such general performance increased for hybrid predictive coding suggesting that the amortize bottom up connection help to ‘stabilize’ learning in the hybrid network. </p>
<p class="text"> Simultaneous classiﬁcation and generation on the MNIST dataset for hybrid predictive coding, standard predictive coding and amortised inference. Figure shows the averaged mean-squared error between the lowest level of the hierarchy and the top-down predictions from the superordinate layer, plotted against batches.<br/> Images are generated by activating a single nodes in the highest layer (corresponding to a single digit), and performing top-down predictions in a layer-wise fashion. The imagescorrespond to the predicted nodes at the lowest layer(D) As in (C) but for standard predictive coding. </p>
<p class="text"> Asymptotic convergence demonstrates that placing an uncertainty-aware threshold on the number of iterations has no inﬂuence on (asymptotic) model performance. Amortised predictions provide increasingly accurate estimates of variables, reducing the need for costly iterative inference.<br/> The accuracy of the pure iterative predictive coding network is unstable and decreases over time when there are an insufﬁcient number of iterations. The amortized feedforward pass in the hybrid model enables the network to furnish accurate beliefs within many fewer inference steps. When the number of variational iterations is lower, the relative difference between these accuracies is far less pronounced.<br/> With slight modiﬁcations, the amortised predictions of the model can approximate the backpropagation algorithm, meaning that the bottom-up connectivity implements something akin to a multi-layer perceptron. Learning a generative model is generally more data-ef ﬁcient than learning discriminative models. </p>
<p class="text"> Hybrid predictive coding enables effective inference and maintains higher performance with fewer inferential iterations required than standard predictive coding. The accuracy of HPC and the amortised expectations is mostly unaffected by the reduced number of iterations. The classiﬁcation accuracy of standard predictive codes slowly decreases over the course of training.<br/> There are 5 random network initializations over 5 random networks. Shaded areas are the standard deviation. Plotted are mean accuracies over five random networks initializations. The average accuracy of these initializations is 1% higher than the average of 1.17%. The standard deviation is 1/2%. </p>
<p class="text"> Hybrid predictive coding retains good performance with as few as 100 training examples. Speed at which the amort-ised predictions converge is signiﬁcantly affected by dataset size. Performance of HPC is not negatively affected by the poor accuracy of the HPC model.<br/> They quantify the model’s uncertainty as to the correct label by the entropy of its distribution over the predicted-labeled. entropy begins high and monotonically decreases through an inference progressivelyiteration. This suggests that in general the iterative inference process serves to sharpen and clarify beliefs. They also investigated in more detail the computational savings the hybrid model achieves through its accurate initializationof the. iterative.<br/> HPC achieves a substantial computational saving over standard predictive coding while obtaining equalor higher performance, as shown in previous ﬁgures. The ability for amortised inference to ‘shortcut’ iterative inference is facilitated by the stationary data distribu-tion used so far.<br/> The notion that the brain performs or approximates Bayesian inference has gained signiﬁcant traction in recent years. To enact a change in data distribution for the second half of training and testing, they utilise only half of the data distribution. This dramatically increases the number of iterations required to reach perceptual certainty. </p>
<p class="text"> The accuracy of hybrid predictive coding is lower than with the full dataset, but still high given the minimal amount of data (0.17 percent) Bottom-up, amortised inference is far more sensitive to a lack of data, compared to the full data set.<br/> Shaded areas represent the standard deviation of deviation. The average deviation is 1.19 per cent higher than the national average of 1.6 per cent. The standard deviation is 2.2 per cent lower than the average deviation of 2.5 per cent for the average American. Shaded area represents the national standard deviation. </p>
<p class="text"> The initial amortized guess has relatively high entropy (uncertainty over labels) which progressively reduces during iterative inference. The number of inference iterations required progressivelydecays towards 0 on a well learned task, the number of infertations required progressively decreases towards 0.<br/> Hybrid predictive coding (HPC) combines amortised and iterative inference in a principled manner to achieve perceptual inference. However, when there is a change in data distribution, additional iterations are used to classify the new, more challenging, stimuli. HPC architecture combines both top-down and bottom-up ways of learning in the brain.<br/> Hybrid predictive coding inherentlybalances the contributions of these two components in a data-driven ‘uncertainty aware’ fashion. The top-down generative aspect of the model allows effective inference in low data regimes through relatively slow iterative inference. The bottom-up (discriminative) aspect allows fast inference in stable data regimes. </p>
<p class="text"> HPC architecture retains the beneﬁts afforded by standard predictive coding, such as the ability to learn in both a supervised and unsupervised manner. It also enables fast inference, a method for shortcutting the costly and time-consuming process of iteratively minimising prediction errors.<br/> Hybrid predictive coding can be considered an example of learning to infer through inference. The learning signal for amortised inference is provided by the outcome of iterative inference. This mechanism provides a straightforward way to generate ‘correct’ layerwise targets internally for the amortized-inference process to treat as supervised targets.<br/> This dynamic enables rapid responses to novel events while also allowing for improved context-sensitive disambiguation of uncertain stimuli due to the reﬁning effect of additional iterative inference steps. Over time, in statistically stable (stationary) environments, bottom up predictions will learn to accurately capture perceptual beliefs, eschewing the need for costly (costly)iterative inference.<br/> The degree to which bottom-up and top-down predictions contribute to the ﬁnal beliefs is adaptively modulated based on their relative uncertainty. The resulting scheme naturally facilitates an adaptive trade-off between speed and accuracy, which is not possible when utilising either method of inference in isolation.<br/> Because the model retains the neural architecture and Hebbian updates of predictive coding, it inherits the wealth of neurophysiological evidence that has accrued in favthe of this theory (Walsh et al.212121) It inherits a wealth of evidence that supports this theory. </p>
<p class="text"> The amortised and iterative inference schemes use identical connectivity and weight updates. They posit a uniﬁed learning algorithm that underwrites both forms of inference. The model provides a compelling account of feed-forward activity during perception. It accords neatly with empirical results that core object recognition and “gist” perception occur on time scales which preclude the use of recurrent dynamics.<br/> The model can straightforwardly account for these results, as it proposes that recurrent activity implements a process of iterative inference. The model can also explain the effectiveness of the initial feed-forward sweep in core object recognition (Serre, Oliva& Poggio, 2007; VanRullen, 2007)<br/> The work builds upon efforts to combine the relative beneﬁts of generative and discriminative models. It builds upon architectures that incorporate generative feedback into feed-forward neural networks (Huang et al., 2020) These previous results demonstrate that generative. feedback enables robustness to noise and adversarial attacks.<br/> In visual neuroscience, object recognition is often separated into two distinct phases: feedforward and recurrent processing. Feedforward processing provides coarse-grained representations for core object recognition and so-called ’gist’ perception, while recurrent processing persists over longer periods (Kreiman & Serre, 2020) </p>
<p class="text"> This account of perception is remarkably consistent with the proposed model. In this context, feedforward sweepcorresponds to the amortised ‘best guess’ at perceptual beliefs, which is implemented by feedforward connectivity in the model. Amortised predictions are insensitive to current context, as they map directly from data to beliefs.<br/> In the model they see a slower increase in accuracy for amortised inference, compared to the full hybrid predictive coding architecture, for small datasets. Small datasets cannot be modelled well with purely amortized inference, but can be well by combining the combination of both iterative and amortize inference components.<br/> The model casts the recurrent pro-pro-actively-cessing in the visual system as a process of iterative inference, where beliefs are iteratively reﬁned based on top-down predictions interacting with bottom-up beliefs and with sensory input. The model makes several predictions which have been corroborated by empirical evidence.<br/> In the model, recurrent dynamics are driven by prediction errors. The model is the first to combine these into a common and biologically plausible probabilistic predictive-coding ar-itecture. Doing so provides a principled arbitration between speed and accuracy in perceptual processing (i.e. when prediction errors are minimised)<br/> When amortised predictions generate accurate beliefs, there will be no prediction errors and no recurrent activity. Alternatively, when amortising predictions generate inaccurate beliefs, prediction errors will be large and iterations of recurrent activity are engaged to beliefs. This arbitration arises naturally from the probabilistic rep-resentations within the model. </p>
<p class="text"> Predictive coding has been shown to explain a diverse range of perceptual phenomena. Recent work has demonstrated that predictive coding provides a local approximation to backpropagation; the algorithm underwriting many of the recent successes in machine learning. The model inherits the wealth of empirical evidence that has been gathered in its favthe.<br/> While predictive coding has emerged as a promising candidate for understanding cortical function, its iterative nature tinges poorly with some established facts about visual perception. The model augments predictive coding with additional bottom-up connectivity, which provides amortised estimates of perceptual beliefs using a single forward pass.<br/> The feedforward nature of the amortisedconnections means that representations can be extracted rapidly without relying on recurrent activity (Serre et al., 2007) Although predictions are generally associated with top-down recurrent processing, this bottom-up forwardpass can also be interpreted as its own kind of prediction.<br/> Generative methods are more efﬁcient in low data regimes. They can be used for a wider range of downstream tasks, and enable better generalisation. But discriminative methods only learn about features relevant for discrimination, whereas generative methods learn about the data distribution itself. </p>
<p class="text"> The model combines generative and discriminative components within a single architecture. The top-down connectivity implements a generative model, while the bottom-up connectivity implements the classi-classifier. The model operates within the biologically plausible scheme of predictive coding and automatically arbitrates the relative in-uence of bottom-down predictions. It also enables generative replay (Shin, Lee, Kim & Kim, Kim and Kim, 2017)<br/> This describes the process of generating fake data (using some generative model) which is then used for downstream tasks. The generated data can be used to overcome ‘catastrophic forgetting’ (Kirkpatrick et al., 2017) and enable continual learning (van de Ven et al. 2020)<br/> Hybrid predictive coding architecture combines iterative and mathematical techniques to jointly optimize a single uniﬁed energy function using only local Hebbian updates. This architecture enables both rapid and computationally cheap inference when in stable environ-ments where a good model can be learnt, while also providing ﬂexible, context sensitive, and more accurate inference.<br/> Hybrid model also can learn rapidly from small data-sets, and is inherently able to detect its own uncertainty and adaptively respond to changing environments. It offers a new perspective on the biological relevance of the feedforward sweeps and iterative recur-rent activity observed in the visual cortex during perceptual tasks. </p>
