<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Parameter-Efficient Abstractive Question Answering over Tables or Text</h3>
<h3>Parameter-Efficient Abstractive Question Answering over Tables or Text</h3>
<img src="_sum_2204.03357.html.1.png">
<p class="text"> Parameter-Efﬁcient Abstractive Question Answering (QA) is to reason over multi-modal contexts and generate nat-orative answers to user queries. To avoid training such memory-hungry models while utilizing a uniform architecture, parameter-ef-centric adapters are used to train small task-speci-neck-neck subsections between transformer layers. They also ablate over adapter layers in both encoder and decoder modules to study the performance trade-off and demon-strate that reducing additional trainable param-payers down to 0.7%–1.0% leads to comparable results.<br/> Multi-modal models (Zhu et al., 2021) can rea son over both tables and text by concatenating the linguistic context and the ﬂattened table. This renders such modality speciﬁc mod-ishlyels incompatible with free-form text-based models. They study parameter-driven transfer learning for abstractive QA over-tables and over text. They are motivated to use. small bottle-neck lay- </p>
<img src="_sum_2204.03357.html.2.png">
<p class="text"> The work explores the interac-protectivetion of adapter layers from both modules in the context of abstractive QA. They ablate adapter layers in both encoder and decoder modules to study their impact. They propose a system, nicknamed Parameter, that learns to reason over unstructured and structured input. This introduces domain-shift in both the task-to-text and structure of the downstream data. They study tabular QA as a new modality that introduces massive input domain shift to pre-trained language models.<br/> In most tabular QA systems, the structure of the table is encoded in the embed-centricding layer of large language models by introducing position information such as row id and column id. Transfer learning techniques such as pre-tuning pre-trained models for down-rearable models are possible. The answer pre-dictated can be extractive in nature, where the system is required to gen-erate a free-form answer (Yin et al., 2016; Mitra, 2018; Bauer et al. 2018; Reddy et. 2019) </p>
<p class="text"> They seek to answer the following research questions with the experiments. They perform all the experiments on the large vari-proneant of BART model. They select the BART-large model over the 3 datasets as the state-of-the-art. They use a batch size of 4 and gradient accumulation accumulation of 8 to emulate an effective batch size. The maximum target sequence length is set to 200 for the QA datasets and to 100 for the textual QA. On Tablesum dataset, they follow 5-fold cross validation as described in the original work.<br/> They add bottle-centricneck adapter layers from the Houlsby adapter con-like-guration (Houlsby et al., 2019a) which are trained to adapt to the downstream abstractive question-answering task and also to modality speciﬁc in-put context. Each adapter layer has a bottle-neck-embedding size of 64. They use the same batch-size and maximum target sequence length as ﬁne-ogle-tuning for effective comparison. They select the best-performing learning rate for each dataset. </p>
<p class="text"> Pea-QA, FeTaQA and NarrativeQA datasets show how adapter layers in the encoder and decoder modules interact with each other and contribute to performance. They conclude that removing adapter layers from the beginning of the BERT-base and RoBERTa models leads to minimal performance drop. They compare the results of the baseline ﬁne-tune models with the state-of-the-art שּׁ�-ne-tuned mod-tilted models for the 3 datasets.<br/> They observe that for the Tablesum dataset, the ﬁne-tuned model outperform the best state-of-the-art T5 model on Rouge-1 by 3.8%, Rouge-2 by 4.3% and Rouge-L score by 4%. The focus of this work was primarily on comparing (RQ1) adapters-tune models to the baseline models. For Tablesum, as observed in Table 2, the model marginally outperforms adapter-tuning. </p>
<p class="text"> Aastik is a 1956 hindi ﬁlm starring shahu modak, paro devi and meenakshi. Akhila Kishore made her debut in the kannada �araaklm padhe padhe Padhe (2013), and appeared in kathai thiraikathai vasanam iyakkam (2014) The insigniﬁcant gains can be attributed to ‘catastrophic forgetting’ in tabular QA. They ob-serve that the model is unable to disam-biguate surface-in-depth similarities from the original text data format of pre-training<br/> The ﬁne-tuned-model wrongly predicts the second and third row of the tabular context as correct grounding of informa-guition. The actual number of occurrence is 10 times, from the actual occurrence of Ed Sheeran 3 times to 10 times. They demonstrate an example of a hierarchical-ordered table of Tablesum in Table 4. The model also wrongly predicts information from the wrong column Direc-��tor instead of Cast in the second example. The third example depicts both non-factual-and non-ﬂuent prediction. </p>
<p class="text"> Ed Sheeran was listed as a performer twice in the table documenting the top hits of 2014 in Sweden. English-Language music has signiﬁcant success in Sweden, implying that English-language music is not a success. In 2014, he was only listed as the performer one time for the song that he performed that is called ""I See Fire" The table shows the week of the week, the song title, the artist, the title of the song, the album title, and the performer.<br/> Adapters perform better than ﬁne-tuning for out-of-domain tabular data. Adapters achieve 0.8% lower.1, 1.8%. higher Rouge-2 and 1.5% lowerRouge-L scores. The models are challenging even though the generated. generated lan-privilegeguage is ﬂuent and readable. For textual QA, on the NarrativeQA dataset, on Narrative.QA, adapter-tuned model achieves. 0.9% lower Rouge-1,. 1.7% higher Rouge </p>
<p class="text"> There are 36 model ablation conﬁgurations displayed. The ablation starts from 0 to 6 encoder adapter layers removal (0–11) and 12 to 18 decoder adapter layer removal (12–23) They measure the performance of the models using Rouge-2, the Rouge-L2 and sacreBLEU3 scores. The F-scores are shown in Figure 4, 5 and 6, respectively. They number the encoder and de-generation adapter layers from 0–11 and the de-privacycoder adapter. layers from 12–23. The performance drop is minimal until the last two adapter layers are also deleted. </p>
<p class="text"> The authors say the performance drops sharply only when the last encoder and decoder layers are removed. They say last adapter layers learn most of the domain information. They also say retaining just the last 50% of adapter layers increases the performance by 0.7% without signiﬁcant compromise to performance. The authors' work was funded by the NWO and the UK's Innovational Research In Vidi, the Netherlands, the UK and the US Department of Science and the Netherlands. </p>
