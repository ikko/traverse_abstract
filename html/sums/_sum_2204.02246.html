<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization</h3>
<h3>Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization</h3>
<img src="_sum_2204.02246.html.1.png">
<p class="text"> Reward-Switching Policy Optimization (RSPO) is a paradigm to dis-cover diverse strategies in complex RL environments. RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCocontinuous control to multi-agent stag-hunt games and StarCraftII challenges. It has been a popular consensus consensus with theoretical justiﬁcations that most local optima are very close to the global optimum. Therefore, discovering a diverse set of policies can be critical for many RL applications, such as producing natural dialogues in chatbot. </p>
<img src="_sum_2204.02246.html.2.png">
<p class="text"> Reward-Switching PolicyOptimization (RSPO) is a novel algorithm for continuously discovering novel strategies under a single reward function. RSPO discovers novel strategies by solving a ﬁltering-based objective, which restricts the RL policy to converge to a solution that is different from a set of locally optimal policies. After a novel strategy is obtained, it becomes another reference policy for future RL optimization. They propose to use cross-entropy-based diversity metric for policy optimization and two diversity-driven intrinsic rewards.<br/> In reinforcement learning, one of the most popular paradigms is population-based training with multi-objective optimization. Representative works include the family of qualitative-diversity (QD) algorithms, such as MAP-Elites (Cully et al., 2015) and DvD algorithms. There are also some recent works that combine QD algorithms and policy-gradient algorithms (Cideron et al. 2020; Tang et. 2021) PSRO focuses on learning Nash equilibrium strategies in zero-sum games by maintaining strategy oracle. </p>
<p class="text"> In theory, solving the constraint problem in Eq. (2) may lead to a solution that is not a local optimum but the unconstrained objective J(θ) can be located on the horizon of the constraint space (i.e., D(πθ, πi) = δ), which is undesirable according to the original goal. Trajectory Filtering objective reduces the population-based objective to a standard constrained optimization problem for a single policy, which is much easier to solve. Such an iterative procedure does not require a large number of policies.<br/> The main issue in Eq. (5) is that trajectory ﬁltering may reject a signiﬁcant number of trajectories. This may break learning due to the lack of feasible trajectories in the early stage of policy learning. They show in App. G that solving Eq., (5), is roughly equivalent to solving. (2) with an even stronger diversity constraint. In addition, they also remark that trajectory. (4) shares a conceptually similar motivation with the clipping term in ProximalPolicy Optimization (Schulman et al., 2017). </p>
<p class="text"> An intrinsic reward rint(st, at; πj) = − log π(at|st) is applied to each state-action pair from every rejected trajectory. The reward prediction function f(s, a; ψj) is expected to predict the extrinsic environment reward more accurately on state-actions that are more frequently visited by π j and less accurately on rarely visited pairs. They propose two different types of intrinsic rewards to promote diversity exploration: one islihood-based, which directly follows Eq. (6) and focuses more on behavior novelty, and the other is reward-prediction based.<br/> Reward-Switching Policy Optimization (RSPO) is adaptively ‘switching’ between extrinsic and intrinsic rewards during policy gradients. They use behavior-driven intrinsic reward for computational simplicity and augmenting it with intrinsic reward in more challenging scenarios. By combining these two intrinsic rewards together, they approximately maximize the divergence of both actions and behaviors between policies to effectively promote diversity. They also introduce two implementation enhancements for better empirical performances, especially in some performance-sensitive scenarios. </p>
<p class="text"> An empirical way of adjusting δ is sensitive to each reference policy. They use automatic threshold selection by default. Smoothed-switching empirically improves training stability when a large number of reference policies exist. In particle world and stag-hunt games, all the local localoptima can be precisely calculated. In MuJoCo control and StarCraft II Multi-Agent Challenge (SMAC) they qualitatively demonstrate that the method can discover a large collection of visually-distinct strategies. The implementation is based on PPO (Schulman et al., 2017) on a desktop machine with one CPU.<br/> They compare RSPO with several baselines, including PG, Diversity-Inducing Policy Gradient (DIPG) (Masood & Doshi-Velez, 2019), PBT-CE, DvD (Parker-Holder et al., 2020b) and Random Network Distillation (RND) RND is designed to explore the policy with the highest reward, so they only evaluate RND in the hard mode. The landmark size decreases at an exponential rate while the reward gain is only marginal, making it exponentially harder to discover smaller landmarks. </p>
<p class="text"> RSPO optimizing the soft objective in Eq. (4) with the default behavior-driven intrinsic reward rint.B(no-switch), which was able to discover the policy towards the second largest landmark. The optimal NE with the highest rewards for both agents in these games are risky cooperation, i.e., a big penalty will be given to an agent if the other agent stops cooperation. This makes most self-play RL algorithms converge to the safe non-cooperative strategies with lower rewards. The Monster-Hunt game (Fig. 3) contains a monster and two apples.<br/> The non-cooperative NE for eating apples is a safe NE and easy to discover but has lower rewards. They adopt both behavior-driven and reward-driven intrinsic rewards to tackle Monster-Hunt. Figure 4 illustrates all the discovered strategies by RSPO over 20 iterations, which covers a wide range of human-interpretable strategies, including the apple-eating strategy as well as the non-cannoying apple-eating strategy. Figure.4 illustrates the discovery of strategies over 20 different iterations. </p>
<p class="text"> Published as a conference paper at ICLR 2022. They observe a surprisingly diverse sub-optimal cooperation cooperation behaviors. Intrinsic re-ward is critical when learning a policy distinct from Apple NE. For RPG, even using the domain knowledge to change the reward structure of the game, it never discover any strategy beyond the non-cooperative Apple NE. Regarding the RSPO variants, both reward switching and intrinsic rewards are necessary. This implies that the learning policy to discover the optimal NE-alone fails to maximize the divergence of exploration tomaximize the divergence.<br/> If only one player steps on the light, it receives a penalty of −0.9L, where L is the number of previous cooperation steps. For each integer L, there is a corresponding NE where both players follow the light for L steps then simultaneously stop cooperation. RSPO directly learns the optimal cooperative NE (i.e., always cooperate) in the second iteration. They also measure the total number of discovered NEs by different methods over 10 iterations in Fig. 7b. </p>
<p class="text"> They evaluate RSPO in the continuous control domain, including Half-Cheetah, Hopper, Walker2dand Humanoid, and compare it with baseline methods. They adopt Population Diversity, a determinant-based diversity criterion, to evaluate the diversity of derived policies by different methods. Results are summarized in Table 2 and RSPo achieves comparable performance in Half-cheetah and Humanoid and substantially outperforms all the baselines. They conclude that even with the same intrinsic reward, population-based training (PBT-CE) cannot discover sufirdiscover sufﬁciently novel policies compared with iterative learning (RSPO)<br/> Reward-Switching Policy Optimization (RSPO) is a simple, generic, and effective iterative learning algorithm that can continuously discover novel strategies. Empirically, RSPO can successfully tackle a wide range of challenging RL domains under both single-agent and multi-agent settings. They remark that there may not exist an appropriate quantitative diversity metric for such a sophisticated MARL game in the existing literature. They leave further theoretical justiﬁcations and sample efrenance improvements as future work. </p>
<p class="text"> Published as a conference paper at ICLR 2022: "The hanabi challenge: A new frontier for ai research" Researchers: Emergent tool use from multi-agent autocurricula. The cross-entropy method for continuous multi-extremal optimization. Methodology and Computing in Applied Probability, 8:383–407, 2006. ArXiv preprint arXiv:2006:2006.08505, 2020. An evolutionary approach. Finding multiple solutions for multimodal optimization problems using a multi-objective evolutionary approach.<br/> Researchers have developed a game-theoretic approach to multiagent reinforcement learning. Advances in Neural Information Processing are published in 2020. The results will be presented at the Symposium on EmpiricalMethods in Natural Language Processing (SNAP) in February 2019. The Symposium is hosted by the National Institute of Neurotechnology (NIPS) in New York City, New York, NY, NY and NY. For more information on the study, please visit the symposium.com/niPS. </p>
<p class="text"> Published as a conference paper at ICLR 2022. Unifying behavioral and response diversity for open-ended learning in zero-sum games. Why do local methods solve nonconvex problems? Beyond the Worst-Case Analysis of Algorithms, pp. 465, 2020. Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and S. Whiteson. Maven: Multi-agent variationalexploration. Efﬁcient continuous pareto exploration in multi-task.learning.<br/> Researchers: Quality diversity is a new frontier for evolutionarycomputation. Frontiers Robotics AI, 3:40, 2016. URL: http://doi.org/researchers-robotics-a-professionals.com/gomena.org: The Star-ishlycraft multi-agent challenge is a multiagent challenge. Researchers: "The Star-glyglycraft multagent challenge" is the work of the International Foundation for AutonomousAgents and Multiagent Systems. </p>
<p class="text"> Proximal policy optimizing algorithms. Published as a conference paper at ICLR 2022. Researchers: Discovering diverse multi-agent strategic behavior via reward randomization. The surprising effectiveness of mappo in cooperative, multagent games. An artificial economist: Improving equality and productivity with AI-driven tax policies. The AI economist. An artificially-powered AI economist could be a model for the future of the economy. An algorithm that could be used to predict future outcomes of a game that rewards rewards. An approach to AI economics. An attempt to use AI as a model to improve society. </p>
<p class="text"> The strategies induced by baseline methods and RSPO are discussed in Table 4 and Table 5. They expect different emergent strategies to be visually different and human-interpretable. They present screenshots of emergent. strategies in the SMAC environment in Fig. 14 and Fig. 15 for the hard map 2c_vs_64zg and 2m_vs.1z respectively. Figure 14a shows an aggressive strategy that agents need to control the 2 colossi to ﬁght against 64 zergs. </p>
<p class="text"> Two colossi keep staying together on the left side of the cliff to ﬁre at all the coming enemies. The colossis make use of the terrain to play hit-and-run. The results show that many emergent behaviors are discovered by RSPO that are properly optima that are well-learned by the algorithm. Strategies induced by baseline methods and RSPo in SMAC map 2c_vs_64zg. The results are presented in Table 6.1. </p>
<p class="text"> The median evaluation winning rate and standard deviation on the hard map 2c_vs_64zg is 98.4%(6.4%), which is slightly lower than the state-of-the-art 100%(0) The policies discovered by the algorithm are both diverse and high-quality winning strategies (local optima) showing intelligent emergent behaviors. They note that high population diversity scores might not necessarily imply a diverse strategy pool in complex environments like SMAC. They hypothesize that this is due to complex game dynamics in SMAC. </p>
<p class="text"> They are so far the ﬁrst paper that ever reports such a visually diverse collection of winning strategies on a hard map in SMAC. They have scaled the denominator of the RBF kernel in the Population Diversity matrix by a factor 10, such that the difference can be demonstrated more clearly. They focus on the data efﬁciency, i.e., the proportion of accepted trajectories in a batch. The default values used in the experiments can be found in Table 12 and the result is shown in Fig. 12 and Table 9. </p>
<p class="text"> PGA-MAP-Elites algorithm requires a human-deﬁned behavioral descriptor (BD) to map a neural policy into a low-dimensional (discretized) space for behavior clustering. RSPO may slow down convergence, fail to discover non-trivial local optima due to lack of exploration (Fig. 1) or get stuck during exploration ( Fig. 5) Evolutionary methods, as another popular line of research, have also shown promising results in a variety of domains.<br/> When measuring population diversity, these unconverged policies would contribute a lot even though many of them may have unsatisfying behaviors/returns. By contrast, the objective of RSPO aims to ﬁnd diverse local optima. This also suggests a further research direction to bridge such a convergence-diversity gap. The quality of BD can strongly inﬂuence the performance of evolutionary methods. Note thatthe BD in Escalation provides a particularly clear signal on whether a policy reaches a local policy. </p>
<p class="text"> An improper BD may lead to a largely constrained behavior space. Without an informative BD, evolutionary methods typically require a large population size. They do not report the population diversity score in the main body of the paper. They also evaluate SMERL in SMAC with a latent dimension of 5, which only induces 1 strategy on each map across all possible latent variables. They hypothesize that in such a complex environment with a large state space and long horizon, the skill latent variable may be particularly challenging to be determined from game states.<br/> In the easy mode, the fthe goals spawn uniformly randomly with x ∈ [−1, 1] and y ∉ (0.5, 0.5), at each step, the agent receives an additional goal. In the medium and hard modes, the goals spawn at least uniformly randomly. At each step of the experiment, the agents receive an extra goal for each step. The experiment was created by Openai's team of engineers and engineers from the OpenAI team of researchers at MIT and Google. </p>
<p class="text"> Both Monster-Hunt and Escalation are based on a 5×5 grid. In both games, the action space contains five discrete actions. The policy output is a softmax distribution over these actions. They used vector representations with continuous values for observations, i.e., two real numbers to represent the 2-dimensional positions for each entity. They use the MuJoCo environments from Gym (version 0.17.3) with a moving forward reward. Detailed description of state/space and reward function can be found in Rashid et al. </p>
<p class="text"> The PPO hyperparameters they use for each experiment are shown in Table 11. PPO with separate policy and value networks as the algorithm backbone of RSPO in all experiments. They also note that the hidden size is 64 across the 4 domains, which may hurt performance especially in the MuJoCo environments. They use a large batch size for PPO training to alleviate the defect of trajectory ﬁltering. In the late stage of training, more trajectories are accepted and learning smoothly converges to a locally optimal solution. </p>
<p class="text"> The algorithm-speciﬁc hyperparameters are inherited from other papers which have proposed the algorithm or utilized the algorithm as a baseline. They use SMERSMERL to tackle perturbation in the continuous control domain. They also utilize the Bayesian Bandits module to automatically adjust the coeferenastic loss from {0, 0.5 to 0.1 in SMAC and 1.0 in other environments. They use the open-source implementation3.PGA-MAP-Elites to implement the RSPO algorithm. They performed a grid search over 2e0, 1e-1, 2e-3 on the Lagrange.multiplier in<br/> They train SMERL with a latent dimension of 5 with intrinsic reward coefﬁcient α = 10.0%; they train SMerL with an intrinsic reward of 10.1%, they say. The reward is a reward of being trained with a hidden dimension of five with intrinsic rewards of 10%. They are happy to present the results to the public with a public version of this article. They hope to use this to improve the accuracy of the search for a new type of reward system. </p>
<p class="text"> Published as a conference paper at ICLR 2022 and trajectory threshold raio ϵ = 0.1, same as the original paper. For a fair comparison, they train SMERL for latent-dimdim× more environment frames. They use the approximate Jensen-Shannon divergence with action discounting kernel as an auxiliary loss in population-based training. They performed a grid search over 1 e-2, 1e-2,. 1e0 on the TrajDiv loss and over 1e.1 on the action-kernel discounting factor. They provide the following theorem that connects the deﬁnition of PD with the original PD in continuous action space.<br/> In the case of stochastic policies, they have a set of policies. They also have a definition of the policy framework. They have a formula for a policy framework that is based on a policy that is dependent on a single policy. They say that the policy is conditional on the policy of the government. They then say the policy was conditional on a given policy that allows the policymaker to make a policy change. The policy framework is then defined as a framework for the policymaking process. </p>
<p class="text"> The accumulative KL-divergence of a trajectory is deﬁned as an accumulative kL-Divergence. They choose to use cross-entropy instead of KL-diversgence instead. Maximizing KL-difference between policies could be problematic in RL problems. The above derivation suggests that optimization would inherently encourage learning a.policy with small entropy with small. This is usually undesirable. Therefore, they choose to. instead of using KLdivergence. Instead, they. choose the. approach to KJSD(πi) = det(KJSD) </p>
<p class="text"> They claim that solving the trajectory ﬁltering objective in Eq. (5) and the reward switching objective in. (6) is equivalent to solving. (2) with an even stronger.diversity measure DﬁLter(πi, πj) They omit the discounted factor γ for conciseness. For each global optimal policy π∗, they have. a big constant to. each possible trajectory. The assumption ensures that the problem is solvable. For any MDP with bounded rewards, they can shape the reward function without changing the optimal policies.<br/> They would like to prove Theorem G.1 by showing that the optimal value of Eq. (17) over π is greater than the value over ρ /∈ S. They use S to denote the subspace of feasible policies satisfying all the distance. Constraints are S = { π | ∀1 ≤ i ≤ k, Dﬁlter(π, πi) ≥ δ, D ﬁLter( π) ≥ k, i.i) ≤ i < k, d < k > 1. </p>
<p class="text"> The right-hand side of Eq. (17) can be written as ‘apologeticmax.’ (19) The solution of the right-handed side is a globally optimal policy π∗ /∈ S. By induction, by induction, the solution is the same form as the solution of eq.(17) The aim is to prove that the maximum function should betained when π. (obtained) when the value of the function is 1 for π, because the constrained objective and the unconained objective have the same.<br/> Theorem G.2. (Switching Objective) Consider the constrained optimization problem in Eq. (16) Given Assumption. 1, 2 and 3, for any δ > 0, there exists some λ > 0 such that solving the following unconstrained optimization problem. (17) must be a solution of Eq., G. 2, G. 3 and G. 4. Theorem: G. (G. 2) G. 1: For any given Assumption, for example, there is some such a certain number of λ < 0, such as 0. </p>
<p class="text"> The critical part is to show that the optimal value of Eq. (24) w.r.t. a particular iteration k over π ∈ S is greater than the value of π /∈ S. Consequently, the solution should satisfy the constraints in E. (16) and the all proposition holds by induction. For π/S, the unconstrained optimization problem changes to the constraint-satisfying set of τ given a policy π. The right-hand side of E.Q. (23) changes to E.T. (25) and E.Y. (26)<br/> They would like to show that the expectation conditioned on the event {τ ∈ T} and Pπ(τ) =apologeticEτ∼π [Φk(T) =. (24) as:. (26) The expectation of the event is conditioned on an expectation that the event occurs. The expectation is the expectation of an event such as the event that is expected to occur. They say that this expectation is based on the expectation that an event will occur. </p>
<p class="text"> In practice, the constraint-violated trajectories will have a very low probability to be sampled. In this case, the diversity constraints will be rarely violated given a limited number of trajectory samples. This is empirically justiﬁed as shown in Fig. 5 and Fig. 12 where the trajectory acceptance rate consistently stays at 1 in the later stage of training. The algorithm can effectively discover the optimal solutions when the algorithm finds theoremsare found, they can even justify the set of the algorithm.<br/> The goal of RSPO is not just to just find a policy with high reward. Instead, RSPo aims to find as many distinct local optima as possible. The goal is to find a single policy that can approach the global optimal solution to produce the highest reward. They believe there will be still still huge room for further theoretical analysis, which they leave as future work. It is possible that a policy produced by the method reaches a suboptimal solution if nearly-optimal solutions have been all discovered. </p>
<p class="text"> As a general solution, RSPO can deﬁnitely be applied as an exploration method by escaping sub-optimal strategies. It is indeed possible that an iteration “fails”, e.g., some constraints may be violated or the policies may simply converge to previous modes leading. They empirically observe that these “failure” iterations typically lead to visually indistinguishable behaviors, which would not not notaffect the evaluation metric. They use MuJoCo under a personal license. </p>
<p class="text"> Published as a conference paper at ICLR 2022, published as a paper by RSPO. Diverse strategies discovered on the 2c_vs_64zg map in SMAC. Aggressive left-wave cleanup and smart blocking. Cliff-sniping, smart blocking, fire attractor and distant sniper sniper. Cliff walk and fire-attraction strategies were discovered on SMAC map. Diversive strategies discovered by RSSO on 2c vs. 64zg maps. </p>
<p class="text"> Published as a conference paper at ICLR 2022. RSPO discovered diverse strategies on the 2m_vs_1z map in SMAC. Swinging, parallel hit-and-run, one-sided vertical swinging and alternative distraction are indicated by yellow arrows. The movement-based strategies were discovered on the SMAC map. The study was published as a paper by RSPo and published in the International Conference of the International Statistical Statistical Association of the Organization of Statistical Studies and Organization of Standards and Organization (ISO) </p>
