<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Deep Understanding based Multi-Document Machine Reading Comprehension</h3>
<h3>Deep Understanding based Multi-Document Machine Reading Comprehension</h3>
<img src="abstract.png">
<p class="text"> Researchers propose a deep understanding based model for multi-document machine reading comprehension. It has three cascaded deep understanding modules which are designed to understand the accurate semantic.meaning of words, the interactions between the input question and documents, and the supporting cues for the correct answer. They evaluate the model on two large scale benchmark datasets, namely TriviaQA Web and DuReader. Extensive experiments show that the model achieves state-of-the-art results on both datasets. The authors contribute equally to this research and are listed randomly. </p>
<img src="abstract.png">
<p class="text"> Machine reading comprehension (MRC) aims to answer questions by reading given documents. It is considered one of the core abilities of artificial intelligence (AI) and foundation of many AI-related applications like next-generation search engines and conversational agents. In real-world scenarios, MRC is often required to answer. It requires a model have the ability of accurately understanding the semantic meaning of words in a document and its corresponding question. Human readers can well-overcome challenges by using some reading patterns like “read + verify” or multi-step reasoning.<br/> There are usually three kinds of hierarchical understandings when human readers conduct a reading comprehension task, including the semantic meaning understanding of the words. They further notice that there are. usually three. kinds of understandings. when human.readers conduct a. reading comprehension. task, such as the hierarchical understanding of. the words 'synthetical information' and 'information' are different ways to understand the meaning of a word. The study is published by ACM Trans. Asian Low- </p>
<p class="text"> They categorize the MRC task into single-document and multi-document MRC tasks. They classify the main modules in the models of this kind of task into following fthe layers. The models with large amount of parameters require very large memory-hardware, which may be unaffordable to many users. The emergence of BERT [Devlin et al. 2018] and lots of its variants (like RoBERTa) greatly boost the benchmark performance of current MRC models due to their strong capacity for capturing sentence-level language representations.<br/> XLNet is an auto-regressive based model, and can handle long text theoretically. XLNet, it is an uni-directional model which can make predictions based on forward information only. Researchers often design similar layers as in the single-document MRC task, but integrate new techniques to make full use of the multi-document information. The research was published in the Trans Asian Low-Resour. Lang. Inf. Process., Vol. 1, No. 1 January 2022. </p>
<p class="text"> Deep Understanding based Multi-Document Machine Machine Reading Comprehension (MRC) Method is based on reading patterns used by humans. It mainly consists of three understanding modules that are designed to imitate human’s three kinds of understandings. The Accurate Word Semantic Meaning Understanding module will generate a vector representation for each word in these input texts. The Interaction Understanding module further mines the interactions between the question and its documents, and then output a new vector representation. In these modules, these modules are integrated into integrated question-aware features. </p>
<p class="text"> The Answer Supporting Cue Understanding module mines the cue information which can indicate the possibility of a word in the documents being a context word in an answer. This module will output a refined vector representation for each word in documents. The model computes the probabilities of each word being the start and end tokens of an answer. Based on these probabilities, an answer can be deduced. It is also in line with human readers’ reading pattern that they often grasp the most basic step when solving a reading comprehension task. </p>
<p class="text"> Deep Understanding based Multi-Document Machine Machine Reading Comprehension1:7:7.7: 7:7 literal meaning of a word firstly, and then verify this meaning by placing it into different contexts to obtain the accurate semantic meaning of this word in the given context. Given a question and 𝑘 documents, they use some widely used word embedding techniques2 to generate word representations for them. Each item in these sequences can be viewed as a coarse semantic meaning for the corresponding word. Then these coarse semantic.meaning will be refined by integrating context information to get the final accurate semantic. meaning.<br/> They use the GloVe word embeddings [Pennington et al. 2014] that are generated by a CNN model and the highway network. All of them are widely used in existing MRC models. They use W(.)apologeticf, where W. (5) is trainable matrices. They also use the character embeddeddings generated by the CNN model, and the network network, to test the knowledge of how well they can be used in the MRC model. </p>
<p class="text"> Fthe steps will be iterated 𝐿 times to obtain the final accurate semantic meaning for each word. This repeated manner has been proven to be effective [Liu et al. 2018] for an MRC task. They use the attention method used in BiDAF [Seo et. 2016] to find which words are more helpful from the perspective of finding an answer, which is in line with the principle of attention mechanism. They denote the final word-syntheticrepresentation sequences for 𝑄 and 𝓓 𝓄 as the final semantic-meaning for the corresponding word.<br/> They use a BiDAF-alike fusion method to combine the attention vectors and the embeddings obtained in previous word semantic meaning understanding module together to yield a document representation sequence G. Each of its items 𝑔𝐷𝑡 is computed with Eq. (6) The input of this module contains word representations of both the question-question question and documents, but the output only contains the word representations. This is because that the input of the module contains words for question-aware information and documents. </p>
<p class="text"> The number of input tokens is different from the number of output tokens. In the multi-document MRC, every document is expected to contain the answer or some information that is highly related to the answer. If an answer candidate in a document is the correct answer, it would be highly possible to achieve extra supporting cues3 from other documents. They design an intra-document and inter-document self-attention based method to collect these supporting cues. In other words, this module is designed to highlight the answer’s content words from the perspective of other words in the same document.<br/> They define the answer supporting cues as a kind of information that is very helpful for locating an answer. They concatenate all the words’ representations obtained in previous intra-document supporting cue understanding step together to form a new representation for this document. Then the inter-document self-attention is performed on 𝑃 to generate 𝐹𝑝 = {𝑓 � 1.2.3...,.., � and ‘-’ is a word in the concatenated document. </p>
<p class="text"> They use a pointer networks based method that is similar to the ones in BiDAF [Seo et al. 2016] to predict the probability of each word in 𝐹𝑝 being the start or the end of an answer span. They define the loss function as the negative sum of the log probabilities of the predicted distributions of the true start and end indices over all samples, as shown in Eq. (12) They evaluate the model on Trivia Web [AQvia Web].<br/> TriviaQA is an English MRC dataset containing over 650K question-answer-evidence triples. It includes 95K question answer pairs authored by trivia enthusiasts. Six per question on average are generated from either Wikipedia or Web search. Note there are two separated datasets: one is for the single-document MRC, and the other for the multi-document multidiary MRC benchmark datasets. The dataset is published in the Asian Low-Resour. Lang. Inf. </p>
<p class="text"> DuReader is a Chinese multi-document MRC dataset that contains 200K questions, 420K answers and 1M documents. It has three advantages over previous MRC datasets. DuReader provides free-form reference answers that not all can’t be found in the input documents. During training, they also design a simple string matching based data preprocessing module to filter out some irrelevant sentences from each question’s given documents. They choose the span that achieves the highest ROUGE-L score with its reference answers as the golden span for training.<br/> The RoBERTa model is implemented by a transformer code base, which can be found at following website:https://://://github.com/huggingface/transformers/. The RoberTa model was used to make ROUGE-L and BLEU4. In the in-house experiments, they find that using answer achieves better experimental results than using “question”. Then they report all the other experiments based on these two fixed parameters. The results are shown in Table 2, from which they can see that. </p>
<p class="text"> Effect of repeated numbers on DuReader.ROUGE-L/BLEU4: 1.2/48.03/47.87/49.03: 2.2.2: 2:12/50.74: 1:12.12: 12:12-2:12:12; 2:13:1:12 (2:4:12) 1-1-2-3:1-4:2-1.1-1. Main results on TriviaQA Web were generated by us.<br/> The phenomenon is also common on other free-form MRC datasets like MSACM Trans. Asian Low-Resour. Lang. Inf. Process., Vol. 1, No. 1. Publication date: January 2022. For confidential support call the Samaritans on 08457 909090 or visit http://www.samaritans.org/. In the U.S. call the National Suicide Prevention Lifeline at 1-800-273-8255 or go to http </p>
<p class="text"> The main experimental results are summarized in Table 3 and 4. From these results they can see that the model is very effective: on both datasets and under all evaluation metrics, it consistently performs all the compared state-of-the-art baselines. They argue this is mainly due to the issue of handling long documents. XLNet can not make full use of the given documents due to its auto-regressive mechanism, but XLNet is a pretrained model, some words’ semantic meaning generated by XLNet may not well match the true scenario in an MRC dataset. </p>
<p class="text"> Table 5. Ablation experiments on TriviaQA Web (upper part) and DuReader (bottom part) They can see that the model is adaptable to different choices other than BiDAF. They also conduct experiments that perform a repeated operation in this interaction-understanding module by a simple linear transformation operation. The more documents provide supporting cues, the more likely an answer candidate will be the correct answer. The interaction un-privacyderstanding modules are important in the model. When a model is integrated into the framework of the model, it always achieves much better results than the original version. </p>
<p class="text"> Table 7. Comparisons of using different interaction understanding methods on TriviaQA Web (upper part) and DuReader (bottom part). Table 8. Effect of repeated numbers (N) for the interaction understanding module on DuReader and Trivia QA. Table 8 shows that the similarities between these two.representation become lower and lower as the repeated number increases. Table 9 shows that when N = 3, the performance of the model is even worse than that of removing the whole interaction understanding. </p>
<p class="text"> They quantitatively compare the parameter numbers of several models whose source codes are available. They can see that the model is more parameter efficient: it achieves better results with fewer parameters. They do not compare the run time of different models because it is difficult to provide a fairevaluation environment: coding tricks, hyper-parameter settings (like batch-size, learning rate, etc), parallelization, and a lot of non-model factors affect run time. The results are shown in Table 10, in which all the listed examples are taken from TriviaQA Web. </p>
<p class="text"> The unanswerable kind of errors account for significantly larger proportion of errors on TriviaQA Web than on DuReader. The redundant kind of error account for a large proportion of all the errors, followed by those of the incomplete and redundant errors. The other kinds of errors include partial matching errors, yes/no errors, singular and plural errors. In most cases, the locations of the predicted answers are very close to the golden answers. In fact, these errors could be corrected only when a model do understand the main semantic meaning. </p>
<p class="text"> The best results achieved by the model on TriviaQA Web and DuReader test set leader-boards were No.17 and No.38 respectively. The answer “Mount Kilimanjaro" achieves different attention weights when using different answer supporting cue understanding submodules. When all the modules used, the answer achieves the highest attention weight, which increases the probability of it being the answer. They propose a simple but effective deep understanding based multi-document MRC multi-Document MRC model. It uses neither sophisticated technologies nor any pretrained language models. </p>
<p class="text"> The main novelties of the work are as follows: the model has a general framework that includes three understanding modules that imitate human’s three kinds of understandings during reading comprehension. It even plays a better role than an extra language model like XLNet but with far less parameters. The designed answer supporting cue understanding is effective, and it can increase the probability of finding answers. The work is supported by the National Natural Science Foundation of China (No.61572120) and the Fundamental Research Funds for the Central Universities.<br/> The study was published in Science in China Series F: Information Sciences, 10 (2020), 202102. It is the work of the Association for Computational Linguistics and the AAAI Conference on Artificial Intelligence, Vol. 33.29–6537. The study is published in the journal of the Asian Low-Resthe Lang. Inf. Lang. Process. Process., Vol. 1, No. 1 (2019),. The study will be published in January 2022, with publication date: January 2022. </p>
<p class="text"> Researchers at the 28th International Conference on Computational Linguistics discuss how to improve machine-readability and language-recognition algorithms. The authors also discuss how a BERT pretraining approach can be applied to medical reading comprehension. The study is published by the Association for Computational. Linguistic.computing.org and the International.comLinguistics.org.org, the.comprehensible.org. The.computers.org will publish their findings at the.accelerate.commodation.org for the next year’s conference on the topic.commissioning.commented.org/2019.<br/> Researchers: Multi-style Generative Reading Comprehension. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Vol. 1. 2227–2237. 2019. 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) will be held at the Asian Low-Resthe Lang Inf. Lang. Inf. Process. 1, No. 1 (January 2022) </p>
<p class="text"> Researchers from the Association for Computational. Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019b. Enhancing Pre-TrainedLanguage Representations with Rich Knowledge for Machine Reading Comprehension. 2019b: Enhancing pre-trained language representations with rich knowledge for machine reading comprehension. 2020b: Multi-task Learning with Sample Re-weighting for machine Reading. 2018b: Joint Training of Candidate Extraction and AnswerSelection for Reading.Selection.<br/> Researchers from the Association for Computational Linguistics discuss how to use language-recognition systems. They also discuss how they are able to learn with noisy data. Their findings will be published in the journal ArXiv preprint arXiv:2105.04241 (2021) and in the Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 9636–9643 (2020) The authors also discuss the use of BERTserini to answer questions with an end-to-end open-Domain question. </p>
