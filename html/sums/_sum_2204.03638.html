<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer</h3>
<h3>Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer</h3>
<img src="_sum_2204.03638.html.1.png">
<p class="text"> Long Video Generation with Time-Agnostic.VQGAN and Time-Sensitive Transformer. Researchers at University of Maryland, Georgia Tech and University of Georgia create a frame-synthesis algorithm that synthesizes videos with thousands of frames. They say TATS can generate diverse, coherent, and. high-quality long videos. They also showcase conditional extensions of the approach for generating meaningful long videos by incorporating tempo-driven information with text and audio. Videos and code can be found athttps://songweige.io/projects/tats/index.html. </p>
<img src="_sum_2204.03638.html.2.png">
<p class="text"> Video synthesis has been an exciting yet long-standing problem. GAN-based methods can produce plausible short videos, but extending them to longer videos requires prohibitively high memory and time cost of training and inference. They tackle the problem of long video generation. The core insights lie in removing the dependence on time from VQGAN and enabling the transformer to capture long-range temporal dependence. The model is trained on short video clips, e.g., 16-frame clips, like previous methods. At inference time, they use a sliding window approach on the transformer. </p>
<p class="text"> VQGAN is a variant of VQVAE that uses perceptual and GAN losses to achieve better reconstruction quality when increasing the bottleneck compression rates. They are the first to generate long videos and analyze their quality. The model achieves state-of-the-art short and long video gen-otypeseration results on the UCF-101, Sky Time-lapse, Taichi-HD, and Taichi HD datasets. They show that the model can generate meaningful videos according to the story flow provided by text or audio. </p>
<p class="text"> VQGAN additionally adopts a perceptual loss and a discriminator fD to improve the reconstruction quality. They also use feature matching losses to stabilize the GAN training. After training the video, each video can be encoded into its discrete representation z = q(fE(x) and z0 is given as the start of sequence token. To synthesize videos longer than the training-length, they generalize sliding attention window for the use </p>
<p class="text"> VQGAN and a time-sensitive transformer can generate longer videos without degrading quality They discuss the reason and a simple yet effective fix. They provide some unbalanced effects to tokens at different temporal position When a short video clip is encoded, the zero paddings in the temporal dimension also get encoded and affect the output tokens. The tokens closer to the temporal boundary will be affected more significantly. For real data to match z(1:t−1), they have to contain these zero-frames, which is not the case in practice. Therefore, the transformer needs to see sequences that start with tokens similar to z(z(t) </p>
<p class="text"> Real-frame padding makes the encoder temporally shift-equivariant but introduces extra computations. Replicate padding makes decent approximation to real-looking frames while bringing no computational overhead. After removing all the padding, one needs to pad real frames to both ends of the input videos to obtain the desired output size. The number of needed real frames can be as large as O(Ld, where L is the number of layers and d is the compression rate. </p>
<p class="text"> The time-agnostic property of the VQGAN makes it feasible to generate long-term videos using a transformer with sliding window attention. The spirit of a long video is in its underlying story, which requires both predicting the motions in the next few frames and the ability to plan on how the events in the video proceed. They consider an autoregressive transformer that generates 4× more sparse video tokens. For the interpolation transformer, it fills in the missing frames between any two adjacent frames generated by the au-privileged transformer. </p>
<p class="text"> They report FVD and KVD on Taichi-HD and Sky Time-lapse datasets, IS and FVD on the UCF-101 dataset. They measure the longest audio-conditioned generation evaluation at the 45th frame. They follow the pre-previous methods to use Fr´echet Video Distance (FVD) and KernelVideo Distance (KVD) as the evaluation metrics. In addition, they follow methods evaluated on methods evaluated by a trained model to report the Inception Score (C3DISISIS)<br/> They adopt a compression rate d = 4 in tem-poral dimension and d = 8 in spatial dimensions. For audio-conditioned models, they train another.VQGAN to compress the Short-Time Fourier Transform (STFT) data. They refer to the model with a singleautoregressive transformer as TATS-base and the proposed hierarchical trans-former as TAT-hierarchical. They train a decoder-only transformer with size between GPT- </p>
<p class="text"> They demonstrate the effectiveness of the TATS-base model under a standard short video generation setting, where only 16 frames are generated for each video. They measure video quality with respect to the duration by evaluating every 16 frames extracted side-by-side from the generated videos. They compare the proposed methods with the Vanilla VQGAN and the state-of-the-art models including MoCoGAN-ogle-HD, DIGAN, and CCVS They also explore generation with class labels as conditional information. </p>
<p class="text"> They report the FVD changes of these generated clips compared with the first generated 16 frames in Figure 5. They show that the generation still progressivelydegrades severely after certain number frames on the UCF-101 and Taichi-HD datasets. The generated long videos should follow a consistent topical theme. They use the same trained C3D model for IS-Calculated and propose two metrics, Class Coherence Score (CCS) and Inception Coherence score (ICS) at time step t, measuring the theme similarity between the non-overlapped 16 frames w.r.t.t. </p>
<p class="text"> This section shows qualitative results of videos with 1024 frames and discusses their common properties. 1024 frames per video approaches upper bound in the FVD. They find that some video themes contain repeated events such as Handstand Push-Up classes in the UCF-101 dataset and videos in the Taichi-HD dataset. They show that with enough training data available for a single theme, the model learns to “stitch” the long videos through generat-guiing smooth transitions between different scenes and motions. </p>
<p class="text"> Every 100th, 10th, and consecutive frames are extracted from a generated sky video with 1024 frames. They discuss the related work in video generation using different models including Generative Adversarial Networks (GAN) and implicit neural representations (INR) The earliest work generates en-tire videos using 3D deconvolutionals. Subsequent works separate the temporal branch from 2D frame generation and adopt RNN or LSTM models to pre-dict the latent vectors as the input to image generators. </p>
<p class="text"> Autoregressive models generate images or videos pixel-by-pixel and have become a ubiquitous generative model for video prediction. They propose TATS, a time-agnostic VQGAN and time-sensitive transformer-like model, that is only trained on clips with tens of frames. They show that such a VQVAE-based video generator is promising to generate long videos with long-range dependence. They hope it can encourage future works on more interesting formsof video synthesis with realistic number of frames, perhaps even movies. </p>
<p class="text"> They find that directly applying GAN losses to VideoGPT leads to severe discriminator collapse. The axial-attention layers introduced in videoGPT interact with the losses and exacerbate the collapse. They present several empirical methods they found effective in stabilizing the losses. They also discuss the undesired temporal dependence induced by the zero padding, ablations on different potential solutions, and description of the interpolation attention. They use a pure TATS model to stabilize training 3D-VQGAN with GAN loss losses. </p>
<p class="text"> A more powerful decoder helps the reconstruction follow the discriminator closely. They use Synced Batch Normalization as a replace of Group Normalization used in original VQGAN and accumulate gradients across multiple steps. Blob-shaped artifacts often appear in the reconstruction and exaggerate along with the intermediate feature maps as shown in Figure 11, which was also observed in StyleGANs and can be attributed to the normalization layers. This is especially pronounced in the training video VQAN due to the small batch sizes. They aim to show that when zero padding is used, there are barely tokens starting with z(1:t−1 </p>
<p class="text"> They use a shift-equivariant version of fE by moving moving moving paddings to the input. The number of paddings needed is N = O(Ld), where L is L is the number of convolutional layers in the encoder. They show that no such tokens are starting with z(1:t−1) in the training set. They propose an equivariance score as a measure of time agnostic, which calculates the percentage of tokens that are identical when the same frames are positioned at the beginning of the clips. </p>
<p class="text"> The more paddings are removed, the more time-agnostic the encoder becomes. They report the mean and standard deviation across 1024 clips using a video VQGAN trained on the UCF-101 dataset across different padding strategies. They also partially remove the zero-paddings from different numbers of layers to picture the trade-off varies more accurately. They find that the replicate padding, which gives 0.75 consistency score, already resolves the time-dependence issue well in practice. They use replicate paddings and no real frames in the experiments. </p>
<p class="text"> They validate the approach on the UCF-101, Sky Time-lapse, Taichi-HyperHD, AudioSet-Drum, and MUGEN datasets. They train the model on its train split which includes 9, 537 videos following the official splits. They also report number of frames in videos of every class in Figure 16.4 and number of long videos per video in Figure 15.1-16.4. </p>
<p class="text"> UCF-101, Taichi-HD, MUGEN, AudioSet-Drum and Sky Time-lapse datasets all have at least a certain number of frames. They train the model on the train split and test using videos from the test split. They follow to sample frames from every 4 frames when train-ing the TATS-base model for a fair comparison. To calculate the VFDs and DVDs, they generate 2, 048 and 512 videos for short-and long video evaluation, respectively, considering time cost. </p>
<p class="text"> They train the VQGAN on 8 NVIDIA V100 32GB. GPUs with batch size = 2 on each gpu and accumulated.batches = 6 for 30K steps. Each model usually takes around 57 hours to train. They use the AdamW optimizer with a base learning rate of 4.5e−6, where they linearly scale up by the total batch size. They train TATS-base on UCF-101 dataset for 1.35M as it keeps improving results. </p>
<p class="text"> They provide additional descriptions on the baselines they compared with and other GAN-based video generation models in this section. TGAN proposes to generate a fixed number of latent vectors as input to an image generator that synthesizes the corresponding frames. DVD-GAN adopts a similar archi-centrictecture as MoCoGAN with a focus on scaling up training. HVG introduces a hierarchical pipeline to interpolate and upsample low-resolution and low-frame-rate videos gradually. ProgressiveVGAN extends Progressive-generation to video generation by simultaneously generating in the temporal di-rection progressively. </p>
<p class="text"> Long Video Generation focus on producing videos from noise. Video prediction starts from a real frame while video genera-heticaltion starts from generated frame. Some video generation models fall into the middle part that predicts future frames given frames generated by an image-generator. They have shown that these models also suffer from quality degradation. The paper focuses on more realistic and complex videos with weak or no conditional information available. The human faces in the following images are blurred to protect privacy to avoid the deep fake policy risk. </p>
<p class="text"> Unconditional and class-conditional generation results of UCF-101 videos with 1, 024 frames that contains repeated action. Failure cases of long videos with smooth transitions. Sky Diving, Boxing Punching Bag, Fencing and Taichi-HD videos with long videos. Every 10th frame between frame 300 to frame 400.. Class-Conditional Generation Results:Unconditionalgenerationsof long videos on the dataset. </p>
