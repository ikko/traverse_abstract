<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask</h3>
<h3>Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask</h3>
<img src="_sum_2204.02908.html.1.png">
<p class="text"> Question Generation for Reading Comprehension Assessment by modeling How and What to Ask? They propose a two-step model that takes advantage of previous data and can generate questions for a targeted comprehension skill. They show that the HTA-WTA model tests for strong SCRS by asking deep inferential questions. Reading is an invaluable skill, and is core to com-ogleicating in the digital age. Reading also sharpens their memory, and improves social cognitive skills (Halliday, 1973; Mason, 2017)<br/> They take inspiration from continual learn-centricing (Parisi et al., 2019), which orders a set of learn-consuming tasks to improve model performance. They begin by training a model on the general task of QG (How to ask: HTA) and follow with the task of interest: Generating a targeted question of a particular type of question. Xiv.arXiv:2204.02908v1 [csCL] 6 Apr 2022: They begin </p>
<img src="_sum_2204.02908.html.2.png">
<p class="text"> They build a novel QG dataset for SBRCS. The dataset contains advanced reading comprehen-centric-centric skills extracted from stories. They propose a two-step method to generate question-related questions from a given story. The method takes advantage of previous datasets to improve generalizability, and then, teaches a model how to ask predeﬁned styles of ques-pronetions. The experimental setting is described in Section 5. The results and analysis are presented in Section 6. (What to ask: WTA)<br/> The last group is only on generating questions that start with interrogative words (what, how, etc.) QG has been used to solve many real-life prob-orative questions. They believe that advanced reasoning skills (e.g. comprehending) are usually used in children’s stories to assess comprehension skills. They use a extensive set of reading compre-orativehension skills that deeply evaluates the abilities of the readers. The stories (passages) are multi-genre, self-contained narratives. This content variety leads to content variety. </p>
<p class="text"> There are 726 stories, which can have questions from multiple skill types (described in Section 3.1) 25 professionalprofessionals contributed to the writing process (18 teachers, 7 graduate students) 25 annotators were asked to write a question per skill for a given story. Not every skill is applicable to ev-gyrophyry story, so some skills were discarded for some stories. Overall, they gen-erate 4K question-answer pairs, with an average of 5.5 pairs per story. Collected dataset’s statistics.<br/> They propose a two-step method to generate skill-related questions from a given story: HTA followed by WTA. HTA teaches the model the typical for-glymat for comprehension questions using large pre-previously released datasets. They use two well-known datasets, SQuAD (Rajpurkar et al., 2016) and Cos-ophobicmosQA (Huang et al. 2019) In HTA, they train (ne-tune) a model on large QG datasets, and then, they further train the model to teach the model what to ask (WTA) </p>
<p class="text"> The How to Ask (HTA) and What To Ask (WTA)QG models take a passage/story as input and gen-orative a question. The type of generated question is not controlled and is left for the system to decide how the question should be generated. They propose this setting to generate fewer literal questions. The model can generate more than one question-answer pair, but it can't learn to generate infer-iopential questions. They use a special token to control the style of the generated question.<br/> Decoding strategies are crucial and directly impact output quality. Beam Search (Reddy,1977) is the most common algorithm. Nucleus sampling selects the smallest possible set of possible models. A similar technique was used in the literature to include per-centricsona pro-generation agents in dialogue agents to produce more coherent and meaningful conversations (Scialomomet al., 2020) In Beam-Search, the output of a model is found by maxi-gling the model probability </p>
<p class="text"> They use BLEURT (BLEURT-20) to evaluate a generated text by giving it a value between 0.0 and 1.0. They also report standard MT metric (BLEU) and perform an additional manual evaluation. They train all models with a maximum of ten epochs with an early stop-pointer value of 1 (patience) based on the validation set. They use a single NVIDIA TITAN RTX with 24G RAM to evaluate the performance of models that showed state-of-the-art results.<br/> CGC-QG (Liu et al., 2019a): a Clue Guided-Guide network for Question Generation. AnswerQuest (Roemmele et al. 2021): a model that uses as a ﬁrst step a pre-pipeline model to retrieve the relevant sentence that has the answer from a document. And then the sentence is fed to a transformer-based sequence-to-sequence model that is enhanced with a copy mecha-centricism. </p>
<p class="text"> HTA-WTA’s BLEURT score out-performs all of the previous QG models by a notice-worthy margin. They argue that the Clue Words Prediction Module learns important cues, increasing the uni-gram overlap with the gold references (BLEU-1) This may be due to the fact that they did not include the skill-related input in the encoder, which guides the model to generate skill related questions. The CGC-QG model achieves a higher BLEurT score than the HTA WTA model. The One-Step model performs similarly to the previous baselines.<br/> They select a sample of 110 story-question pairs from the test-dataset, for both models. Then, they perform a hu-ishlyman evaluation using crowdworkers on Amazon.com/Mechanical Turk. They use a "master" qualiﬁcation to restrict the participation of workers in the study to those who have a high HIT accuracy. Table 4 shows the average rating as-signed by the workers for the 3 criteria. The results show that both the model score high values as it has been left to decide the question. </p>
<p class="text"> Models’ performances (percentages) on the collected dataset. For all scores, higher is better. HTA-WTA model performed perfectly on the given sample of Predicting and Figurative Language (1.0 for each skill) They evaluate the results using accuracy (see Table 4) The result for One-Step model is 0.16, and 0.8 for HTA WTA model. Human evaluation ratings for the 3 criteria are similar in terms of the given categories.<br/> Injecting only 10% of the data led to a boost in performance of 5.99% (BLEURT) The result at 10% (33.21%) exceeds the results of most of the baselines and is higher than T5-WTA and NQG-MAX models when trained on all the datasets. The behavithe was previously reported by Stap-ishlypen et al. (2020) Further research is needed to investigate the causes of this behavithe. In most cases, the perfor-glymance gradually improves as data grows. </p>
