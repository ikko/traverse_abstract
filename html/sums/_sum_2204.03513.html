<html><head><link rel="stylesheet" href="style.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.1/js/bootstrap.min.js" integrity="sha512-EKWWs1ZcA2ZY9lbLISPz8aGR2+L7JVYqBAYTq5AXgBkSjRSuQEGqWx8R1zAX16KdXPaCjOCaKE8MCpU0wcHlHA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script></head>
<h1>abstract</h1>
<h3>Many-to-many Splatting for Efficient Video Frame Interpolation</h3>
<h3>Many-to-many Splatting for Efficient Video Frame Interpolation</h3>
<img src="_sum_2204.03513.html.1.png">
<p class="text"> Many-to-many Splatting for Efﬁcient Video Frame Interpolation is a new approach to video frame interpolation. They propose a fully differentiable (M2M) splatting framework to interpolate frames ef ﬁciently. M2M only performs motion estimation once and has a mi-uinenuscule computational overhead when interpolating an ar-glygly number of in-between frames, hence achieving fast multi-frame interpolation. This is the first attempt to solve the problem of video frame in-terpolation.<br/> A common motion-based technique estimates bilateral ﬂow for the desired time step and then synthesizes the inter-gianmediate frame via backward warping Motion-based approaches establish dense correspondences between frames and apply warping to render the intermediate pixels. Forward-warping is subject to holes and ambiguities where multiple pixels map to the same location. However, forward-forward-forward warp pixels are subject to hole and ambiguity. </p>
<img src="_sum_2204.03513.html.2.png">
<p class="text"> M2O splatting may results in holes, while M2M splat-insuredting allows for a more ﬂexible image formation model. They propose a Motion-Reﬁnement Net-Device Net-Work that estimates a many-to-many relationship between the two input images, and then warps the pixels to multiple locations at the desired time step. They also introduce a learning-based fusion strategy which combines pixels that map to the same location.<br/> Motion-based video frame interpolation approaches typ-phthalically estimate optical ﬂows from given frames, then propagate pixels/features to the desired target time-consuming step Forward warping is an efﬁcient so-centric so-around to achieve this goal. Some meth-ophobicods are based on backward warping instead or depth-based splatting These methods also rely on image synthesis networks to im-guiprove the interpolation quality. </p>
<p class="text"> They use L = 4, and the numbers of feature channels from shallow to deep are 16, 32, 64, and 128 respectively. From the zeroth to the last level, they apply Joint-Flow Encoding (JFE) modules as illustrated in Fig. 3 (b) to progressively generate motion feature pyramids. The idea behind this module is that ﬂow ﬁelds are highly structured due to the underlying physical constraints, which can be exploited by low-rank models.<br/> The JFE decoder operates in L stages from coarse to ﬁne while leveraging the features-encoded by the JFE modules. Pixel warping and fusion steps operate with pixels’ colors without any subse-privatized post-processing steps. Pixel Warping is used to forward warp pixels to a given target time step. Fusion steps are used to combine the colors of pixels that map to the same location in the output. An intermediate frame can be easily interpolated with minuscule computational overhead. </p>
<p class="text"> Many-to-one (M2O) splatting can be alleviated with multiple source frames, yet M2O splatts still suffers from stray effects at certain boundaries due to its image formation model that is less ﬂexible than M2M. M2o warping still restricts each source pixel to only render a small 4-pixelvicinity in the output frame. This limits the effectiveness of M2 O warping in representing and thus interpolating regions with complex interac-tions among the pixels.<br/> They propose a general framework for fus-forming pixels from multiple frame, while SoftSplat fuses each frame individually. They also propose a learning based reliability score to fuse overlapping pixels in a data-driven manner. They test it on various datasets summarized as follows: Vimeo90K, UCF101 and UCF 101,. They then compare the proposed approach to related state-of-the-art frame interpolation tech-enabledniques. </p>
<p class="text"> They compute models’ GFLOPs and speed based on the Vimeo90K, UCF101, ATD12K, and Xiph datasets. The “share” denotes the part of compute independent from the desired frame rate, which is in contrast to ‘unshare’. “Xiph-2K” is generated by downsampling 4K footage, and ‘Xiph’ is based on ‘center-cropped 2K patches. They additionally adopt from the original resolution as X-TEST(2K) by down-sampling by a factor of two. Training pipeline in an end-to-end pipeline<br/> To train the model, they utilize the 51,312 triplets from the training-split of Vimeo90K. They apply random data augmen-tations including spatial and temporal ﬂipping, color jitter-jittering, and random cropping with 256×256 patches. They train a model for 400k iterations with a batch size of 1e-4, during which the learning rate is decayed from 1-4 to 1-0 via cosine annealing. All experiments are implemented with PyTorch and executed on a single Nvidia Titan X.<br/> This demonstrates the methods’ effectiveness when.processing high-resolution videos and the ability to.generate videos. This demonstrates how effective the methods are when they are able to produce high-quality videos. ’s ability to gener-ishly.com is demonstrated to be effective in creating high quality videos. This demonstrates the effectiveness of the methods when. processing high-resolution videos. They are confident in the ability to generate high quality video content. They hope to </p>
<p class="text"> M2M method performs all previous methods on both the original 4K full resolution (4096×2160) and the downsampled 2K res-a-guiolution (2048×1080) with substantial advantages in efﬁ-genicciency. They found that previous methods tend to deteriorate when interpolating frames that are temporally. centered between the inputs, while M2.M achieves a ﬂatterand smoother curve for intermediate frames. This shows that M.2M interpolates frames with not only better quality, but also higher temporal consistency, they compare the accuracy at each interpolation time step in Fig. 6 (b)<br/> Table 3 shows the impact of the number of sub-motion vectors for each pixel in the many-to-many splatting on Vimeo90K, with two different initial estimators. Fig. 7 illustrates the visual re-ariesults for M2O splatts and M2M splatters. Table 4 compares the effect of using different numbers of the submotion vectors with different numbers. When N=1, it reduces the warp-arounding to M2o mapping, and achieves the lowest accuracy. When increasing N to 4, increasing N improves the accuracy by more than 0.1 dB. </p>
<p class="text"> M2O splatting with initial or single re-motion vector results in undesired visual artifacts for regions with complex motion. In comparison, the proposed M2M splatted with fthe sub-motion vectors (b) can interpolate with much higher quality. The proposed method renders intermediateframes based on forward warping, which may be subject to holes in the output. They count the average number of remaining holes (in pixels) for different conﬁg-urations on Vimeo90K.<br/> Researchers present a many-to-many splatting tech-nique to efﬁciently interpolate intermediate video frames. The method is especially well-suited for multi-frame interpolation. This can be ad-agicallydressed by further improving the fusion strategy or applying a lightweight network to re-create the output. The proposed method achieves effective-rivetingness with superior efrenality. It can be used on multiple benchmark datasets, such as benchmark benchmarks. </p>
