{"url": "https://arxiv.org/abs/2204.03458", "ref": "2204.03458", "plot": "Video Diffusion Models", "image": "images/_pdf_2204.03458.png", "theme": "Artificial Intelligence", "summary": " Generating temporally coherent high fidelity video is an important milestone in generative modeling research. They propose a diffusion model for video generation that shows very promising initial results. They present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on an unconditional video generation benchmark. To generate long and higher resolution videos, they introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. They find to reduce the variance of minibatch gradients and speed up optimization."}