{"url": "https://arxiv.org/abs/2204.03592", "ref": "2204.03592", "plot": "Testing the limits of natural language models for predicting human language judgments", "image": "images/_pdf_2204.03592.png", "theme": "Artificial Intelligence", "summary": " They compared the model-human consistency of diverse language models using a novel experimental approach. For each controversial sentence pair, two language models disagree about which sentence is more likely to occur in natural text. Controversial sentence pairs proved highly effective at revealing model failures and identifying models that aligned most closely with human judgments. The most human-consistent model tested was GPT-2, although experiments also revealed significant shortcomings of its alignment with human perception. The most realistic model tested is GPT 2, but experiments also showed significant shortcomings."}