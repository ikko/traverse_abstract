{"url": "https://arxiv.org/abs/2204.03558", "ref": "2204.03558", "plot": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic", "image": "abstract.png", "theme": "Computation and Language", "summary": " As natural language processing systems become more widespread, it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized. There is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks. They find that many systems demonstrate statistically significant unisectional social biases. They use these tools to measure gender, racial, ethnic, and. intersectional social biases across five models trained on emotion regression tasks in English, Spanish, and Arabic."}