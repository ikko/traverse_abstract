{"url": "https://arxiv.org/abs/2204.03489", "ref": "2204.03489", "plot": "Position-based Prompting for Health Outcome Generation", "image": "images/_pdf_2204.03489.png", "theme": "Computation and Language", "summary": " Pre-trained Language Models (PLMs) using prompts indirectly implied that language models can be treated as knowledge bases. They observe that satisfying a particular linguistic pattern in prompts is an unsustainable constraint that unnecessarily lengthens the probing task. They therefore explore an idea of using a position-attention mechanism to capture positional information of each word in a prompt relative to the mask to be filled, hence avoiding the need to re-construct prompts when the prompts linguistic pattern changes. Using various biomedical PLMs, the approach consistently outperforms a baseline in which the default mask language model is used to predict masked tokens."}