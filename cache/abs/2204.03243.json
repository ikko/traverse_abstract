{"url": "https://arxiv.org/abs/2204.03243", "ref": "2204.03243", "plot": "Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators", "image": "images/_pdf_2204.03243.png", "theme": "Computation and Language", "summary": " AMOS pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. The main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models. For better pretraining efficiency, they propose a way to assemble multiple MLMs into one unified auxiliary model."}