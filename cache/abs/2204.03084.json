{"url": "https://arxiv.org/abs/2204.03084", "ref": "2204.03084", "plot": "Knowledge Infused Decoding", "image": "images/_pdf_2204.03084.png", "theme": "Computation and Language", "summary": " Pre-trained language models (LMs) tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. They present Knowledge Infused Decoding (KID) -a novel decoding algorithm for generative LMs. KID dynamically infuses external knowledge into each step of the LM decoding. It maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint."}