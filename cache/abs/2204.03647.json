{"url": "https://arxiv.org/abs/2204.03647", "ref": "2204.03647", "plot": "Adapting CLIP For Phrase Localization Without Further Training", "image": "images/_pdf_2204.03647.png", "theme": "Artificial Intelligence", "summary": " Supervised or weakly supervised methods for phrase localization (textual grounding) either rely on human annotations or some other supervised models. Obtaining these annotations is labor-intensive and may be difficult to scale in practice. They propose to leverage recent advances in contrastive language-vision models, CLIP, pre-trained on image and caption pairs collected from the internet. They adapt CLIP to generate high-resolution spatial feature maps. Importantly, they can extract feature maps from both ViT and ResNet CLIP model while maintaining semantic properties of an image embedding."}