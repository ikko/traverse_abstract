{"url": "https://arxiv.org/abs/2204.03227", "ref": "2204.03227", "plot": "Accelerating Attention through Gradient-Based Learned Runtime Pruning", "image": "images/_pdf_2204.03227.png", "theme": "Computation and Language", "summary": " Self-attention is a key enabler of state-of-art accuracy for various transformer-based Natural Language Processing models. The attention mechanism calculates a correlation score for each word with respect to the other words in a sentence. Commonly, only a small subset of words highly correlates with the word under attention. As such, a significant amount of computation is inconsequential due to low attention scores and can potentially be pruned. To best utilize this mathematical innovation, they devise a bit-serial architecture, dubbed LeOPArd, for transformer language models with bit-level early termination microarchitectural mechanism."}