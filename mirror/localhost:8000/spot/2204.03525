<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Traverse Abstract</title>
    <link rel="stylesheet" href="/html/style.css">
</head>
  <a id="toggle" class="toggle-dark-mode button">☾</a>
  <script src="/html/script.js"></script>

<title>Traverse Abstract - Temporal Alignment for History Representation in Reinforcement Learning - Artificial Intelligence</title>
<body>
  <div  class="leftmargin">

     <h1> <a href="/">Traverse Abstract</a></h1>
     <p class="page-title"> Temporal Alignment for History Representation in Reinforcement Learning </p>
      <span class="card">
              <p class="page-theme"><a href="/theme/Artificial_Intelligence">Artificial Intelligence</a></p>
              <img class="media-object" src="/html/sums/_sum_2204.03525.html.1.png"/>
              <p class="media-body media-body-page article p1">Temporal Alignment for History Representation in Reinforcement Learning is inspired by human memory. The method aligns temporally-close frames, revealing a general, slowly varying state of the environment. It can be interpreted as a metric that captures the temporal relations of observations. It is based on contrastive loss, which pushes away other samples from the batch. The method (TempAl) has been used to train the agent using the common self-supervised contrastive learning method. It has been applied to Atari 2600 Pong games from the Arcade Learning Environment.&lt;br/&gt; In this paper, they propose a Tem poral Alignment for History Representation (TempAl) ap-guiproach. They obtain therepresentation by aligning past state observations using the pairwise cross-entropy loss function, In more detail, they minimize the distance between the latent representations of two or three frames. This is a form of self-supervision, in which no additional annotation is needed for the representation. The 3 stages are iterated during training, and thus past embeddings can be modiﬁed to the agent.</p>
              <img class="media-object" src="/html/sums/_sum_2204.03525.html.2.png"/>
              <p class="media-body media-body-page article  p2">They use the pairwise cross-entropy loss function to align the agent’s past observations and to obtain low-dimensional embeddings. They evaluate the method using the PPO algorithm using the Arcade LearningEnvironment (ALE) benchmark. The experiments conﬁrm provide a boost with respect to the most common instantaneous-only input. They show that TempAl outperforms the baseline in cases, where the history can increase observability. The source code of the method and of all the experiments is available athttps://github.com/htdt/tempal.&lt;br/&gt; In RL, observations are obtained dynamically, thus labeling them is impractical. In this paper, they propose a com pact history representation that can be processed with much-preferred networks. In the experiments, they use a small 1D CNN with kernel size 1, which has only 68 trainable parameters. They show in Sec. IV-D that the proposal improves the performance for most of the environments. They also present related works focused on rep-privilegeresentation learning.</p>
              
                  <p class="media-body media-body-page article pn">The RL agent consists of two branches: instantaneous and history. The former is a common RL architecture, which processes fthe recent observations and outputs policy probabilities and the state value. The latter is designed to process embeddings of N history steps. In this branch, they use two sub-networks with described architecture, one for policy and one for value. They also include ablationstudies (IV-E) with only the history branch. Fig. 2 shows the full architecture of the agent.</p>
              
                  <p class="media-body media-body-page article pn">They evaluate TempAl on the challenging deep RL benchmark They use all 49 available classic Atari 2600 video game environments. In all the experiments they sample pairs from 5 last rollouts, each rollout contains 128 steps of 8 parallel environments. For 14 environments they can see a strong improvement with respect to the baseline. In these cases, the history provides a clear advantage for the RL algorithm. A small difference with the baseline indicates that the game is likely reactive and the history does not add information.</p>
              
                  <p class="media-body media-body-page article pn">They presented a method for history representation in RL. It is based on the idea that important information about the past can be “compressed” using temporal alignment. The history-only method performs better than random, thus the agent was able to extract necessary information in every environment. The approach was demonstrated on Atari games, the most popular with imagevations. The agent runs the obtained policy storing. the. embeddings for each observation, produced by E(·). Then embedding are.projected to 2D with the t-SNE algorithm.
 SSL has recently enjoyed remarkable suc-centriccess in the natural image domain, The representation is based on SSL, therefore TempAl can be used in environments with natural observations, e.g. in robotics. They believe that the work can inspire further experimentation, advancing RL to practical uses. The work was supported by the EU H2020 AI4MediaProject under Grant 951911. The author of this article has not stated that the work has been independently independently verified.</p>
              
                  <p class="media-body media-body-page article pn">Researchers: “Playing atari with deep reinforcement learn-likeing” in NIPS Deep Learning Workshop, 2013. “A graph placement methodology for fast chip design,” Nature, no. 7862, is published in June 2021. ‘A unifying mutual information view of metric learning: metrics learning: ‘cross-entropy vs. pairwise losses,’ in European conference on computer vision.Springer, 2020, pp. 548–564.
 Researchers at the ArXiv preprint arXiv:1707.06347, aprox-reformal policy optimization algorithms, have published a number of papers on machine learning. Authors include: “Prox-glyglyphanimal policy optimization algorithm” and “Deep varia-grotesquetional reinforcement learning for pomdps,” in International Conference on. PMLR, 2020, pp.21 271–21 284, 2020.
 The authors write in the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 750–15 758. The authors also discuss deep metric learning with multi-class n-pair loss loss in Advances in Neural Information Processing Systems, and how to master chess, go, chess and shogi by planning with a newly-learned model. They also discuss how to learn atari games with limited data with a model that is self-predictive representations.
 Researchers at the International Conference on Robotics and Automation (ICRA) and IEEE, 2018, pp. 1134–1141.. : Self-supervised learning of spatiotemporally coherent metrics,” in 2018 IEEE international conference on robotics and automatomation (IEEE, 2018) ‚�: “Unsupervised-likelyvised-learning of invariances.” “Theoretic-like learning” is a formative form of learning that is “supervised”</p>
              
          <a class="media" href="https://arxiv.org/abs/2204.03525" target="_blank">
            <h2 class="paper">Paper</h2>
          </a>
      </span>
  </div>
</body>

</html>