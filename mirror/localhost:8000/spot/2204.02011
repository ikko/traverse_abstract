<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Traverse Abstract</title>
    <link rel="stylesheet" href="/html/style.css">
</head>
  <a id="toggle" class="toggle-dark-mode button">‚òæ</a>
  <script src="/html/script.js"></script>

<title>Traverse Abstract - ELECRec: Training Sequential Recommenders as Discriminators - </title>
<body>
  <div  class="leftmargin">

     <h1> <a href="/">Traverse Abstract</a></h1>
     <p class="page-title"> ELECRec: Training Sequential Recommenders as Discriminators </p>
      <span class="card">
              <p class="page-theme"><a href="/theme/"></a></p>
              <img class="media-object" src="/html/sums/abstract.png"/>
              <p class="media-body media-body-page article p1"></p>
              <img class="media-object" src="/html/sums/abstract.png"/>
              <p class="media-body media-body-page article  p2"></p>
              
                  <p class="media-body media-body-page article pn">The improvements range from 38.28% to.137.16% in HR and NDCG, respectively, compared with the besteline method. Table 1 shows the overall performance comparisons on fthe datasets. Ablation study of ELECRec on Yelp and Beauty shows the effectiveness of the proposed method‚Äôs efficiency on the Sports dataset compared with a typical SR model SASRec trained with a NIP task. Efficiency comparison (Performance on validation set over validation set and training time)
 The discriminator tries to update the embeddings of plausible items, either sampled from the generator or present in the original sequence so that the SR model can distinguish them more easily. In general, the ùúÜ = 0.5 gives the best performance on Beauty and Toys in HR@5.5. When ùõº is too large, most of items in the sequence are replaced by the generator and viewed as negative class. In comparison, the generator takes more responsibility for training the whole item-based model.</p>
              
                  <p class="media-body media-body-page article pn">In this work, they propose to train sequential recommenders as dis-criminators instead of generators so that the user behavior sequence and item representations are more accurate. They propose to generate high-quality training samples for the discriminator via a jointly-trained generator. The discriminator can steadily improve its ability to discriminate the true item correlations. The research was published in the Proceedings of the 25th ACM.IGKDD International Conference on Knowledge Discovery &amp; Data Mining. In Proceedings of. the. ACM (PGKDD) 2022, July 11-15, 2022.
 Researchers from the ACM Inter-national Conference on Information &amp; Knowledge Management. 2030. 2020. Augmenting sequential.recommendation with pseudo-prior items via reversely pre-training transformer. 2021. Decided how to use a combination of self-supervision in sequential recommenders. The research is published on the OpenAI blog.com/OpenAI blog, OpenAI.1, 8 (2019), 9.1 (2019) and 9.2 (2021)
 In Advances in neural information processing systems: Attention is all you need. The study was published by the ACM International Conference on Information &amp; Knowledge. In. Pro-proceedings of the 29th ACM conference on Information and Knowledge. 1893‚Äì1902. The study is published in the journal ACM journal IJCAI.com: 2018. The journal is published by ACM SIGIR.com/pubpubpub/pub/gomeni.com. It is published on October 1, 2018.</p>
              
          <a class="media" href="https://arxiv.org/abs/2204.02011" target="_blank">
            <h2 class="paper">Paper</h2>
          </a>
      </span>
  </div>
</body>

</html>