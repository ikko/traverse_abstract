<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Traverse Abstract</title>
    <link rel="stylesheet" href="/html/style.css">
</head>
  <a id="toggle" class="toggle-dark-mode button">☾</a>
  <script src="/html/script.js"></script>

<title>Traverse Abstract - Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning - Artificial Intelligence</title>
<body>
  <div  class="leftmargin">

     <h1> <a href="/">Traverse Abstract</a></h1>
     <p class="page-title"> Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning </p>
      <span class="card">
              <p class="page-theme"><a href="/theme/Artificial_Intelligence">Artificial Intelligence</a></p>
              <img class="media-object" src="/html/sums/_sum_2204.02863.html.1.png"/>
              <p class="media-body media-body-page article p1">Demonstrate Once, Imitate Immediately (DOME) is a novel method for one-shot imitation learning. DOME uses an image-conditioned object segmentation network followed by a learned visual servoing network, to move the robot’s end-effector to the same relative pose to an object as during the demonstration. After which the task can be completed by replaying the demonstration’�vevelocities. They show that DOME achieves near 100% success rate on 7 real-world everyday tasks.&lt;br/&gt; Demonstrate Once, iMitate immEdiately(DOME) then solves a task by performing visual servoing until the bottleneck is reached, at which point the original demonstration’s EE velocities are simply replayed. They found that DOME not only performed several baselines, but was able to learn tasks from only a single demonstration at near 100% success rate. To the best of the knowledge, this is the ﬁrst method that can learn a range of real-world tasks such as these from just one demonstration, and then be deployed immediately.</p>
              <img class="media-object" src="/html/sums/_sum_2204.02863.html.2.png"/>
              <p class="media-body media-body-page article  p2">Behavioural cloning (BC) methods consist of learning to predict an expert’s actions using supervised learning,, Inverse reinforcement learning (IRL) methods imitate an expert by inferring a reward function based on the provided demonstrations. These methods require a large number of demonstrations or expensive real-world interaction (e.g. environment resetting) to be effective. In the work, they address this issue by relying only on a single demonstration and no further interaction to learn a task.&lt;br/&gt; DOME consists of two separate controllers, a learning-based visual servoing controller that takes the EE from its initial pose to the bottleneck pose, and a demonstration replay controller that completes the task. To start a demonstration, the user brings the robot’s EE to a bottleneck pose. At that point, the relative conﬁguration between the object and the EE is the same as during the demonstration. At this point the user triggers the capture and capture of the bottleneck image, and the bottleneck segmentation is automatically segmented.</p>
              
                  <p class="media-body media-body-page article pn">In practice they implemented the network as three independent Siamese CNNs that predict exy, es and er from the same image inputs. They train all the models entirely in simulation using supervised learning. In order to generate the data, they use Blender and BlenderProc and create a simulated environment with a virtual camera and random objects from the real world. They randomise various parameters such as light, colour, location and strength of light and texture conditions, such as the colthe or texture parameters.
 They used a 7 DoF Rethink Sawyer robot, to which they mounted a Microsoft Azure camera on the wrist. They set the initial EE pose such that it lies roughly 45cm above the table. The task space (where they placed the objects) was deﬁned such that the object would always be at least partially in view in the image at the EE’s initial pose, giving an area of approximately 20cm × 15cm. They chose 7 everyday manipulation tasks that are illustrated in Fig. 4.</p>
              
                  <p class="media-body media-body-page article pn">Researchers used Behavioural Cloning (BC) and Residual Reinforcement Learning (RRL) baselines to test their ability to solve tasks. They also used a DOME with hand-crafted visual servoing (DOME) and a Coarse-to-Fine IL (CTF) baseline to compare the network that is learned from a single demonstration. The DOME network was designed to reach 4.6 DoF targets, but unlike the servoing network, it does not have the potential scale to scale.
 When evaluating a method for a task, they performed a single demonstration and allowed any necessary data-collection and network training to take place. They also recorded successful task completion rates for the distractor and non-distractor cases. For each method, they also recorded the number of real-world training samples, network training time and the data collection time it took to train it. They see that the method manages to solve all tasks with near 100% success rate, and does not require any further data collection or training time after the demonstration.</p>
              
                  <p class="media-body media-body-page article pn">DOME (ours) evaluated the task success rate on each task against baselines. They increased the gains of the servoing network by a factor of ×3, ×5, and gainedgainsby increasing the average speed for reach-the-bottlenecking the bottleneck by the same factor. DOME achieved 100% success rate across all tasks and gain values tested. It became apparent that DOME is robust to changing the. values for the. servoing. network gains.
 They performed a controlled experiment where they did not use the segmenetaion network. Instead, they used the ground truth segmentation masks, to which they added various degrees of noise before inputting them to the servoing network. The mean position error increases by a few millimetres to almost 20cm, and the median angle error from below 2 to more than 50cm. They note that in practice how much visual servoing error will cause a task to fail is task-dependent.</p>
              
                  <p class="media-body media-body-page article pn">They aim to answer the fol-progressive question: What is the best neural network architecture to use for image-conditioned object segmentation? In this work they benchmark three natural alternatives that can be applied to the problem. The ﬁrst model they built is a U-Net architecture, with its input being a channel-wise concatenation of the bottleneck and live images. The second model (Tiling) processes the bottleneck image with a convolutional network to create a 256-dimensional conditioning vector.
 Researchers trained the three segmentation models on datasets with no lights, with no light source randomisation, using instead a single ambient light source and no textures. They then created a fully randomised 80, 000 image dataset with no distracting objects. They then trained all three models on each of these datasets and report their average IoU performance on the test set in Fig. 7: Segmentation network dataset content ablation. They also note that the Concatenation model performs best in all but the largest data-training dataset size.</p>
              
                  <p class="media-body media-body-page article pn">DOME is the first imitation learning method that can be deployed without requiring further data collection or training after a single demonstration or prior knowledge of the task and object. DOME takes the ﬁeld a step closer to the promise of practical one-shot imitation learning. Limitations and Future Work are exciting avenues for future work, and they believe the correct path to a system that can imitate a task in a truly practical and general way is to address these limitations and failure modes, which they aim to address in future work.
 Researchers at the International Conference on Robotics and Automation (ICRA) have proposed new ways to teach robots how to control their robotic systems. The research is being published in the journal of computer vision, the International Journal of Computer Vision, 2004, and the International journal of Computer vision, 2004. The authors also discussed the impact of the recent advances in neural-information-processing systems on robot control. The research was published on the ArXiv preprint arXiv:1802.09477, a preprint version of this article.
 An algorithmic perspective on imitation learning. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning (ICML), 2000. An algorithm for inverse. reinforcement learning for. industrial insertion tasks with visual inputs and natural. reward signals. An autonomous land vehicle in a neural network. In IEEE/RSJ. International Conference. on Intelligent Robots and Systems (IROS), 2021. The paper also discusses deep reinforcement learning on robotics problems with sparse rewards. The authors of this article have published a pre-print of arXiv: 1811.06711.
 Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. inverse reinforcement learning. included reinforcement learning in AAAI Conference on Artiﬁcial Intelligence, 2008. informablereformability. “Deep imitation learning” is a form of reinforcement learning that can be taught in virtual reality.</p>
              
          <a class="media" href="https://arxiv.org/abs/2204.02863" target="_blank">
            <h2 class="paper">Paper</h2>
          </a>
      </span>
  </div>
</body>

</html>